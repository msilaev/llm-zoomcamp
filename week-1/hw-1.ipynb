{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8913c5",
   "metadata": {},
   "source": [
    "## LLM zoomcamp Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6cd704",
   "metadata": {},
   "source": [
    "### Q 1 \n",
    "\n",
    "Run Elastic Search 8.17.6, and get the cluster information. What's the version.build_hash value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c83312",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker stop $(docker ps -q)\n",
    "docker rm $(docker ps -aq)\n",
    "\n",
    "docker pull docker.elastic.co/elasticsearch/elasticsearch:8.17.6\n",
    "\n",
    "docker run -p 9200:9200 -e \"discovery.type=single-node\" -e \"ES_JAVA_OPTS=-Xms1g -Xmx1g\" --memory=\"2g\" -e \"xpack.security.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.17.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f2893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch<9.0.0,>=8.0.0 in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (8.18.1)\n",
      "Requirement already satisfied: elastic-transport<9,>=8.15.1 in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from elasticsearch<9.0.0,>=8.0.0) (8.17.1)\n",
      "Requirement already satisfied: python-dateutil in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from elasticsearch<9.0.0,>=8.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from elasticsearch<9.0.0,>=8.0.0) (4.14.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch<9.0.0,>=8.0.0) (2.4.0)\n",
      "Requirement already satisfied: certifi in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from elastic-transport<9,>=8.15.1->elasticsearch<9.0.0,>=8.0.0) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.5 in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from python-dateutil->elasticsearch<9.0.0,>=8.0.0) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install 'elasticsearch>=8.0.0,<9.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf52347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "869677bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://localhost:9200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ac5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8064bae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version.build_hash value is dbcbbbd0bc4924cfeb28929dc05d82d662c527b7 \n"
     ]
    }
   ],
   "source": [
    "print(f\"version.build_hash value is {data[\"version\"][\"build_hash\"]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dadede86",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eba5cb",
   "metadata": {},
   "source": [
    "### Q 2\n",
    "\n",
    "Index the data in the same way as was shown in the course videos. Make the course field a keyword and the rest should be text.\n",
    "\n",
    "Which function do you use for adding your data to elastic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5422f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/llm-zoomcamp/week-1/minsearch.py:10: UserWarning: Now minsearch is installable via pip: 'pip install minsearch'. Remove the downloaded file and re-install it with pip.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f99ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "index_name = \"llm-zoomcamp\"  # Use a valid, lowercase name\n",
    "\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "try:\n",
    "    es_client.indices.create(index=index_name, body=mapping)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete the index if it already exists (optional, for a clean start)\n",
    "#if es_client.indices.exists(index=index_name):\n",
    "#    es_client.indices.delete(index=index_name)\n",
    "\n",
    "# Create the index with the mapping\n",
    "#es_client.indices.create(index=index_name, body=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e5a25eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc57832b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/948 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [00:04<00:00, 201.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e884630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "152eafbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [00:03<00:00, 300.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6a68ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use es_client.index to add data to elastic.\n"
     ]
    }
   ],
   "source": [
    "print(f\"We use es_client.index to add data to elastic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70778cd",
   "metadata": {},
   "source": [
    "### Q3. Searching\n",
    "Now let's search in our index.\n",
    "\n",
    "We will execute a query \"How do execute a command on a Kubernetes pod?\".\n",
    "\n",
    "Use only question and text fields and give question a boost of 4, and use \"type\": \"best_fields\".\n",
    "\n",
    "What's the score for the top ranking result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f46061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top hits score for query 'How do execute a command on a Kubernetes pod?': 43.790237\n"
     ]
    }
   ],
   "source": [
    "query_0 = \"How do execute a command on a Kubernetes pod?\"\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"multi_match\": {\n",
    "            \"query\": query_0,\n",
    "            \"fields\": [\"question^4\", \"text\"],\n",
    "            \"type\": \"best_fields\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es_client.search(index=index_name, body=query)\n",
    "top_hits = response['hits']['hits'][0][\"_score\"]\n",
    "\n",
    "print(f\"Top hits score for query '{query_0}': {top_hits}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6968ab",
   "metadata": {},
   "source": [
    "### Q4. Filtering\n",
    "Now ask a different question: \"How do copy a file to a Docker container?\".\n",
    "\n",
    "This time we are only interested in questions from machine-learning-zoomcamp.\n",
    "\n",
    "Return 3 results. What's the 3rd question returned by the search engine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d60dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I copy files from my local machine to docker container? You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "To copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\n",
      "docker cp /path/to/local/file_or_directory container_id:/path/in/container\n",
      "Hrithik Kumar Advani\n",
      "How do I copy files from my local machine to docker container? You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "To copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\n",
      "docker cp /path/to/local/file_or_directory container_id:/path/in/container\n",
      "Hrithik Kumar Advani\n",
      "How do I copy files from a different folder into docker container’s working directory? You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "In the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\n",
      "COPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\t\t\t\t\t\t\t\t\t\t\tGopakumar Gopinathan\n",
      "How do I copy files from a different folder into docker container’s working directory? You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "In the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\n",
      "COPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\t\t\t\t\t\t\t\t\t\t\tGopakumar Gopinathan\n",
      "How do I debug a docker container? Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\n",
      "docker run -it --entrypoint bash <image>\n",
      "If the container is already running, execute a command in the specific container:\n",
      "docker ps (find the container-id)\n",
      "docker exec -it <container-id> bash\n",
      "(Marcos MJD)\n",
      "3rd question: How do I debug a docker container?\n"
     ]
    }
   ],
   "source": [
    "query_0 = \"How do copy a file to a Docker container?\"\n",
    "\n",
    "query = {\n",
    "    \"size\": 10000,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [ \n",
    "                {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query_0,\n",
    "                \"fields\": [\"question\", \"text\"],\n",
    "                \"type\": \"best_fields\"\n",
    "        }\n",
    "    }\n",
    "    ], \n",
    "    \"filter\": [ { \"term\": {\n",
    "                \"course\": \"machine-learning-zoomcamp\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    }\n",
    "} \n",
    "}\n",
    "\n",
    "response = es_client.search(index=index_name, body=query)\n",
    "unique_response = []\n",
    "unique_questions = []\n",
    "\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    question = hit[\"_source\"][\"question\"]\n",
    "\n",
    "    #print( hit[\"_source\"][\"question\"],  hit[\"_source\"][\"text\"])\n",
    "    \n",
    "    if question not in unique_questions:\n",
    "        \n",
    "        unique_questions.append(question)\n",
    "        unique_response.append(hit)\n",
    "        \n",
    "    \n",
    "    if len(unique_questions) >= 3:\n",
    "        break   \n",
    "\n",
    "print(f\"3rd question: {unique_questions[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad8461",
   "metadata": {},
   "source": [
    "### Q5. Building a prompt\n",
    "Now we're ready to build a prompt to send to an LLM.\n",
    "\n",
    "Take the records returned from Elasticsearch in Q4 and use this template to build the context. Separate context entries by two linebreaks (\\n\\n)\n",
    "\n",
    "context_template = \"\"\"\n",
    "Q: {question}\n",
    "A: {text}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a83561d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_template = \"\"\"\n",
    "Q: {question}\n",
    "A: {text}\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\\n\\n\".join([\n",
    "        context_template.format(\n",
    "            question= doc['_source']['question'],\n",
    "            text= doc['_source']['text']\n",
    "        ) for doc in search_results\n",
    "        ] )\n",
    "    \n",
    "    #context = \"\"\n",
    "    \n",
    "    #for doc in search_results:\n",
    "        \n",
    "    #    context = context + f\"section: {doc['_source']['section']}\\nquestion: {doc['_source']['question']}\\nanswer: {doc['_source']['text']}\\n\\n\"\n",
    "    \n",
    "    #prompt = context_template.format(question=query, context=context).strip()\n",
    "    \n",
    "    prompt = prompt_template.format(question = query, context=context)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74cd0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(query_0, unique_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f73ec75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lenght of the prompt is 1447\n"
     ]
    }
   ],
   "source": [
    "print(f\"The lenght of the prompt is { len(prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911c104",
   "metadata": {},
   "source": [
    "### Q6. Tokens\n",
    "When we use the OpenAI Platform, we're charged by the number of tokens we send in our prompt and receive in the response.\n",
    "\n",
    "The OpenAI python package uses tiktoken for tokenization:\n",
    "\n",
    "Let's calculate the number of tokens in our query:\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ce30bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspaces/llm-zoomcamp/venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "028aa75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4f088d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens in the prompt is 322\n"
     ]
    }
   ],
   "source": [
    "tokens = encoding.encode(prompt)\n",
    "num_tokees = len(tokens)\n",
    "print(f\"The number of tokens in the prompt is {num_tokees}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0e5fe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
      "Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "\n",
      "QUESTION: How do copy a file to a Docker container?\n",
      "\n",
      "CONTEXT: \n",
      "Q: How do I copy files from my local machine to docker container?\n",
      "A: You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "To copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\n",
      "docker cp /path/to/local/file_or_directory container_id:/path/in/container\n",
      "Hrithik Kumar Advani\n",
      "\n",
      "Q: How do I copy files from my local machine to docker container?\n",
      "A: You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "To copy a file or directory from your local machine into a running Docker container, you can use the `docker cp command`. The basic syntax is as follows:\n",
      "docker cp /path/to/local/file_or_directory container_id:/path/in/container\n",
      "Hrithik Kumar Advani\n",
      "\n",
      "Q: How do I copy files from a different folder into docker container’s working directory?\n",
      "A: You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "In the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\n",
      "COPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\t\t\t\t\t\t\t\t\t\t\tGopakumar Gopinathan\n",
      "\n",
      "Q: How do I copy files from a different folder into docker container’s working directory?\n",
      "A: You can copy files from your local machine into a Docker container using the docker cp command. Here's how to do it:\n",
      "In the Dockerfile, you can provide the folder containing the files that you want to copy over. The basic syntax is as follows:\n",
      "COPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\t\t\t\t\t\t\t\t\t\t\tGopakumar Gopinathan\n",
      "\n",
      "Q: How do I debug a docker container?\n",
      "A: Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\n",
      "docker run -it --entrypoint bash <image>\n",
      "If the container is already running, execute a command in the specific container:\n",
      "docker ps (find the container-id)\n",
      "docker exec -it <container-id> bash\n",
      "(Marcos MJD)\n",
      "\n",
      "Q: How do I debug a docker container?\n",
      "A: Launch the container image in interactive mode and overriding the entrypoint, so that it starts a bash command.\n",
      "docker run -it --entrypoint bash <image>\n",
      "If the container is already running, execute a command in the specific container:\n",
      "docker ps (find the container-id)\n",
      "docker exec -it <container-id> bash\n",
      "(Marcos MJD)\n",
      "\n",
      "Q: How to copy a dataframe without changing the original dataframe?\n",
      "A: Copy of a dataframe is made with X_copy = X.copy().\n",
      "This is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\n",
      "Any changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\n",
      "(Memoona Tahira)\n",
      "\n",
      "Q: How to copy a dataframe without changing the original dataframe?\n",
      "A: Copy of a dataframe is made with X_copy = X.copy().\n",
      "This is called creating a deep copy.  Otherwise it will keep changing the original dataframe if used like this: X_copy = X.\n",
      "Any changes to X_copy will reflect back to X. This is not a real copy, instead it is a “view”.\n",
      "(Memoona Tahira)\n",
      "\n",
      "Q: Install Docker (udocker) in Google Colab\n",
      "A: I’ve tried to do everything in Google Colab. Here is a way to work with Docker in Google Colab:\n",
      "https://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885\n",
      "%%shell\n",
      "pip install udocker\n",
      "udocker --allow-root install\n",
      "!udocker --allow-root run hello-world\n",
      "Added by Ivan Brigida\n",
      "Lambda API Gateway errors:\n",
      "`Authorization header requires 'Credential' parameter. Authorization header requires 'Signature' parameter. Authorization header requires 'SignedHeaders' parameter. Authorization header requires existence of either a 'X-Amz-Date' or a 'Date' header.`\n",
      "`Missing Authentication Token`\n",
      "import boto3\n",
      "client = boto3.client('apigateway')\n",
      "response = client.test_invoke_method(\n",
      "restApiId='your_rest_api_id',\n",
      "resourceId='your_resource_id',\n",
      "httpMethod='POST',\n",
      "pathWithQueryString='/test/predict', #depend how you set up the api\n",
      "body='{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}'\n",
      ")\n",
      "print(response['body'])\n",
      "Yishan Zhan\n",
      "Unable to run pip install tflite_runtime from github wheel links?\n",
      "To overcome this issue, you can download the whl file to your local project folder and in the Docker file add the following lines:\n",
      "COPY <file-name> .\n",
      "RUN pip install <file-name>\n",
      "Abhijit Chakraborty\n",
      "\n",
      "Q: Install Docker (udocker) in Google Colab\n",
      "A: I’ve tried to do everything in Google Colab. Here is a way to work with Docker in Google Colab:\n",
      "https://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885\n",
      "%%shell\n",
      "pip install udocker\n",
      "udocker --allow-root install\n",
      "!udocker --allow-root run hello-world\n",
      "Added by Ivan Brigida\n",
      "Lambda API Gateway errors:\n",
      "`Authorization header requires 'Credential' parameter. Authorization header requires 'Signature' parameter. Authorization header requires 'SignedHeaders' parameter. Authorization header requires existence of either a 'X-Amz-Date' or a 'Date' header.`\n",
      "`Missing Authentication Token`\n",
      "import boto3\n",
      "client = boto3.client('apigateway')\n",
      "response = client.test_invoke_method(\n",
      "restApiId='your_rest_api_id',\n",
      "resourceId='your_resource_id',\n",
      "httpMethod='POST',\n",
      "pathWithQueryString='/test/predict', #depend how you set up the api\n",
      "body='{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}'\n",
      ")\n",
      "print(response['body'])\n",
      "Yishan Zhan\n",
      "Unable to run pip install tflite_runtime from github wheel links?\n",
      "To overcome this issue, you can download the whl file to your local project folder and in the Docker file add the following lines:\n",
      "COPY <file-name> .\n",
      "RUN pip install <file-name>\n",
      "Abhijit Chakraborty\n",
      "\n",
      "Q: Why do we need the --rm flag\n",
      "A: What is the reason we don’t want to keep the docker image in our system and why do we need to run docker containers with `--rm` flag?\n",
      "For best practice, you don’t want to have a lot of abandoned docker images in your system. You just update it in your folder and trigger the build one more time.\n",
      "They consume extra space on your disk. Unless you don’t want to re-run the previously existing containers, it is better to use the `--rm` option.\n",
      "The right way to say: “Why do we remove the docker container in our system?”. Well the docker image is still kept; it is the container that is not kept. Upon execution, images are not modified; only containers are.\n",
      "The option `--rm` is for removing containers. The images remain until you remove them manually. If you don’t specify a version when building an image, it will always rebuild and replace the latest tag. `docker images` shows you all the image you have pulled or build so far.\n",
      "During development and testing you usually specify `--rm` to get the containers auto removed upon exit. Otherwise they get accumulated in a stopped state, taking up space. `docker ps -a` shows you all the containers you have in your host. Each time you change Pipfile (or any file you baked into the container), you rebuild the image under the same tag or a new tag. It’s important to understand the difference between the term “docker image” and “docker container”. Image is what we build with all the resources baked in. You can move it around, maintain it in a repository, share it. Then we use the image to spin up instances of it and they are called containers.\n",
      "Added by Muhammad Awon\n",
      "\n",
      "Q: Why do we need the --rm flag\n",
      "A: What is the reason we don’t want to keep the docker image in our system and why do we need to run docker containers with `--rm` flag?\n",
      "For best practice, you don’t want to have a lot of abandoned docker images in your system. You just update it in your folder and trigger the build one more time.\n",
      "They consume extra space on your disk. Unless you don’t want to re-run the previously existing containers, it is better to use the `--rm` option.\n",
      "The right way to say: “Why do we remove the docker container in our system?”. Well the docker image is still kept; it is the container that is not kept. Upon execution, images are not modified; only containers are.\n",
      "The option `--rm` is for removing containers. The images remain until you remove them manually. If you don’t specify a version when building an image, it will always rebuild and replace the latest tag. `docker images` shows you all the image you have pulled or build so far.\n",
      "During development and testing you usually specify `--rm` to get the containers auto removed upon exit. Otherwise they get accumulated in a stopped state, taking up space. `docker ps -a` shows you all the containers you have in your host. Each time you change Pipfile (or any file you baked into the container), you rebuild the image under the same tag or a new tag. It’s important to understand the difference between the term “docker image” and “docker container”. Image is what we build with all the resources baked in. You can move it around, maintain it in a repository, share it. Then we use the image to spin up instances of it and they are called containers.\n",
      "Added by Muhammad Awon\n",
      "\n",
      "Q: Why do I need to provide a train.py file when I already have the notebook.ipynb file?\n",
      "A: Answer: The train.py file will be used by your peers to review your midterm project. It is for them to cross-check that your training process works on someone else’s system. It should also be included in the environment in conda or with pipenv.\n",
      "Odimegwu David\n",
      "\n",
      "Q: Why do I need to provide a train.py file when I already have the notebook.ipynb file?\n",
      "A: Answer: The train.py file will be used by your peers to review your midterm project. It is for them to cross-check that your training process works on someone else’s system. It should also be included in the environment in conda or with pipenv.\n",
      "Odimegwu David\n",
      "\n",
      "Q: How do Lambda container images work?\n",
      "A: I wanted to understand how lambda container images work in depth and how lambda functions are initialized, for this reason, I found the following documentation\n",
      "https://docs.aws.amazon.com/lambda/latest/dg/images-create.html\n",
      "https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html\n",
      "Added by Alejandro aponte\n",
      "\n",
      "Q: How do Lambda container images work?\n",
      "A: I wanted to understand how lambda container images work in depth and how lambda functions are initialized, for this reason, I found the following documentation\n",
      "https://docs.aws.amazon.com/lambda/latest/dg/images-create.html\n",
      "https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html\n",
      "Added by Alejandro aponte\n",
      "\n",
      "Q: How to pass BentoML content / docker container to Amazon Lambda\n",
      "A: Tim from BentoML has prepared a dedicated video tutorial wrt this use case here:\n",
      "https://www.youtube.com/watch?v=7gI1UH31xb4&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=97\n",
      "Konrad Muehlberg\n",
      "\n",
      "Q: How to pass BentoML content / docker container to Amazon Lambda\n",
      "A: Tim from BentoML has prepared a dedicated video tutorial wrt this use case here:\n",
      "https://www.youtube.com/watch?v=7gI1UH31xb4&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=97\n",
      "Konrad Muehlberg\n",
      "\n",
      "Q: Null column is appearing even if I applied .fillna()\n",
      "A: When creating a duplicate of your dataframe by doing the following:\n",
      "X_train = df_train\n",
      "X_val = df_val\n",
      "You’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\n",
      "X_train = df_train.copy()\n",
      "X_val = df_val.copy()\n",
      "Added by Ixchel García\n",
      "\n",
      "Q: Null column is appearing even if I applied .fillna()\n",
      "A: When creating a duplicate of your dataframe by doing the following:\n",
      "X_train = df_train\n",
      "X_val = df_val\n",
      "You’re still referencing the original variable, this is called a shallow copy. You can make sure that no references are attaching both variables and still keep the copy of the data do the following to create a deep copy:\n",
      "X_train = df_train.copy()\n",
      "X_val = df_val.copy()\n",
      "Added by Ixchel García\n",
      "\n",
      "Q: How to fix error after running the Docker run command\n",
      "A: Solution\n",
      "This error was because there was another instance of gunicorn running. So I thought of removing this along with the zoomcamp_test image. However, it didn’t let me remove the orphan container. So I did the following\n",
      "Running the following commands\n",
      "docker ps -a <to list all docker containers>\n",
      "docker images <to list images>\n",
      "docker stop <container ID>\n",
      "docker rm <container ID>\n",
      "docker rmi image\n",
      "I rebuilt the Docker image, and ran it once again; this time it worked correctly and I was able to serve the test script to the endpoint.\n",
      "\n",
      "Q: How to fix error after running the Docker run command\n",
      "A: Solution\n",
      "This error was because there was another instance of gunicorn running. So I thought of removing this along with the zoomcamp_test image. However, it didn’t let me remove the orphan container. So I did the following\n",
      "Running the following commands\n",
      "docker ps -a <to list all docker containers>\n",
      "docker images <to list images>\n",
      "docker stop <container ID>\n",
      "docker rm <container ID>\n",
      "docker rmi image\n",
      "I rebuilt the Docker image, and ran it once again; this time it worked correctly and I was able to serve the test script to the endpoint.\n",
      "\n",
      "Q: Trying to run a docker image I built but it says it’s unable to start the container process\n",
      "A: Ensure that you used pipenv to install the necessary modules including gunicorn. As pipfiles for virtual environments, you can use pipenv shell and then build+run your docker image. - Akshar Goyal\n",
      "\n",
      "Q: Trying to run a docker image I built but it says it’s unable to start the container process\n",
      "A: Ensure that you used pipenv to install the necessary modules including gunicorn. As pipfiles for virtual environments, you can use pipenv shell and then build+run your docker image. - Akshar Goyal\n",
      "\n",
      "Q: How to run a script while a web-server is working?\n",
      "A: Problem description:\n",
      "I started a web-server in terminal (command window, powershell, etc.). How can I run another python script, which makes a request to this server?\n",
      "Solution description:\n",
      "Just open another terminal (command window, powershell, etc.) and run a python script.\n",
      "Alena Kniazeva\n",
      "\n",
      "Q: How to run a script while a web-server is working?\n",
      "A: Problem description:\n",
      "I started a web-server in terminal (command window, powershell, etc.). How can I run another python script, which makes a request to this server?\n",
      "Solution description:\n",
      "Just open another terminal (command window, powershell, etc.) and run a python script.\n",
      "Alena Kniazeva\n",
      "\n",
      "Q: How do I push from Saturn Cloud to Github?\n",
      "A: Connecting your GPU on Saturn Cloud to Github repository is not compulsory, since you can just download the notebook and copy it to the Github folder. But if you like technology to do things for you, then follow the solution description below:\n",
      "Solution description: Follow the instructions in these github docs to create an SSH private and public key:\n",
      "https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-ke\n",
      "y-and-adding-it-to-the-ssh-agenthttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui\n",
      "Then the second video on this module about saturn cloud would show you how to add the ssh keys to secrets and authenticate through a terminal.\n",
      "Or alternatively, you could just use the public keys provided by Saturn Cloud by default. To do so, follow these steps:\n",
      "Click on your username and on manage\n",
      "Down below you will see the Git SSH keys section.\n",
      "Copy the default public key provided by Saturn Cloud\n",
      "Paste these key into the SSH keys section of your github repo\n",
      "Open a terminal on Saturn Cloud and run this command “ssh -T git@github.com”\n",
      "You will receive a successful authentication notice.\n",
      "Odimegwu David\n",
      "\n",
      "Q: How do I push from Saturn Cloud to Github?\n",
      "A: Connecting your GPU on Saturn Cloud to Github repository is not compulsory, since you can just download the notebook and copy it to the Github folder. But if you like technology to do things for you, then follow the solution description below:\n",
      "Solution description: Follow the instructions in these github docs to create an SSH private and public key:\n",
      "https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-ke\n",
      "y-and-adding-it-to-the-ssh-agenthttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui\n",
      "Then the second video on this module about saturn cloud would show you how to add the ssh keys to secrets and authenticate through a terminal.\n",
      "Or alternatively, you could just use the public keys provided by Saturn Cloud by default. To do so, follow these steps:\n",
      "Click on your username and on manage\n",
      "Down below you will see the Git SSH keys section.\n",
      "Copy the default public key provided by Saturn Cloud\n",
      "Paste these key into the SSH keys section of your github repo\n",
      "Open a terminal on Saturn Cloud and run this command “ssh -T git@github.com”\n",
      "You will receive a successful authentication notice.\n",
      "Odimegwu David\n",
      "\n",
      "Q: Read-in the File in Windows OS\n",
      "A: How do I read the dataset with Pandas in Windows?\n",
      "I used the code below but not working\n",
      "df = pd.read_csv('C:\\Users\\username\\Downloads\\data.csv')\n",
      "Unlike Linux/Mac OS, Windows uses the backslash (\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the '\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\n\" to add a new line or \"\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\n",
      "Here’s how we should be loading the file instead:\n",
      "df = pd.read_csv(r'C:\\Users\\username\\Downloads\\data.csv')\n",
      "(Muhammad Awon)\n",
      "\n",
      "Q: Read-in the File in Windows OS\n",
      "A: How do I read the dataset with Pandas in Windows?\n",
      "I used the code below but not working\n",
      "df = pd.read_csv('C:\\Users\\username\\Downloads\\data.csv')\n",
      "Unlike Linux/Mac OS, Windows uses the backslash (\\) to navigate the files that cause the conflict with Python. The problem with using the backslash is that in Python, the '\\' has a purpose known as an escape sequence. Escape sequences allow us to include special characters in strings, for example, \"\\n\" to add a new line or \"\\t\" to add spaces, etc. To avoid the issue we just need to add \"r\" before the file path and Python will treat it as a literal string (not an escape sequence).\n",
      "Here’s how we should be loading the file instead:\n",
      "df = pd.read_csv(r'C:\\Users\\username\\Downloads\\data.csv')\n",
      "(Muhammad Awon)\n",
      "\n",
      "Q: How to handle outliers in a dataset?\n",
      "A: There are different techniques, but the most common used are the next:\n",
      "Dataset transformation (for example, log transformation)\n",
      "Clipping high values\n",
      "Dropping these observations\n",
      "Alena Kniazeva\n",
      "\n",
      "Q: How to handle outliers in a dataset?\n",
      "A: There are different techniques, but the most common used are the next:\n",
      "Dataset transformation (for example, log transformation)\n",
      "Clipping high values\n",
      "Dropping these observations\n",
      "Alena Kniazeva\n",
      "\n",
      "Q: Is a train.py file necessary when you have a train.ipynb file in your midterm project directory?\n",
      "A: Ans: train.py has to be a python file. This is because running a python script for training a model is much simpler than running a notebook and that's how training jobs usually look like in real life.\n",
      "\n",
      "Q: Is a train.py file necessary when you have a train.ipynb file in your midterm project directory?\n",
      "A: Ans: train.py has to be a python file. This is because running a python script for training a model is much simpler than running a notebook and that's how training jobs usually look like in real life.\n",
      "\n",
      "Q: docker  build ERROR [x/y] COPY …\n",
      "A: Solution:\n",
      "This error occurred because I used single quotes around the filenames. Stick to double quotes\n",
      "\n",
      "Q: docker  build ERROR [x/y] COPY …\n",
      "A: Solution:\n",
      "This error occurred because I used single quotes around the filenames. Stick to double quotes\n",
      "\n",
      "Q: Windows version might not be up-to-date\n",
      "A: Problem description:\n",
      "In command line try to do $ docker build -t dino_dragon\n",
      "got this Using default tag: latest\n",
      "[2022-11-24T06:48:47.360149000Z][docker-credential-desktop][W] Windows version might not be up-to-date: The system cannot find the file specified.\n",
      "error during connect: This error may indicate that the docker daemon is not running.: Post\n",
      ".\n",
      "Solution description:\n",
      "You need to make sure that Docker is not stopped by a third-party program.\n",
      "Andrei Ilin\n",
      "\n",
      "Q: Windows version might not be up-to-date\n",
      "A: Problem description:\n",
      "In command line try to do $ docker build -t dino_dragon\n",
      "got this Using default tag: latest\n",
      "[2022-11-24T06:48:47.360149000Z][docker-credential-desktop][W] Windows version might not be up-to-date: The system cannot find the file specified.\n",
      "error during connect: This error may indicate that the docker daemon is not running.: Post\n",
      ".\n",
      "Solution description:\n",
      "You need to make sure that Docker is not stopped by a third-party program.\n",
      "Andrei Ilin\n",
      "\n",
      "Q: Permissions to push docker to Google Container Registry\n",
      "A: When you try to push the docker image to Google Container Registry and get this message “unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials.”, type this below on console, but first install https://cloud.google.com/sdk/docs/install, this is to be able to use gcloud in console:\n",
      "gcloud auth configure-docker\n",
      "(Jesus Acuña)\n",
      "\n",
      "Q: Permissions to push docker to Google Container Registry\n",
      "A: When you try to push the docker image to Google Container Registry and get this message “unauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials.”, type this below on console, but first install https://cloud.google.com/sdk/docs/install, this is to be able to use gcloud in console:\n",
      "gcloud auth configure-docker\n",
      "(Jesus Acuña)\n",
      "\n",
      "Q: Error: failed to compute cache key: \"/model2.bin\" not found: not found\n",
      "A: Initially, I did not assume there was a model2. I copied the original model1.bin and dv.bin. Then when I tried to load using\n",
      "COPY [\"model2.bin\", \"dv.bin\", \"./\"]\n",
      "then I got the error above in MINGW64 (git bash) on Windows.\n",
      "The temporary solution I found was to use\n",
      "COPY [\"*\", \"./\"]\n",
      "which I assume combines all the files from the original docker image and the files in your working directory.\n",
      "Added by Muhammed Tan\n",
      "\n",
      "Q: Error: failed to compute cache key: \"/model2.bin\" not found: not found\n",
      "A: Initially, I did not assume there was a model2. I copied the original model1.bin and dv.bin. Then when I tried to load using\n",
      "COPY [\"model2.bin\", \"dv.bin\", \"./\"]\n",
      "then I got the error above in MINGW64 (git bash) on Windows.\n",
      "The temporary solution I found was to use\n",
      "COPY [\"*\", \"./\"]\n",
      "which I assume combines all the files from the original docker image and the files in your working directory.\n",
      "Added by Muhammed Tan\n",
      "\n",
      "Q: What IAM permission policy is needed to complete Week 9: Serverless?\n",
      "A: Sign in to the AWS Console: Log in to the AWS Console.\n",
      "Navigate to IAM: Go to the IAM service by clicking on \"Services\" in the top left corner and selecting \"IAM\" under the \"Security, Identity, & Compliance\" section.\n",
      "Create a new policy: In the left navigation pane, select \"Policies\" and click on \"Create policy.\"\n",
      "Select the service and actions:\n",
      "Click on \"JSON\" and copy and paste the JSON policy you provided earlier for the specific ECR actions.\n",
      "Review and create the policy:\n",
      "Click on \"Review policy.\"\n",
      "Provide a name and description for the policy.\n",
      "Click on \"Create policy.\"\n",
      "JSON policy:\n",
      "{\n",
      "\"Version\": \"2012-10-17\",\n",
      "\"Statement\": [\n",
      "{\n",
      "\"Sid\": \"VisualEditor0\",\n",
      "\"Effect\": \"Allow\",\n",
      "\"Action\": [\n",
      "\"ecr:CreateRepository\",\n",
      "\"ecr:GetAuthorizationToken\",\n",
      "\"ecr:BatchCheckLayerAvailability\",\n",
      "\"ecr:BatchGetImage\",\n",
      "\"ecr:InitiateLayerUpload\",\n",
      "\"ecr:UploadLayerPart\",\n",
      "\"ecr:CompleteLayerUpload\",\n",
      "\"ecr:PutImage\"\n",
      "],\n",
      "\"Resource\": \"*\"\n",
      "}\n",
      "]\n",
      "}\n",
      "Added by: Daniel Muñoz-Viveros\n",
      "ERROR: failed to solve: public.ecr.aws/lambda/python:3.10: error getting credentials - err: exec: \"docker-credential-desktop.exe\": executable file not found in $PATH, out: ``\n",
      "(WSL2 system)\n",
      "Solved: Delete the file ~/.docker/config.json\n",
      "Yishan Zhan\n",
      "\n",
      "Q: What IAM permission policy is needed to complete Week 9: Serverless?\n",
      "A: Sign in to the AWS Console: Log in to the AWS Console.\n",
      "Navigate to IAM: Go to the IAM service by clicking on \"Services\" in the top left corner and selecting \"IAM\" under the \"Security, Identity, & Compliance\" section.\n",
      "Create a new policy: In the left navigation pane, select \"Policies\" and click on \"Create policy.\"\n",
      "Select the service and actions:\n",
      "Click on \"JSON\" and copy and paste the JSON policy you provided earlier for the specific ECR actions.\n",
      "Review and create the policy:\n",
      "Click on \"Review policy.\"\n",
      "Provide a name and description for the policy.\n",
      "Click on \"Create policy.\"\n",
      "JSON policy:\n",
      "{\n",
      "\"Version\": \"2012-10-17\",\n",
      "\"Statement\": [\n",
      "{\n",
      "\"Sid\": \"VisualEditor0\",\n",
      "\"Effect\": \"Allow\",\n",
      "\"Action\": [\n",
      "\"ecr:CreateRepository\",\n",
      "\"ecr:GetAuthorizationToken\",\n",
      "\"ecr:BatchCheckLayerAvailability\",\n",
      "\"ecr:BatchGetImage\",\n",
      "\"ecr:InitiateLayerUpload\",\n",
      "\"ecr:UploadLayerPart\",\n",
      "\"ecr:CompleteLayerUpload\",\n",
      "\"ecr:PutImage\"\n",
      "],\n",
      "\"Resource\": \"*\"\n",
      "}\n",
      "]\n",
      "}\n",
      "Added by: Daniel Muñoz-Viveros\n",
      "ERROR: failed to solve: public.ecr.aws/lambda/python:3.10: error getting credentials - err: exec: \"docker-credential-desktop.exe\": executable file not found in $PATH, out: ``\n",
      "(WSL2 system)\n",
      "Solved: Delete the file ~/.docker/config.json\n",
      "Yishan Zhan\n",
      "\n",
      "Q: How to output only a certain number of decimal places\n",
      "A: You can use round() function or f-strings\n",
      "round(number, 4)  - this will round number up to 4 decimal places\n",
      "print(f'Average mark for the Homework is {avg:.3f}') - using F string\n",
      "Also there is pandas.Series. round idf you need to round values in the whole Series\n",
      "Please check the documentation\n",
      "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\n",
      "Added by Olga Rudakova\n",
      "\n",
      "Q: How to output only a certain number of decimal places\n",
      "A: You can use round() function or f-strings\n",
      "round(number, 4)  - this will round number up to 4 decimal places\n",
      "print(f'Average mark for the Homework is {avg:.3f}') - using F string\n",
      "Also there is pandas.Series. round idf you need to round values in the whole Series\n",
      "Please check the documentation\n",
      "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round\n",
      "Added by Olga Rudakova\n",
      "\n",
      "Q: How can I annotate a graph?\n",
      "A: Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\n",
      "plt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\nOptimal F1 Score: {optimal_f1_score:.2f}',\n",
      "xy=(optimal_threshold, optimal_f1_score),\n",
      "xytext=(0.3, 0.5),\n",
      "textcoords='axes fraction',\n",
      "arrowprops=dict(facecolor='black', shrink=0.05))\n",
      "Quinn Avila\n",
      "\n",
      "Q: How can I annotate a graph?\n",
      "A: Matplotlib has a cool method to annotate where you could provide an X,Y point and annotate with an arrow and text. For example this will show an arrow pointing to the x,y point optimal threshold.\n",
      "plt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\nOptimal F1 Score: {optimal_f1_score:.2f}',\n",
      "xy=(optimal_threshold, optimal_f1_score),\n",
      "xytext=(0.3, 0.5),\n",
      "textcoords='axes fraction',\n",
      "arrowprops=dict(facecolor='black', shrink=0.05))\n",
      "Quinn Avila\n",
      "\n",
      "Q: How to select column by dtype\n",
      "A: What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\n",
      "df.select_dtypes(include=np.number).columns.tolist()\n",
      "df.select_dtypes(include='object').columns.tolist()\n",
      "Added by Gregory Morris\n",
      "\n",
      "Q: How to select column by dtype\n",
      "A: What if there were hundreds of columns? How do you get the columns only with numeric or object data in a more concise way?\n",
      "df.select_dtypes(include=np.number).columns.tolist()\n",
      "df.select_dtypes(include='object').columns.tolist()\n",
      "Added by Gregory Morris\n",
      "\n",
      "Q: Can I use LinearRegression from Scikit-Learn for this week?\n",
      "A: Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.\n",
      "\n",
      "Q: Can I use LinearRegression from Scikit-Learn for this week?\n",
      "A: Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it.\n",
      "\n",
      "Q: How to unzip a folder with an image dataset and suppress output?\n",
      "A: Problem:\n",
      "A dataset for homework is in a zipped folder. If you unzip it within a jupyter notebook by means of ! unzip command, you’ll see a huge amount of output messages about unzipping of each image. So you need to suppress this output\n",
      "Solution:\n",
      "Execute the next cell:\n",
      "%%capture\n",
      "! unzip zipped_folder_name.zip -d destination_folder_name\n",
      "Added by Alena Kniazeva\n",
      "Inside a Jupyter Notebook:\n",
      "import zipfile\n",
      "local_zip = 'data.zip'\n",
      "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
      "zip_ref.extractall('data')\n",
      "zip_ref.close()\n",
      "\n",
      "Q: How to unzip a folder with an image dataset and suppress output?\n",
      "A: Problem:\n",
      "A dataset for homework is in a zipped folder. If you unzip it within a jupyter notebook by means of ! unzip command, you’ll see a huge amount of output messages about unzipping of each image. So you need to suppress this output\n",
      "Solution:\n",
      "Execute the next cell:\n",
      "%%capture\n",
      "! unzip zipped_folder_name.zip -d destination_folder_name\n",
      "Added by Alena Kniazeva\n",
      "Inside a Jupyter Notebook:\n",
      "import zipfile\n",
      "local_zip = 'data.zip'\n",
      "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
      "zip_ref.extractall('data')\n",
      "zip_ref.close()\n",
      "\n",
      "Q: Using a variable to score\n",
      "A: https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\n",
      "Metrics can be used on a series or a dataframe\n",
      "~~Ella Sahnan~~\n",
      "\n",
      "Q: Using a variable to score\n",
      "A: https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696475675887119\n",
      "Metrics can be used on a series or a dataframe\n",
      "~~Ella Sahnan~~\n",
      "\n",
      "Q: WSL Cannot Connect To Docker Daemon\n",
      "A: Due to the uncertainties associated with machines, sometimes you can get the error message like this when you try to run a docker command:\n",
      "”Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”\n",
      "Solution: The solution is simple. The Docker Desktop might no longer be connecting to the WSL Linux distro. What you need to do is go to your Docker Desktop setting and then click on resources. Under resources, click on WSL Integration. You will get a tab like the image below:\n",
      "Just enable additional distros. That’s all. Even if the additional distro is the same as the default WSL distro.\n",
      "Odimegwu David\n",
      "\n",
      "Q: WSL Cannot Connect To Docker Daemon\n",
      "A: Due to the uncertainties associated with machines, sometimes you can get the error message like this when you try to run a docker command:\n",
      "”Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?”\n",
      "Solution: The solution is simple. The Docker Desktop might no longer be connecting to the WSL Linux distro. What you need to do is go to your Docker Desktop setting and then click on resources. Under resources, click on WSL Integration. You will get a tab like the image below:\n",
      "Just enable additional distros. That’s all. Even if the additional distro is the same as the default WSL distro.\n",
      "Odimegwu David\n",
      "\n",
      "Q: I’m new to Slack and can’t find the course channel. Where is it?\n",
      "A: Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\n",
      "Click “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\n",
      "Browse the list of public channels in your workspace, or use the search bar to search by channel name or description.\n",
      "Select a channel from the list to view it.\n",
      "Click Join Channel.\n",
      "Do we need to provide the GitHub link to only our code corresponding to the homework questions?\n",
      "Yes. You are required to provide the URL to your repo in order to receive a grade\n",
      "\n",
      "Q: I’m new to Slack and can’t find the course channel. Where is it?\n",
      "A: Here’s how you join a in Slack: https://slack.com/help/articles/205239967-Join-a-channel\n",
      "Click “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\n",
      "Browse the list of public channels in your workspace, or use the search bar to search by channel name or description.\n",
      "Select a channel from the list to view it.\n",
      "Click Join Channel.\n",
      "Do we need to provide the GitHub link to only our code corresponding to the homework questions?\n",
      "Yes. You are required to provide the URL to your repo in order to receive a grade\n",
      "\n",
      "Q: WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available\n",
      "A: When running docker build -t dino-dragon-model it returns the above error\n",
      "The most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format:\n",
      "https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\n",
      "Pastor Soto\n",
      "\n",
      "Q: WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available\n",
      "A: When running docker build -t dino-dragon-model it returns the above error\n",
      "The most common source of this error in this week is because Alex video shows a version of the wheel with python 8, we need to find a wheel with the version that we are working on. In this case python 9. Another common error is to copy the link, this will also produce the same error, we need to download the raw format:\n",
      "https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl\n",
      "Pastor Soto\n",
      "\n",
      "Q: Crucial Links\n",
      "A: These links apply to all projects, actually. Again, for some cohorts, the modules/syllabus might be different, so always check in your cohort’s folder as well for additional or different instructions, if any.\n",
      "Midterm Project Sample: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/cohorts/2021/07-midterm-project\n",
      "MidTerm Project Deliverables: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/projects\n",
      "Submit MidTerm Project: https://docs.google.com/forms/d/e/1FAIpQLSfgmOk0QrmHu5t0H6Ri1Wy_FDVS8I_nr5lY3sufkgk18I6S5A/viewform\n",
      "Datasets:\n",
      "https://www.kaggle.com/datasets and https://www.kaggle.com/competitions\n",
      "https://archive.ics.uci.edu/ml/index.php\n",
      "https://data.europa.eu/en\n",
      "https://www.openml.org/search?type=data\n",
      "https://newzealand.ai/public-data-sets\n",
      "https://datasetsearch.research.google.com\n",
      "What to do and Deliverables\n",
      "Think of a problem that's interesting for you and find a dataset for that\n",
      "Describe this problem and explain how a model could be used\n",
      "Prepare the data and doing EDA, analyze important features\n",
      "Train multiple models, tune their performance and select the best model\n",
      "Export the notebook into a script\n",
      "Put your model into a web service and deploy it locally with Docker\n",
      "Bonus points for deploying the service to the cloud\n",
      "\n",
      "Q: Crucial Links\n",
      "A: These links apply to all projects, actually. Again, for some cohorts, the modules/syllabus might be different, so always check in your cohort’s folder as well for additional or different instructions, if any.\n",
      "Midterm Project Sample: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/cohorts/2021/07-midterm-project\n",
      "MidTerm Project Deliverables: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/projects\n",
      "Submit MidTerm Project: https://docs.google.com/forms/d/e/1FAIpQLSfgmOk0QrmHu5t0H6Ri1Wy_FDVS8I_nr5lY3sufkgk18I6S5A/viewform\n",
      "Datasets:\n",
      "https://www.kaggle.com/datasets and https://www.kaggle.com/competitions\n",
      "https://archive.ics.uci.edu/ml/index.php\n",
      "https://data.europa.eu/en\n",
      "https://www.openml.org/search?type=data\n",
      "https://newzealand.ai/public-data-sets\n",
      "https://datasetsearch.research.google.com\n",
      "What to do and Deliverables\n",
      "Think of a problem that's interesting for you and find a dataset for that\n",
      "Describe this problem and explain how a model could be used\n",
      "Prepare the data and doing EDA, analyze important features\n",
      "Train multiple models, tune their performance and select the best model\n",
      "Export the notebook into a script\n",
      "Put your model into a web service and deploy it locally with Docker\n",
      "Bonus points for deploying the service to the cloud\n",
      "\n",
      "Q: How to do AWS configure after installing awscli\n",
      "A: Problem description:\n",
      "In video 9.6, after installing aswcli, we should configure it with aws configure . There it asks for Access Key ID, Secret Access Key, Default Region Name and also Default output format. What we should put for Default output format? Leaving it as  None is okay?\n",
      "Solution description:\n",
      "Yes, in my I case I left everything as the provided defaults (except, obviously, the Access key and the secret access key)\n",
      "Added by Bhaskar Sarma\n",
      "\n",
      "Q: How to do AWS configure after installing awscli\n",
      "A: Problem description:\n",
      "In video 9.6, after installing aswcli, we should configure it with aws configure . There it asks for Access Key ID, Secret Access Key, Default Region Name and also Default output format. What we should put for Default output format? Leaving it as  None is okay?\n",
      "Solution description:\n",
      "Yes, in my I case I left everything as the provided defaults (except, obviously, the Access key and the secret access key)\n",
      "Added by Bhaskar Sarma\n",
      "\n",
      "Q: How to easily get file size in powershell terminal ?\n",
      "A: To check your file size using the powershell terminal, you can do the following command lines:\n",
      "$File = Get-Item -Path path_to_file\n",
      "$FileSize = (Get-Item -Path $FilePath).Length\n",
      "Now you can check the size of your file, for example in MB:\n",
      "Write-host \"MB\":($FileSize/1MB)\n",
      "Source: https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size.\n",
      "Added by Mélanie Fouesnard\n",
      "\n",
      "Q: How to easily get file size in powershell terminal ?\n",
      "A: To check your file size using the powershell terminal, you can do the following command lines:\n",
      "$File = Get-Item -Path path_to_file\n",
      "$FileSize = (Get-Item -Path $FilePath).Length\n",
      "Now you can check the size of your file, for example in MB:\n",
      "Write-host \"MB\":($FileSize/1MB)\n",
      "Source: https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size.\n",
      "Added by Mélanie Fouesnard\n",
      "\n",
      "Q: In HW10 Q6 what does it mean “correct value for CPU and memory”? Aren’t they arbitrary?\n",
      "A: Yes, the question does require for you to specify values for CPU and memory in the yaml file, however the question that it is use in the form only refers to the port which do have a define correct value for this specific homework.\n",
      "Pastor Soto\n",
      "\n",
      "Q: In HW10 Q6 what does it mean “correct value for CPU and memory”? Aren’t they arbitrary?\n",
      "A: Yes, the question does require for you to specify values for CPU and memory in the yaml file, however the question that it is use in the form only refers to the port which do have a define correct value for this specific homework.\n",
      "Pastor Soto\n",
      "\n",
      "Q: Is there a way to serve up a form for users to enter data for the model to crunch on?\n",
      "A: Yes, you can create a mobile app or interface that manages these forms and validations. But you should also perform validations on backend.\n",
      "You can also check Streamlit: https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md\n",
      "Alejandro Aponte\n",
      "\n",
      "Q: Is there a way to serve up a form for users to enter data for the model to crunch on?\n",
      "A: Yes, you can create a mobile app or interface that manages these forms and validations. But you should also perform validations on backend.\n",
      "You can also check Streamlit: https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md\n",
      "Alejandro Aponte\n",
      "\n",
      "Q: Why cpu vals for Kubernetes deployment.yaml look like “100m” and “500m”? What does \"m\" mean?\n",
      "A: In Kubernetes resource specifications, such as CPU requests and limits, the \"m\" stands for milliCPU, which is a unit of computing power. It represents one thousandth of a CPU core.\n",
      "cpu: \"100m\" means the container is requesting 100 milliCPUs, which is equivalent to 0.1 CPU core.\n",
      "cpu: \"500m\" means the container has a CPU limit of 500 milliCPUs, which is equivalent to 0.5 CPU core.\n",
      "These values are specified in milliCPUs to allow fine-grained control over CPU resources. It allows you to express CPU requirements and limits in a more granular way, especially in scenarios where your application might not need a full CPU core.\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Why cpu vals for Kubernetes deployment.yaml look like “100m” and “500m”? What does \"m\" mean?\n",
      "A: In Kubernetes resource specifications, such as CPU requests and limits, the \"m\" stands for milliCPU, which is a unit of computing power. It represents one thousandth of a CPU core.\n",
      "cpu: \"100m\" means the container is requesting 100 milliCPUs, which is equivalent to 0.1 CPU core.\n",
      "cpu: \"500m\" means the container has a CPU limit of 500 milliCPUs, which is equivalent to 0.5 CPU core.\n",
      "These values are specified in milliCPUs to allow fine-grained control over CPU resources. It allows you to express CPU requirements and limits in a more granular way, especially in scenarios where your application might not need a full CPU core.\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Correlation before or after splitting the data\n",
      "A: Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\n",
      "Answer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.\n",
      "\n",
      "Q: Correlation before or after splitting the data\n",
      "A: Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\n",
      "Answer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.\n",
      "\n",
      "Q: Dumping/Retrieving only the size of for a specific Docker image\n",
      "A: Using the command docker images or docker image ls will dump all information for all local Docker images. It is possible to dump the information only for a specified image by using:\n",
      "docker image ls <image name>\n",
      "Or alternatively:\n",
      "docker images <image name>\n",
      "In action to that it is possible to only dump specific information provided using the option --format which will dump only the size for the specified image name when using the command below:\n",
      "docker image ls --format \"{{.Size}}\" <image name>\n",
      "Or alternatively:\n",
      "docker images --format \"{{.Size}}\" <image name>\n",
      "Sylvia Schmitt\n",
      "\n",
      "Q: Dumping/Retrieving only the size of for a specific Docker image\n",
      "A: Using the command docker images or docker image ls will dump all information for all local Docker images. It is possible to dump the information only for a specified image by using:\n",
      "docker image ls <image name>\n",
      "Or alternatively:\n",
      "docker images <image name>\n",
      "In action to that it is possible to only dump specific information provided using the option --format which will dump only the size for the specified image name when using the command below:\n",
      "docker image ls --format \"{{.Size}}\" <image name>\n",
      "Or alternatively:\n",
      "docker images --format \"{{.Size}}\" <image name>\n",
      "Sylvia Schmitt\n",
      "\n",
      "Q: How to test AWS Lambda + Docker locally?\n",
      "A: This deployment setup can be tested locally using AWS RIE (runtime interface emulator).\n",
      "Basically, if your Docker image was built upon base AWS Lambda image (FROM public.ecr.aws/lambda/python:3.10) - just use certain ports for “docker run” and a certain “localhost link” for testing:\n",
      "docker run -it --rm -p 9000:8080 name\n",
      "This command runs the image as a container and starts up an endpoint locally at:\n",
      "localhost:9000/2015-03-31/functions/function/invocations\n",
      "Post an event to the following endpoint using a curl command:\n",
      "curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'\n",
      "Examples of curl testing:\n",
      "* windows testing:\n",
      "curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \"{\\\"url\\\": \\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\"}\"\n",
      "* unix testing:\n",
      "curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}'\n",
      "If during testing you encounter an error like this:\n",
      "# {\"errorMessage\": \"Unable to marshal response: Object of type float32 is not JSON serializable\", \"errorType\": \"Runtime.MarshalError\", \"requestId\": \"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\", \"stackTrace\": []}\n",
      "just turn your response at lambda_handler() to string - str(result).\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: How to test AWS Lambda + Docker locally?\n",
      "A: This deployment setup can be tested locally using AWS RIE (runtime interface emulator).\n",
      "Basically, if your Docker image was built upon base AWS Lambda image (FROM public.ecr.aws/lambda/python:3.10) - just use certain ports for “docker run” and a certain “localhost link” for testing:\n",
      "docker run -it --rm -p 9000:8080 name\n",
      "This command runs the image as a container and starts up an endpoint locally at:\n",
      "localhost:9000/2015-03-31/functions/function/invocations\n",
      "Post an event to the following endpoint using a curl command:\n",
      "curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'\n",
      "Examples of curl testing:\n",
      "* windows testing:\n",
      "curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \"{\\\"url\\\": \\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\"}\"\n",
      "* unix testing:\n",
      "curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}'\n",
      "If during testing you encounter an error like this:\n",
      "# {\"errorMessage\": \"Unable to marshal response: Object of type float32 is not JSON serializable\", \"errorType\": \"Runtime.MarshalError\", \"requestId\": \"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\", \"stackTrace\": []}\n",
      "just turn your response at lambda_handler() to string - str(result).\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Fix error during installation of Pipfile inside Docker container\n",
      "A: I tried the first solution on Stackoverflow which recommended running `pipenv lock` to update the Pipfile.lock. However, this didn’t resolve it. But the following switch to the pipenv installation worked\n",
      "RUN pipenv install --system --deploy --ignore-pipfile\n",
      "\n",
      "Q: Fix error during installation of Pipfile inside Docker container\n",
      "A: I tried the first solution on Stackoverflow which recommended running `pipenv lock` to update the Pipfile.lock. However, this didn’t resolve it. But the following switch to the pipenv installation worked\n",
      "RUN pipenv install --system --deploy --ignore-pipfile\n",
      "\n",
      "Q: Can I use Scikit-Learn’s train_test_split for this week?\n",
      "A: Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it\n",
      "\n",
      "Q: Can I use Scikit-Learn’s train_test_split for this week?\n",
      "A: Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it\n",
      "\n",
      "Q: Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”\n",
      "A: As per AWS documentation:\n",
      "https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\n",
      "You need to do: (change the fields in red)\n",
      "aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\n",
      "Alternatively you can run the following command without changing anything given you have a default region configured\n",
      "aws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin \"$(aws sts get-caller-identity --query \"Account\" --output text).dkr.ecr.$(aws configure get region).amazonaws.com\"\n",
      "Added by Humberto Rodriguez\n",
      "\n",
      "Q: Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”\n",
      "A: As per AWS documentation:\n",
      "https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html\n",
      "You need to do: (change the fields in red)\n",
      "aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com\n",
      "Alternatively you can run the following command without changing anything given you have a default region configured\n",
      "aws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin \"$(aws sts get-caller-identity --query \"Account\" --output text).dkr.ecr.$(aws configure get region).amazonaws.com\"\n",
      "Added by Humberto Rodriguez\n",
      "\n",
      "Q: How to install easily kubectl on windows ?\n",
      "A: To install kubectl on windows using the terminal in vscode (powershell), I followed this tutorial: https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff\n",
      "I first downloaded kubectl with curl, with these command lines: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows\n",
      "At step 3, I followed the tutorial with the copy of the exe file in a specific folder on C drive.\n",
      "Then I added this folder path to PATH in my environment variables.\n",
      "Kind can be installed the same way with the curl command on windows, by specifying a folder that will be added to the path environment variable.\n",
      "Added by Mélanie Fouesnard\n",
      "\n",
      "Q: How to install easily kubectl on windows ?\n",
      "A: To install kubectl on windows using the terminal in vscode (powershell), I followed this tutorial: https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff\n",
      "I first downloaded kubectl with curl, with these command lines: https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows\n",
      "At step 3, I followed the tutorial with the copy of the exe file in a specific folder on C drive.\n",
      "Then I added this folder path to PATH in my environment variables.\n",
      "Kind can be installed the same way with the curl command on windows, by specifying a folder that will be added to the path environment variable.\n",
      "Added by Mélanie Fouesnard\n",
      "\n",
      "Q: Getting the same result\n",
      "A: While running the docker image if you get the same result check which model you are using.\n",
      "Remember you are using a model downloading model + python version so remember to change the model in your file when running your prediction test.\n",
      "Added by Ahmed Okka\n",
      "\n",
      "Q: Getting the same result\n",
      "A: While running the docker image if you get the same result check which model you are using.\n",
      "Remember you are using a model downloading model + python version so remember to change the model in your file when running your prediction test.\n",
      "Added by Ahmed Okka\n",
      "\n",
      "Q: How much Python should I know?\n",
      "A: Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\n",
      "Introduction to Python – Machine Learning Bookcamp\n",
      "You can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: How much Python should I know?\n",
      "A: Check this article. If you know everything in this article, you know enough. If you don’t, read the article and join the coursIntroduction to Pythone too :)\n",
      "Introduction to Python – Machine Learning Bookcamp\n",
      "You can follow this English course from the OpenClassrooms e-learning platform, which is free and covers the python basics for data analysis: Learn Python Basics for Data Analysis - OpenClassrooms . It is important to know some basics such as: how to run a Jupyter notebook, how to import libraries (and what libraries are), how to declare a variable (and what variables are) and some important operations regarding data analysis.\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: How do I sign up?\n",
      "A: Machine Learning Zoomcamp FAQ\n",
      "The purpose of this document is to capture frequently asked technical questions.\n",
      "We did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\n",
      "Data Engineering Zoomcamp FAQ\n",
      "In the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\n",
      "work\n",
      "\n",
      "Q: How do I sign up?\n",
      "A: Machine Learning Zoomcamp FAQ\n",
      "The purpose of this document is to capture frequently asked technical questions.\n",
      "We did this for our data engineering course and it worked quite well. Check this document for inspiration on how to structure your questions and answers:\n",
      "Data Engineering Zoomcamp FAQ\n",
      "In the course GitHub repository there’s a link. Here it is: https://airtable.com/shryxwLd0COOEaqXo\n",
      "work\n",
      "\n",
      "Q: Target variable transformation\n",
      "A: Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\n",
      "Only if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\n",
      "This can help to understand skewness and how it can be applied to the distribution of your data set.\n",
      "https://en.wikipedia.org/wiki/Skewness\n",
      "Pastor Soto\n",
      "\n",
      "Q: Target variable transformation\n",
      "A: Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\n",
      "Only if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\n",
      "This can help to understand skewness and how it can be applied to the distribution of your data set.\n",
      "https://en.wikipedia.org/wiki/Skewness\n",
      "Pastor Soto\n",
      "\n",
      "Q: What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz?\n",
      "A: Answer: Ideally midterms up to module-06, capstones include all modules in that cohort’s syllabus. But you can include anything extra that you want to feature. Just be sure to document anything not covered in class.\n",
      "Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\n",
      "More discussions:\n",
      "[source1] [source2] [source3]\n",
      "\n",
      "Q: What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz?\n",
      "A: Answer: Ideally midterms up to module-06, capstones include all modules in that cohort’s syllabus. But you can include anything extra that you want to feature. Just be sure to document anything not covered in class.\n",
      "Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\n",
      "More discussions:\n",
      "[source1] [source2] [source3]\n",
      "\n",
      "Q: How does the project evaluation work for you as a peer reviewer?\n",
      "A: I am not sure how the project evaluate assignment works? Where do I find this? I have access to all the capstone 2 project, perhaps, I can randomly pick any to review.\n",
      "Answer:\n",
      "The link provided for example (2023/Capstone link ): https://docs.google.com/forms/d/e/1FAIpQLSdgoepohpgbM4MWTAHWuXa6r3NXKnxKcg4NDOm0bElAdXdnnA/viewform contains a list of all submitted projects to be evaluated. More specific, you are to review 3 assigned peer projects. In the spreadsheet are 3 hash values of your assigned peer projects. However, you need to derive the your hash value of your email address and find the value on the spreadsheet under the (reviewer_hash) heading.\n",
      "To calculate your hash value run the python code below:\n",
      "from hashlib import sha1\n",
      "def compute_hash(email):\n",
      "return sha1(email.lower().encode('utf-8')).hexdigest()\n",
      "# Example usage **** enter your email below (Example1@gmail.com)****\n",
      "email = \"Example1@gmail.com\"\n",
      "hashed_email = compute_hash(email)\n",
      "print(\"Original Email:\", email)\n",
      "print(\"Hashed Email (SHA-1):\", hashed_email)\n",
      "Edit the above code to replace Example1@gmail.com as your email address\n",
      "Store and run the above python code from your terminal. See below as the Hashed Email (SHA-1) value\n",
      "You then go to the link: https://docs.google.com/spreadsheets/d/e/2PACX-1vR-7RRtq7AMx5OzI-tDbkzsbxNLm-NvFOP5OfJmhCek9oYcDx5jzxtZW2ZqWvBqc395UZpHBv1of9R1/pubhtml?gid=876309294&single=true\n",
      "Lastly, copy the “Hashed Email (SHA-1): bd9770be022dede87419068aa1acd7a2ab441675” value and search for 3 identical entries. There you should see your peer project to be reviewed.\n",
      "By Emmanuel Ayeni\n",
      "\n",
      "Q: How does the project evaluation work for you as a peer reviewer?\n",
      "A: I am not sure how the project evaluate assignment works? Where do I find this? I have access to all the capstone 2 project, perhaps, I can randomly pick any to review.\n",
      "Answer:\n",
      "The link provided for example (2023/Capstone link ): https://docs.google.com/forms/d/e/1FAIpQLSdgoepohpgbM4MWTAHWuXa6r3NXKnxKcg4NDOm0bElAdXdnnA/viewform contains a list of all submitted projects to be evaluated. More specific, you are to review 3 assigned peer projects. In the spreadsheet are 3 hash values of your assigned peer projects. However, you need to derive the your hash value of your email address and find the value on the spreadsheet under the (reviewer_hash) heading.\n",
      "To calculate your hash value run the python code below:\n",
      "from hashlib import sha1\n",
      "def compute_hash(email):\n",
      "return sha1(email.lower().encode('utf-8')).hexdigest()\n",
      "# Example usage **** enter your email below (Example1@gmail.com)****\n",
      "email = \"Example1@gmail.com\"\n",
      "hashed_email = compute_hash(email)\n",
      "print(\"Original Email:\", email)\n",
      "print(\"Hashed Email (SHA-1):\", hashed_email)\n",
      "Edit the above code to replace Example1@gmail.com as your email address\n",
      "Store and run the above python code from your terminal. See below as the Hashed Email (SHA-1) value\n",
      "You then go to the link: https://docs.google.com/spreadsheets/d/e/2PACX-1vR-7RRtq7AMx5OzI-tDbkzsbxNLm-NvFOP5OfJmhCek9oYcDx5jzxtZW2ZqWvBqc395UZpHBv1of9R1/pubhtml?gid=876309294&single=true\n",
      "Lastly, copy the “Hashed Email (SHA-1): bd9770be022dede87419068aa1acd7a2ab441675” value and search for 3 identical entries. There you should see your peer project to be reviewed.\n",
      "By Emmanuel Ayeni\n",
      "\n",
      "Q: I can’t create the environment on AWS Elastic Beanstalk with the command proposed during the video\n",
      "A: I struggled with the command :\n",
      "eb init -p docker tumor-diagnosis-serving -r eu-west-1\n",
      "Which resulted in an error when running : eb local run --port 9696\n",
      "ERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\n",
      "I replaced it with :\n",
      "eb init -p \"Docker running on 64bit Amazon Linux 2\" tumor-diagnosis-serving -r eu-west-1\n",
      "This allowed the recognition of the Dockerfile and the build/run of the docker container.\n",
      "Added by Mélanie Fouesnard\n",
      "\n",
      "Q: I can’t create the environment on AWS Elastic Beanstalk with the command proposed during the video\n",
      "A: I struggled with the command :\n",
      "eb init -p docker tumor-diagnosis-serving -r eu-west-1\n",
      "Which resulted in an error when running : eb local run --port 9696\n",
      "ERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\n",
      "I replaced it with :\n",
      "eb init -p \"Docker running on 64bit Amazon Linux 2\" tumor-diagnosis-serving -r eu-west-1\n",
      "This allowed the recognition of the Dockerfile and the build/run of the docker container.\n",
      "Added by Mélanie Fouesnard\n",
      "\n",
      "Q: Docker run error\n",
      "A: docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\n",
      "You need to restart the docker services to get rid of the above error\n",
      "Krishna Anand\n",
      "\n",
      "Q: Docker run error\n",
      "A: docker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\n",
      "You need to restart the docker services to get rid of the above error\n",
      "Krishna Anand\n",
      "\n",
      "Q: The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash)\n",
      "A: $ docker exec -it 1e5a1b663052 bash\n",
      "the input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'\n",
      "Fix:\n",
      "winpty docker exec -it 1e5a1b663052 bash\n",
      "A TTY is a terminal interface that supports escape sequences, moving the cursor around, etc.\n",
      "Winpty is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.\n",
      "More info on terminal, shell, console applications hi and so on:\n",
      "https://conemu.github.io/en/TerminalVsShell.html\n",
      "(Marcos MJD)\n",
      "\n",
      "Q: The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash)\n",
      "A: $ docker exec -it 1e5a1b663052 bash\n",
      "the input device is not a TTY.  If you are using mintty, try prefixing the command with 'winpty'\n",
      "Fix:\n",
      "winpty docker exec -it 1e5a1b663052 bash\n",
      "A TTY is a terminal interface that supports escape sequences, moving the cursor around, etc.\n",
      "Winpty is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.\n",
      "More info on terminal, shell, console applications hi and so on:\n",
      "https://conemu.github.io/en/TerminalVsShell.html\n",
      "(Marcos MJD)\n",
      "\n",
      "Q: How much time do I need for this course?\n",
      "A: Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article\n",
      "\n",
      "Q: Chart for classes and predictions\n",
      "A: How to visualize the predictions per classes after training a neural net\n",
      "Solution description\n",
      "classes, predictions = zip(*dict(zip(classes, predictions)).items())\n",
      "plt.figure(figsize=(12, 3))\n",
      "plt.bar(classes, predictions)\n",
      "Luke\n",
      "\n",
      "Q: How much time do I need for this course?\n",
      "A: Around ~10 hours per week. Timur Kamaliev did a detailed analysis of how much time students of the previous cohort needed to spend on different modules and projects. Full article\n",
      "\n",
      "Q: Failed to read Dockerfile\n",
      "A: When you create the dockerfile the name should be dockerfile and needs to be without extension. One of the problems we can get at this point is to create the dockerfile as a dockerfile extension Dockerfile.dockerfile which creates an error when we build the docker image. Instead we just need to create the file without extension: Dockerfile and will run perfectly.\n",
      "Added by Pastor Soto\n",
      "\n",
      "Q: Failed to read Dockerfile\n",
      "A: When you create the dockerfile the name should be dockerfile and needs to be without extension. One of the problems we can get at this point is to create the dockerfile as a dockerfile extension Dockerfile.dockerfile which creates an error when we build the docker image. Instead we just need to create the file without extension: Dockerfile and will run perfectly.\n",
      "Added by Pastor Soto\n",
      "\n",
      "Q: My dataset is too large and I can't loaded in GitHub , do anyone knows about a solution?\n",
      "A: You can use git-lfs (https://git-lfs.com/) for upload large file to github repository.\n",
      "Ryan Pramana\n",
      "\n",
      "Q: My dataset is too large and I can't loaded in GitHub , do anyone knows about a solution?\n",
      "A: You can use git-lfs (https://git-lfs.com/) for upload large file to github repository.\n",
      "Ryan Pramana\n",
      "\n",
      "Q: Why linear regression doesn’t provide a “perfect” fit?\n",
      "A: Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\n",
      "A: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\n",
      "As our model is linear, how would you draw a line to fit all the \"dots\"?\n",
      "You could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Why linear regression doesn’t provide a “perfect” fit?\n",
      "A: Q: “In lesson 2.8 why is y_pred different from y? After all, we trained X_train to get the weights that when multiplied by X_train should give exactly y, or?”\n",
      "A: linear regression is a pretty simple model, it neither can nor should fit 100% (nor any other model, as this would be the sign of overfitting). This picture might illustrate some intuition behind this, imagine X is a single feature:\n",
      "As our model is linear, how would you draw a line to fit all the \"dots\"?\n",
      "You could \"fit\" all the \"dots\" on this pic using something like scipy.optimize.curve_fit (non-linear least squares) if you wanted to, but imagine how it would perform on previously unseen data.\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: '403 Forbidden' error message when you try to push to a GitHub repository\n",
      "A: Type the following command:\n",
      "git config -l | grep url\n",
      "The output should look like this:\n",
      "remote.origin.url=https://github.com/github-username/github-repository-name.git\n",
      "Change this to the following format and make sure the change is reflected using command in step 1:\n",
      "git remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\n",
      "(Added by Dheeraj Karra)\n",
      "\n",
      "Q: '403 Forbidden' error message when you try to push to a GitHub repository\n",
      "A: Type the following command:\n",
      "git config -l | grep url\n",
      "The output should look like this:\n",
      "remote.origin.url=https://github.com/github-username/github-repository-name.git\n",
      "Change this to the following format and make sure the change is reflected using command in step 1:\n",
      "git remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\n",
      "(Added by Dheeraj Karra)\n",
      "\n",
      "Q: Will I get a certificate?\n",
      "A: Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.\n",
      "\n",
      "Q: Will I get a certificate?\n",
      "A: Yes, if you finish at least 2 out of 3 projects and review 3 peers’ Projects by the deadline, you will get a certificate. This is what it looks like: link. There’s also a version without a robot: link.\n",
      "\n",
      "Q: Error building Docker images on Mac with M1 silicon\n",
      "A: Do you get errors building the Docker image on the Mac M1 chipset?\n",
      "The error I was getting was:\n",
      "Could not open '/lib64/ld-linux-x86-64.so.2': No such file or directory\n",
      "The fix (from here): vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
      "Open mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile\n",
      "Replace line 1 with\n",
      "FROM --platform=linux/amd64 ubuntu:latest\n",
      "Now build the image as specified. In the end it took over 2 hours to build the image but it did complete in the end.\n",
      "David Colton\n",
      "\n",
      "Q: Error building Docker images on Mac with M1 silicon\n",
      "A: Do you get errors building the Docker image on the Mac M1 chipset?\n",
      "The error I was getting was:\n",
      "Could not open '/lib64/ld-linux-x86-64.so.2': No such file or directory\n",
      "The fix (from here): vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
      "Open mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile\n",
      "Replace line 1 with\n",
      "FROM --platform=linux/amd64 ubuntu:latest\n",
      "Now build the image as specified. In the end it took over 2 hours to build the image but it did complete in the end.\n",
      "David Colton\n",
      "\n",
      "Q: How to select the alpha parameter in Q6\n",
      "A: Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\n",
      "Answer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\n",
      "Asia Saeed\n",
      "\n",
      "Q: How to select the alpha parameter in Q6\n",
      "A: Question: Regarding RMSE, how do we decide on the correct score to choose? In the study group discussion    about week two homework, all of us got it wrong and one person had the lowest score selected as well.\n",
      "Answer: You need to find RMSE for each alpha. If RMSE scores  are equal, you will select the lowest alpha.\n",
      "Asia Saeed\n",
      "\n",
      "Q: Coloring the background of the pandas.DataFrame.corr correlation matrix directly\n",
      "A: The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\n",
      "Here an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\n",
      "# ensure to have only numerical values in the dataframe before calling 'corr'\n",
      "corr_mat = df_numerical_only.corr()\n",
      "corr_mat.style.background_gradient(cmap='viridis')\n",
      "Here is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\n",
      "np.random.seed = 3\n",
      "df_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\n",
      "df_random.style.background_gradient(cmap='viridis')\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: Coloring the background of the pandas.DataFrame.corr correlation matrix directly\n",
      "A: The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\n",
      "Here an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\n",
      "# ensure to have only numerical values in the dataframe before calling 'corr'\n",
      "corr_mat = df_numerical_only.corr()\n",
      "corr_mat.style.background_gradient(cmap='viridis')\n",
      "Here is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\n",
      "np.random.seed = 3\n",
      "df_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\n",
      "df_random.style.background_gradient(cmap='viridis')\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: Deploying to Digital Ocean\n",
      "A: You may quickly deploy your project to DigitalOcean App Cloud. The process is relatively straightforward. The deployment costs about 5 USD/month. The container needs to be up until the end of the project evaluation.\n",
      "Steps:\n",
      "Register in DigitalOcean\n",
      "Go to Apps -> Create App.\n",
      "You will need to choose GitHub as a service provider.\n",
      "Edit Source Directory (if your project is not in the repo root)\n",
      "IMPORTANT: Go to settings -> App Spec and edit the Dockerfile path so it looks like ./project/Dockerfile path relative to your repo root\n",
      "Remember to add model files if they are not built automatically during the container build process.\n",
      "By Dmytro Durach\n",
      "\n",
      "Q: Dockerfile missing when creating the AWS ElasticBean environment\n",
      "A: I had this error when creating a AWS ElasticBean environment: eb create tumor-diagnosis-env\n",
      "ERROR   Instance deployment: Both 'Dockerfile' and 'Dockerrun.aws.json' are missing in your source bundle. Include at least one of them. The deployment failed.\n",
      "I did not committed the files used to build the container, particularly the Dockerfile. After a git add and git commit of the modified files, the command works.\n",
      "Added by Mélanie Fouesnard\n",
      "\n",
      "Q: Dockerfile missing when creating the AWS ElasticBean environment\n",
      "A: I had this error when creating a AWS ElasticBean environment: eb create tumor-diagnosis-env\n",
      "ERROR   Instance deployment: Both 'Dockerfile' and 'Dockerrun.aws.json' are missing in your source bundle. Include at least one of them. The deployment failed.\n",
      "I did not committed the files used to build the container, particularly the Dockerfile. After a git add and git commit of the modified files, the command works.\n",
      "Added by Mélanie Fouesnard\n",
      "\n",
      "Q: What if I miss a session?\n",
      "A: Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.\n",
      "\n",
      "Q: What if I miss a session?\n",
      "A: Everything is recorded, so you won’t miss anything. You will be able to ask your questions for office hours in advance and we will cover them during the live stream. Also, you can always ask questions in Slack.\n",
      "\n",
      "Q: When I plotted using Matplot lib to check if median has a tail, I got the error below how can one bypass?\n",
      "A: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "\n",
      "Q: How do I get started with Week 2?\n",
      "A: Here are the crucial links for this Week 2 that starts September 18, 2023\n",
      "Ask questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\n",
      "Calendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\n",
      "Week 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\n",
      "Submit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "Youtube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\n",
      "FAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\n",
      "~~Nukta Bhatia~~\n",
      "\n",
      "Q: How do I get started with Week 3?\n",
      "A: Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\n",
      "Submit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "Youtube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\n",
      "~~Nukta Bhatia~~\n",
      "\n",
      "Q: How do I get started with Week 4?\n",
      "A: Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "YouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\n",
      "Sci-Kit Learn on Evaluation:\n",
      "https://scikit-learn.org/stable/model_selection.html\n",
      "~~Nukta Bhatia~~\n",
      "\n",
      "Q: How do I get started with Week 5?\n",
      "A: Week 5 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/05-deployment/homework.md\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "HW 3 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework_3.ipynb\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "YouTube Link: 5.X --- https://www.youtube.com/watch?v=agIFak9A3m8&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=49\n",
      "~~~ Nukta Bhatia ~~~\n",
      "\n",
      "Q: How do I get started with Week 2?\n",
      "A: Here are the crucial links for this Week 2 that starts September 18, 2023\n",
      "Ask questions for Live Sessions: https://app.sli.do/event/vsUpjYsayZ8A875Hq8dpUa/live/questions\n",
      "Calendar for weekly meetings: https://calendar.google.com/calendar/u/0/r?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&pli=1\n",
      "Week 2 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/02-regression/homework.md\n",
      "Submit HW Week 2: https://docs.google.com/forms/d/e/1FAIpQLSf8eMtnErPFqzzFsEdLap_GZ2sMih-H-Y7F_IuPGqt4fOmOJw/viewform (also available at the bottom of the above link)\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "Youtube Link: 2.X --- https://www.youtube.com/watch?v=vM3SqPNlStE&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=12\n",
      "FAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\n",
      "~~Nukta Bhatia~~\n",
      "\n",
      "Q: How do I get started with Week 3?\n",
      "A: Week 3 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/03-classification/homework.md\n",
      "Submit HW Week 3: https://docs.google.com/forms/d/e/1FAIpQLSeXS3pqsv_smRkYmVx-7g6KIZDnG29g2s7pdHo-ASKNqtfRFQ/viewform\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "Youtube Link: 3.X --- https://www.youtube.com/watch?v=0Zw04wdeTQo&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=29\n",
      "~~Nukta Bhatia~~\n",
      "\n",
      "Q: How do I get started with Week 4?\n",
      "A: Week 4 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/04-evaluation/homework.md\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "YouTube Link: 4.X --- https://www.youtube.com/watch?v=gmg5jw1bM8A&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=40\n",
      "Sci-Kit Learn on Evaluation:\n",
      "https://scikit-learn.org/stable/model_selection.html\n",
      "~~Nukta Bhatia~~\n",
      "\n",
      "Q: How do I get started with Week 5?\n",
      "A: Week 5 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/05-deployment/homework.md\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "HW 3 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework_3.ipynb\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "YouTube Link: 5.X --- https://www.youtube.com/watch?v=agIFak9A3m8&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=49\n",
      "~~~ Nukta Bhatia ~~~\n",
      "\n",
      "Q: Kind cannot load docker image\n",
      "A: Problem: Failing to load docker-image to cluster (when you’ved named a cluster)\n",
      "kind load docker-image zoomcamp-10-model:xception-v4-001\n",
      "ERROR: no nodes found for cluster \"kind\"\n",
      "Solution: Specify cluster name with -n\n",
      "kind -n clothing-model load docker-image zoomcamp-10-model:xception-v4-001\n",
      "Andrew Katoch\n",
      "\n",
      "Q: Kind cannot load docker image\n",
      "A: Problem: Failing to load docker-image to cluster (when you’ved named a cluster)\n",
      "kind load docker-image zoomcamp-10-model:xception-v4-001\n",
      "ERROR: no nodes found for cluster \"kind\"\n",
      "Solution: Specify cluster name with -n\n",
      "kind -n clothing-model load docker-image zoomcamp-10-model:xception-v4-001\n",
      "Andrew Katoch\n",
      "\n",
      "Q: Reproducibility in different OS\n",
      "A: When trying to rerun the docker file in Windows, as opposed to developing in WSL/Linux, I got the error of:\n",
      "```\n",
      "Warning: Python 3.11 was not found on your system…\n",
      "Neither ‘pipenv’ nor ‘asdf’ could be found to install Python.\n",
      "You can specify specific versions of Python with:\n",
      "$ pipenv –python path\\to\\python\n",
      "```\n",
      "The solution was to add Python311 installation folder to the PATH and restart the system and run the docker file again. That solved the error.\n",
      "(Added by Abhijit Chakraborty)\n",
      "\n",
      "Q: Getting a syntax error while trying to get the password from aws-cli\n",
      "A: The command aws ecr get-login --no-include-email returns an invalid choice error:\n",
      "The solution is to use the following command instead:  aws ecr get-login-password\n",
      "Could simplify the login process with, just replace the <ACCOUNT_NUMBER> and <REGION> with your values:\n",
      "export PASSWORD=`aws ecr get-login-password`\n",
      "docker login -u AWS -p $PASSWORD <ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images\n",
      "Added by Martin Uribe\n",
      "\n",
      "Q: Getting a syntax error while trying to get the password from aws-cli\n",
      "A: The command aws ecr get-login --no-include-email returns an invalid choice error:\n",
      "The solution is to use the following command instead:  aws ecr get-login-password\n",
      "Could simplify the login process with, just replace the <ACCOUNT_NUMBER> and <REGION> with your values:\n",
      "export PASSWORD=`aws ecr get-login-password`\n",
      "docker login -u AWS -p $PASSWORD <ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images\n",
      "Added by Martin Uribe\n",
      "\n",
      "Q: Conda Environment Setup\n",
      "A: With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\n",
      "Answer:\n",
      "\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\n",
      "(Added by Wesley Barreto)\n",
      "conda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml\n",
      "\n",
      "Q: Conda Environment Setup\n",
      "A: With regards to creating an environment for the project, do we need to run the command \"conda create -n .......\" and \"conda activate ml-zoomcamp\" everytime we open vs code to work on the project?\n",
      "Answer:\n",
      "\"conda create -n ....\" is just run the first time to create the environment. Once created, you just need to run \"conda activate ml-zoomcamp\" whenever you want to use it.\n",
      "(Added by Wesley Barreto)\n",
      "conda env export > environment.yml will also allow you to reproduce your existing environment in a YAML file.  You can then recreate it with conda env create -f environment.yml\n",
      "\n",
      "Q: How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows?\n",
      "A: You can consider several different approaches:\n",
      "Sampling: In the exploratory phase, you can use random samples of the data.\n",
      "Chunking: When you do need all the data, you can read and process it in chunks that do fit in the memory.\n",
      "Optimizing data types: Pandas’ automatic data type inference (when reading data in) might result in e.g. float64 precision being used to represent integers, which wastes space. You might achieve substantial memory reduction by optimizing the data types.\n",
      "Using Dask, an open-source python project which parallelizes Numpy and Pandas.\n",
      "(see, e.g. https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)\n",
      "By Rileen Sinha\n",
      "\n",
      "Q: Filter a dataset by using its values\n",
      "A: We can filter a dataset by using its values as below.\n",
      "df = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\n",
      "You can use | for ‘OR’, and & for ‘AND’\n",
      "Alternative:\n",
      "df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n",
      "Radikal Lukafiardi\n",
      "\n",
      "Q: Reproducibility with TensorFlow using a seed point\n",
      "A: Reproducibility for training runs can be achieved following these instructions: \n",
      "https://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism\n",
      "seed = 1234\n",
      "tf.keras.utils.set_random_seed(seed)\n",
      "tf.config.experimental.enable_op_determinism()\n",
      "This will work for a script, if this gets executed multiple times.\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: Filter a dataset by using its values\n",
      "A: We can filter a dataset by using its values as below.\n",
      "df = df[(df[\"ocean_proximity\"] == \"<1H OCEAN\") | (df[\"ocean_proximity\"] == \"INLAND\")]\n",
      "You can use | for ‘OR’, and & for ‘AND’\n",
      "Alternative:\n",
      "df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n",
      "Radikal Lukafiardi\n",
      "\n",
      "Q: Reproducibility with TensorFlow using a seed point\n",
      "A: Reproducibility for training runs can be achieved following these instructions: \n",
      "https://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism\n",
      "seed = 1234\n",
      "tf.keras.utils.set_random_seed(seed)\n",
      "tf.config.experimental.enable_op_determinism()\n",
      "This will work for a script, if this gets executed multiple times.\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: Do you pass a project based on the average of everyone else’s scores or based on the total score you earn?\n",
      "A: Alexey Grigorev: “It’s based on all the scores to make sure most of you pass.”                                                   By Annaliese Bronz\n",
      "Other course-related questions that don’t fall into any of the categories above or can apply to more than one category/module\n",
      "\n",
      "Q: Do you pass a project based on the average of everyone else’s scores or based on the total score you earn?\n",
      "A: Alexey Grigorev: “It’s based on all the scores to make sure most of you pass.”                                                   By Annaliese Bronz\n",
      "Other course-related questions that don’t fall into any of the categories above or can apply to more than one category/module\n",
      "\n",
      "Q: Alternative way to load the data using requests\n",
      "A: Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\n",
      "# Get data for homework\n",
      "import requests\n",
      "url = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\n",
      "response = requests.get(url)\n",
      "if response.status_code == 200:\n",
      "with open('housing.csv', 'wb') as file:\n",
      "file.write(response.content)\n",
      "else:\n",
      "print(\"Download failed.\")\n",
      "Tyler Simpson\n",
      "\n",
      "Q: Alternative way to load the data using requests\n",
      "A: Above users showed how to load the dataset directly from github. Here is another useful way of doing this using the `requests` library:\n",
      "# Get data for homework\n",
      "import requests\n",
      "url = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\n",
      "response = requests.get(url)\n",
      "if response.status_code == 200:\n",
      "with open('housing.csv', 'wb') as file:\n",
      "file.write(response.content)\n",
      "else:\n",
      "print(\"Download failed.\")\n",
      "Tyler Simpson\n",
      "\n",
      "Q: Pickle error: can’t get attribute XXX on module __main__\n",
      "A: When running a docker container with waitress serving the app.py for making predictions, pickle will throw an error that can't get attribute <name_of_class> on module __main__.\n",
      "This does not happen when Flask is used directly, i.e. not through waitress.\n",
      "The problem is that the model uses a custom column transformer class, and when the model was saved, it was saved from the __main__ module (e.g. python train.py). Pickle will reference the class in the global namespace (top-level code): __main__.<custom_class>.\n",
      "When using waitress, waitress will load the predict_app module and this will call pickle.load, that will try to find __main__.<custom_class> that does not exist.\n",
      "Solution:\n",
      "Put the class into a separate module and import it in both the script that saves the model (e.g. train.py) and the script that loads the model (e.g. predict.py)\n",
      "Note: If Flask is used (no waitress) in predict.py, and predict.py has the definition of the class, When  it is run: python predict.py, it will work because the class is in the same namespace as the one used when the model was saved (__main__).\n",
      "Detailed info: https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules\n",
      "Marcos MJD\n",
      "\n",
      "Q: Pickle error: can’t get attribute XXX on module __main__\n",
      "A: When running a docker container with waitress serving the app.py for making predictions, pickle will throw an error that can't get attribute <name_of_class> on module __main__.\n",
      "This does not happen when Flask is used directly, i.e. not through waitress.\n",
      "The problem is that the model uses a custom column transformer class, and when the model was saved, it was saved from the __main__ module (e.g. python train.py). Pickle will reference the class in the global namespace (top-level code): __main__.<custom_class>.\n",
      "When using waitress, waitress will load the predict_app module and this will call pickle.load, that will try to find __main__.<custom_class> that does not exist.\n",
      "Solution:\n",
      "Put the class into a separate module and import it in both the script that saves the model (e.g. train.py) and the script that loads the model (e.g. predict.py)\n",
      "Note: If Flask is used (no waitress) in predict.py, and predict.py has the definition of the class, When  it is run: python predict.py, it will work because the class is in the same namespace as the one used when the model was saved (__main__).\n",
      "Detailed info: https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules\n",
      "Marcos MJD\n",
      "\n",
      "Q: Can we use pytorch for this lesson/homework ?\n",
      "A: Pytorch is also a deep learning framework that allows to do equivalent tasks as keras. Here is a tutorial to create a CNN from scratch using pytorch :\n",
      "https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\n",
      "The functions have similar goals. The syntax can be slightly different. For the lessons and the homework, we use keras, but one can feel free to make a pull request with the equivalent with pytorch for lessons and homework!\n",
      "Mélanie Fouesnard\n",
      "\n",
      "Q: Can we use pytorch for this lesson/homework ?\n",
      "A: Pytorch is also a deep learning framework that allows to do equivalent tasks as keras. Here is a tutorial to create a CNN from scratch using pytorch :\n",
      "https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/\n",
      "The functions have similar goals. The syntax can be slightly different. For the lessons and the homework, we use keras, but one can feel free to make a pull request with the equivalent with pytorch for lessons and homework!\n",
      "Mélanie Fouesnard\n",
      "\n",
      "Q: How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService?\n",
      "A: The docker image for aws lambda can be created and pushed to aws ecr and the same can be exposed as a REST API through APIGatewayService in a single go using AWS Serverless Framework. Refer the below article for a detailed walkthrough.\n",
      "https://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d\n",
      "Added by Sumeet Lalla\n",
      "\n",
      "Q: How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService?\n",
      "A: The docker image for aws lambda can be created and pushed to aws ecr and the same can be exposed as a REST API through APIGatewayService in a single go using AWS Serverless Framework. Refer the below article for a detailed walkthrough.\n",
      "https://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d\n",
      "Added by Sumeet Lalla\n",
      "\n",
      "Q: How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience\n",
      "A: You’ll need a kaggle account\n",
      "Go to settings, API and click `Create New Token`. This will download a `kaggle.json` file which contains your `username` and `key` information\n",
      "In the same location as your Jupyter NB, place the `kaggle.json` file\n",
      "Run `!chmod 600 <ENTER YOUR FILEPATH>/kaggle.json`\n",
      "Make sure to import os via `import os` and then run:\n",
      "os.environ['KAGGLE_CONFIG_DIR'] = <STRING OF YOUR FILE PATH>\n",
      "Finally you can run directly in your NB: `!kaggle datasets download -d kapturovalexander/bank-credit-scoring`\n",
      "And then you can unzip the file and access the CSV via: `!unzip -o bank-credit-scoring.zip`\n",
      ">>> Michael Fronda <<<\n",
      "\n",
      "Q: How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience\n",
      "A: You’ll need a kaggle account\n",
      "Go to settings, API and click `Create New Token`. This will download a `kaggle.json` file which contains your `username` and `key` information\n",
      "In the same location as your Jupyter NB, place the `kaggle.json` file\n",
      "Run `!chmod 600 <ENTER YOUR FILEPATH>/kaggle.json`\n",
      "Make sure to import os via `import os` and then run:\n",
      "os.environ['KAGGLE_CONFIG_DIR'] = <STRING OF YOUR FILE PATH>\n",
      "Finally you can run directly in your NB: `!kaggle datasets download -d kapturovalexander/bank-credit-scoring`\n",
      "And then you can unzip the file and access the CSV via: `!unzip -o bank-credit-scoring.zip`\n",
      ">>> Michael Fronda <<<\n",
      "\n",
      "Q: Error building docker image on M1 Mac\n",
      "A: Problem:\n",
      "While trying to build docker image in Section 9.5 with the command:\n",
      "docker build -t clothing-model .\n",
      "It throws a pip install error for the tflite runtime whl\n",
      "ERROR: failed to solve: process \"/bin/sh -c pip install https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\" did not complete successfully: exit code: 1\n",
      "Try to use this link: https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\n",
      "If the link above does not work:\n",
      "The problem is because of the arm architecture of the M1. You will need to run the code on a PC or Ubuntu OS.\n",
      "Or try the code bellow.\n",
      "Added by Dashel Ruiz Perez\n",
      "Solution:\n",
      "To build the Docker image, use the command:\n",
      "docker build --platform linux/amd64 -t clothing-model .\n",
      "To run the built image, use the command:\n",
      "docker run -it --rm -p 8080:8080 --platform linux/amd64 clothing-model:latest\n",
      "Added by Daniel Egbo\n",
      "\n",
      "Q: Error building docker image on M1 Mac\n",
      "A: Problem:\n",
      "While trying to build docker image in Section 9.5 with the command:\n",
      "docker build -t clothing-model .\n",
      "It throws a pip install error for the tflite runtime whl\n",
      "ERROR: failed to solve: process \"/bin/sh -c pip install https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\" did not complete successfully: exit code: 1\n",
      "Try to use this link: https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\n",
      "If the link above does not work:\n",
      "The problem is because of the arm architecture of the M1. You will need to run the code on a PC or Ubuntu OS.\n",
      "Or try the code bellow.\n",
      "Added by Dashel Ruiz Perez\n",
      "Solution:\n",
      "To build the Docker image, use the command:\n",
      "docker build --platform linux/amd64 -t clothing-model .\n",
      "To run the built image, use the command:\n",
      "docker run -it --rm -p 8080:8080 --platform linux/amd64 clothing-model:latest\n",
      "Added by Daniel Egbo\n",
      "\n",
      "Q: Tflite_runtime unable to install\n",
      "A: I am getting this error message when I tried to install tflite in a pipenv environment\n",
      "Error:  An error occurred while installing tflite_runtime!\n",
      "Error text:\n",
      "ERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)\n",
      "ERROR: No matching distribution found for tflite_runtime\n",
      "This version of tflite do not run on python 3.10, the way we can make it work is by install python 3.9, after that it would install the tflite_runtime without problem.\n",
      "Pastor Soto\n",
      "Check all available versions here:\n",
      "https://google-coral.github.io/py-repo/tflite-runtime/\n",
      "If you don’t find a combination matching your setup, try out the options at\n",
      "https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\n",
      "which you can install as shown in the lecture, e.g.\n",
      "pip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\n",
      "Finally, if nothing works, use the TFLite included in TensorFlow for local development, and use Docker for testing Lambda.\n",
      "Rileen Sinha (based on discussions on Slack)\n",
      "\n",
      "Q: Tflite_runtime unable to install\n",
      "A: I am getting this error message when I tried to install tflite in a pipenv environment\n",
      "Error:  An error occurred while installing tflite_runtime!\n",
      "Error text:\n",
      "ERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)\n",
      "ERROR: No matching distribution found for tflite_runtime\n",
      "This version of tflite do not run on python 3.10, the way we can make it work is by install python 3.9, after that it would install the tflite_runtime without problem.\n",
      "Pastor Soto\n",
      "Check all available versions here:\n",
      "https://google-coral.github.io/py-repo/tflite-runtime/\n",
      "If you don’t find a combination matching your setup, try out the options at\n",
      "https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\n",
      "which you can install as shown in the lecture, e.g.\n",
      "pip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\n",
      "Finally, if nothing works, use the TFLite included in TensorFlow for local development, and use Docker for testing Lambda.\n",
      "Rileen Sinha (based on discussions on Slack)\n",
      "\n",
      "Q: Question 4: what is `r`, is it the same as `alpha` in sklearn.Ridge()?\n",
      "A: `r` is a regularization parameter.\n",
      "It’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here's how both are used:\n",
      "sklearn.Ridge()\n",
      "||y - Xw||^2_2 + alpha * ||w||^2_2\n",
      "lesson’s notebook (`train_linear_regression_reg` function)\n",
      "XTX = XTX + r * np.eye(XTX.shape[0])\n",
      "`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.\n",
      "\n",
      "Q: Question 4: what is `r`, is it the same as `alpha` in sklearn.Ridge()?\n",
      "A: `r` is a regularization parameter.\n",
      "It’s similar to `alpha` in sklearn.Ridge(), as both control the \"strength\" of regularization (increasing both will lead to stronger regularization), but mathematically not quite, here's how both are used:\n",
      "sklearn.Ridge()\n",
      "||y - Xw||^2_2 + alpha * ||w||^2_2\n",
      "lesson’s notebook (`train_linear_regression_reg` function)\n",
      "XTX = XTX + r * np.eye(XTX.shape[0])\n",
      "`r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding inverse matrix.\n",
      "\n",
      "Q: Question 5: How and why do we replace the NaN values with average of the column?\n",
      "A: You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\n",
      "This method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\n",
      "Added by Anneysha Sarkar\n",
      "\n",
      "Q: Question 5: How and why do we replace the NaN values with average of the column?\n",
      "A: You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\n",
      "This method is called imputing - when you have NaN/ null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns.\n",
      "Added by Anneysha Sarkar\n",
      "\n",
      "Q: Failed to write the dependencies to pipfile and piplock file\n",
      "A: Create a virtual environment using the Cmd command (command) and use pip freeze command to write the requirements in the text file\n",
      "Krishna Anand\n",
      "\n",
      "Q: Failed to write the dependencies to pipfile and piplock file\n",
      "A: Create a virtual environment using the Cmd command (command) and use pip freeze command to write the requirements in the text file\n",
      "Krishna Anand\n",
      "\n",
      "Q: Error downloading  tensorflow/serving:2.7.0 on Apple M1 Mac\n",
      "A: While trying to run the docker code on M1:\n",
      "docker run --platform linux/amd64 -it --rm \\\n",
      "-p 8500:8500 \\\n",
      "-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n",
      "-e MODEL_NAME=\"clothing-model\" \\\n",
      "tensorflow/serving:2.7.0\n",
      "It outputs the error:\n",
      "Error:\n",
      "Status: Downloaded newer image for tensorflow/serving:2.7.0\n",
      "[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:\n",
      "terminate called after throwing an instance of 'google::protobuf::FatalException'\n",
      "what():  CHECK failed: file != nullptr:\n",
      "qemu: uncaught target signal 6 (Aborted) - core dumped\n",
      "/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\n",
      "Solution\n",
      "docker pull emacski/tensorflow-serving:latest\n",
      "docker run -it --rm \\\n",
      "-p 8500:8500 \\\n",
      "-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n",
      "-e MODEL_NAME=\"clothing-model\" \\\n",
      "emacski/tensorflow-serving:latest-linux_arm64\n",
      "See more here: https://github.com/emacski/tensorflow-serving-arm\n",
      "Added by Daniel Egbo\n",
      "\n",
      "Q: Error downloading  tensorflow/serving:2.7.0 on Apple M1 Mac\n",
      "A: While trying to run the docker code on M1:\n",
      "docker run --platform linux/amd64 -it --rm \\\n",
      "-p 8500:8500 \\\n",
      "-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n",
      "-e MODEL_NAME=\"clothing-model\" \\\n",
      "tensorflow/serving:2.7.0\n",
      "It outputs the error:\n",
      "Error:\n",
      "Status: Downloaded newer image for tensorflow/serving:2.7.0\n",
      "[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:\n",
      "terminate called after throwing an instance of 'google::protobuf::FatalException'\n",
      "what():  CHECK failed: file != nullptr:\n",
      "qemu: uncaught target signal 6 (Aborted) - core dumped\n",
      "/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\n",
      "Solution\n",
      "docker pull emacski/tensorflow-serving:latest\n",
      "docker run -it --rm \\\n",
      "-p 8500:8500 \\\n",
      "-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n",
      "-e MODEL_NAME=\"clothing-model\" \\\n",
      "emacski/tensorflow-serving:latest-linux_arm64\n",
      "See more here: https://github.com/emacski/tensorflow-serving-arm\n",
      "Added by Daniel Egbo\n",
      "\n",
      "Q: Capture stdout for each iterations of a loop separately\n",
      "A: I wanted to directly capture the output from the xgboost training for multiple eta values to a dictionary without the need to run the same cell multiple times and manually editing the eta value in between or copy the code for a second eta value.\n",
      "Using the magic cell command “%%capture output” I was only able to capture the complete output for all iterations for the loop, but. I was able to solve this using the following approach. This is just a code sample to grasp the idea.\n",
      "# This would be the content of the Jupyter Notebook cell\n",
      "from IPython.utils.capture import capture_output\n",
      "import sys\n",
      "different_outputs = {}\n",
      "for i in range(3):\n",
      "with capture_output(sys.stdout) as output:\n",
      "print(i)\n",
      "print(\"testing capture\")\n",
      "different_outputs[i] = output.stdout\n",
      "# different_outputs\n",
      "# {0: '0\\ntesting capture\\n',\n",
      "#  1: '1\\ntesting capture\\n',\n",
      "#  2: '2\\ntesting capture\\n'}\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: Capture stdout for each iterations of a loop separately\n",
      "A: I wanted to directly capture the output from the xgboost training for multiple eta values to a dictionary without the need to run the same cell multiple times and manually editing the eta value in between or copy the code for a second eta value.\n",
      "Using the magic cell command “%%capture output” I was only able to capture the complete output for all iterations for the loop, but. I was able to solve this using the following approach. This is just a code sample to grasp the idea.\n",
      "# This would be the content of the Jupyter Notebook cell\n",
      "from IPython.utils.capture import capture_output\n",
      "import sys\n",
      "different_outputs = {}\n",
      "for i in range(3):\n",
      "with capture_output(sys.stdout) as output:\n",
      "print(i)\n",
      "print(\"testing capture\")\n",
      "different_outputs[i] = output.stdout\n",
      "# different_outputs\n",
      "# {0: '0\\ntesting capture\\n',\n",
      "#  1: '1\\ntesting capture\\n',\n",
      "#  2: '2\\ntesting capture\\n'}\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask\n",
      "A: Problem Description:\n",
      "In video 10.3, when I was testing a flask service, I got the above error. I ran docker run .. in one terminal. When in second terminal I run python gateway.py, I get the above error.\n",
      "Solution: This error has something to do with versions of Flask and Werkzeug. I got the same error, if I just import flask with from flask import Flask.\n",
      "By running pip freeze > requirements.txt,I found that their versions are Flask==2.2.2 and Werkzeug==2.2.2. This error appears while using an old version of werkzeug (2.2.2) with new version of flask (2.2.2). I solved it by pinning version of Flask into an older version with pipenv install Flask==2.1.3.\n",
      "Added by Bhaskar Sarma\n",
      "\n",
      "Q: TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask\n",
      "A: Problem Description:\n",
      "In video 10.3, when I was testing a flask service, I got the above error. I ran docker run .. in one terminal. When in second terminal I run python gateway.py, I get the above error.\n",
      "Solution: This error has something to do with versions of Flask and Werkzeug. I got the same error, if I just import flask with from flask import Flask.\n",
      "By running pip freeze > requirements.txt,I found that their versions are Flask==2.2.2 and Werkzeug==2.2.2. This error appears while using an old version of werkzeug (2.2.2) with new version of flask (2.2.2). I solved it by pinning version of Flask into an older version with pipenv install Flask==2.1.3.\n",
      "Added by Bhaskar Sarma\n",
      "\n",
      "Q: waitress-serve shows Malformed application\n",
      "A: Question:\n",
      "When running\n",
      "pipenv run waitress-serve --listen=localhost:9696 q4-predict:app\n",
      "I get the following:\n",
      "There was an exception (ValueError) importing your module.\n",
      "It had these arguments:\n",
      "1. Malformed application 'q4-predict:app'\n",
      "Answer:\n",
      "Waitress doesn’t accept a dash in the python file name.\n",
      "The solution is to rename the file replacing a dash with something else for instance with an underscore eg q4_predict.py\n",
      "Added by Alex Litvinov\n",
      "\n",
      "Q: waitress-serve shows Malformed application\n",
      "A: Question:\n",
      "When running\n",
      "pipenv run waitress-serve --listen=localhost:9696 q4-predict:app\n",
      "I get the following:\n",
      "There was an exception (ValueError) importing your module.\n",
      "It had these arguments:\n",
      "1. Malformed application 'q4-predict:app'\n",
      "Answer:\n",
      "Waitress doesn’t accept a dash in the python file name.\n",
      "The solution is to rename the file replacing a dash with something else for instance with an underscore eg q4_predict.py\n",
      "Added by Alex Litvinov\n",
      "\n",
      "Q: Running ‘nvidia-smi’ in a loop without using ‘watch’\n",
      "A: The command ‘nvidia-smi’ has a built-in function which will run it in subsequently updating it every N seconds without the need of using the command ‘watch’.\n",
      "nvidia-smi -l <N seconds>\n",
      "The following command will run ‘nvidia-smi’ every 2 seconds until interrupted using CTRL+C.\n",
      "nvidia-smi -l 2\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: Running ‘nvidia-smi’ in a loop without using ‘watch’\n",
      "A: The command ‘nvidia-smi’ has a built-in function which will run it in subsequently updating it every N seconds without the need of using the command ‘watch’.\n",
      "nvidia-smi -l <N seconds>\n",
      "The following command will run ‘nvidia-smi’ every 2 seconds until interrupted using CTRL+C.\n",
      "nvidia-smi -l 2\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\n",
      "A: Question:\n",
      "When executing\n",
      "eb local run  --port 9696\n",
      "I get the following error:\n",
      "ERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\n",
      "Answer:\n",
      "There are two options to fix this:\n",
      "Re-initialize by running eb init -i and choosing the options from a list (the first default option for docker platform should be fine).\n",
      "Edit the ‘.elasticbeanstalk/config.yml’ directly changing the default_platform from Docker to default_platform: Docker running on 64bit Amazon Linux 2023\n",
      "The disadvantage of the second approach is that the option might not be available the following years\n",
      "Added by Alex Litvinov\n",
      "\n",
      "Q: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\n",
      "A: Question:\n",
      "When executing\n",
      "eb local run  --port 9696\n",
      "I get the following error:\n",
      "ERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\n",
      "Answer:\n",
      "There are two options to fix this:\n",
      "Re-initialize by running eb init -i and choosing the options from a list (the first default option for docker platform should be fine).\n",
      "Edit the ‘.elasticbeanstalk/config.yml’ directly changing the default_platform from Docker to default_platform: Docker running on 64bit Amazon Linux 2023\n",
      "The disadvantage of the second approach is that the option might not be available the following years\n",
      "Added by Alex Litvinov\n",
      "\n",
      "Q: Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way?\n",
      "A: (Question by Connie S.)\n",
      "The reason it's good/recommended practice to do it after splitting is to avoid data leakage - you don't want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\n",
      "Answered/added by Rileen Sinha\n",
      "\n",
      "Q: Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way?\n",
      "A: (Question by Connie S.)\n",
      "The reason it's good/recommended practice to do it after splitting is to avoid data leakage - you don't want any data from the test set influencing the training stage (similarly from the validation stage in the initial training). See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": https://scikit-learn.org/stable/common_pitfalls.html\n",
      "Answered/added by Rileen Sinha\n",
      "\n",
      "Q: Running out of storage after building many docker images\n",
      "A: Problem description\n",
      "Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance.\n",
      "My first reflex was to remove some zoomcamp directories, but of course those are mostly code so it didn’t help much.\n",
      "Solution description\n",
      "> docker images\n",
      "revealed that I had over 20 GBs worth of superseded / duplicate models lying around, so I proceeded to > docker rmi\n",
      "a bunch of those — but to no avail!\n",
      "It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run\n",
      "> docker system prune\n",
      "See also: https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind\n",
      "Added by Konrad Mühlberg\n",
      "\n",
      "Q: Running out of storage after building many docker images\n",
      "A: Problem description\n",
      "Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance.\n",
      "My first reflex was to remove some zoomcamp directories, but of course those are mostly code so it didn’t help much.\n",
      "Solution description\n",
      "> docker images\n",
      "revealed that I had over 20 GBs worth of superseded / duplicate models lying around, so I proceeded to > docker rmi\n",
      "a bunch of those — but to no avail!\n",
      "It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run\n",
      "> docker system prune\n",
      "See also: https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind\n",
      "Added by Konrad Mühlberg\n",
      "\n",
      "Q: Sequential vs. Functional Model Modes in Keras (TF2)\n",
      "A: It’s quite useful to understand that all types of models in the course are a plain stack of layers where each layer has exactly one input tensor and one output tensor (Sequential model TF page, Sequential class).\n",
      "You can simply start from an “empty” model and add more and more layers in a sequential order.\n",
      "This mode is called “Sequential Model API”  (easier)\n",
      "In Alexey’s videos it is implemented as chained calls of different entities (“inputs”,“base”, “vectors”,  “outputs”) in a more advanced mode “Functional Model API”.\n",
      "Maybe a more complicated way makes sense when you do Transfer Learning and want to separate “Base” model vs. rest, but in the HW you need to recreate the full model from scratch ⇒ I believe it is easier to work with a sequence of “similar” layers.\n",
      "You can read more about it in this TF2 tutorial.\n",
      "A really useful Sequential model example is shared in the Kaggle’s “Bee or Wasp” dataset folder with code: notebook\n",
      "Added by Ivan Brigida\n",
      "Fresh Run on Neural Nets\n",
      "While correcting an error on neural net architecture, it is advised to do fresh run by restarting kernel, else the model learns on top of previous runs.\n",
      "Added by Abhijit Chakraborty\n",
      "\n",
      "Q: Sequential vs. Functional Model Modes in Keras (TF2)\n",
      "A: It’s quite useful to understand that all types of models in the course are a plain stack of layers where each layer has exactly one input tensor and one output tensor (Sequential model TF page, Sequential class).\n",
      "You can simply start from an “empty” model and add more and more layers in a sequential order.\n",
      "This mode is called “Sequential Model API”  (easier)\n",
      "In Alexey’s videos it is implemented as chained calls of different entities (“inputs”,“base”, “vectors”,  “outputs”) in a more advanced mode “Functional Model API”.\n",
      "Maybe a more complicated way makes sense when you do Transfer Learning and want to separate “Base” model vs. rest, but in the HW you need to recreate the full model from scratch ⇒ I believe it is easier to work with a sequence of “similar” layers.\n",
      "You can read more about it in this TF2 tutorial.\n",
      "A really useful Sequential model example is shared in the Kaggle’s “Bee or Wasp” dataset folder with code: notebook\n",
      "Added by Ivan Brigida\n",
      "Fresh Run on Neural Nets\n",
      "While correcting an error on neural net architecture, it is advised to do fresh run by restarting kernel, else the model learns on top of previous runs.\n",
      "Added by Abhijit Chakraborty\n",
      "\n",
      "Q: Computing the hash for the leaderboard and project review\n",
      "A: Leaderboard Links:\n",
      "2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\n",
      "2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\n",
      "Python Code:\n",
      "from hashlib import sha1\n",
      "def compute_hash(email):\n",
      "return sha1(email.lower().encode('utf-8')).hexdigest()\n",
      "You need to call the function as follows:\n",
      "print(compute_hash('YOUR_EMAIL_HERE'))\n",
      "The quotes are required to denote that your email is a string.\n",
      "(By Wesley Barreto)\n",
      "You can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: Computing the hash for the leaderboard and project review\n",
      "A: Leaderboard Links:\n",
      "2023 - https://docs.google.com/spreadsheets/d/e/2PACX-1vSNK_yGtELX1RJK1SSRl4xiUbD0XZMYS6uwHnybc7Mql-WMnMgO7hHSu59w-1cE7FeFZjkopbh684UE/pubhtml\n",
      "2022 - https://docs.google.com/spreadsheets/d/e/2PACX-1vQzLGpva63gb2rIilFnpZMRSb-buyr5oGh8jmDtIb8DANo4n6hDalra_WRCl4EZwO1JvaC4UIS62n5h/pubhtml\n",
      "Python Code:\n",
      "from hashlib import sha1\n",
      "def compute_hash(email):\n",
      "return sha1(email.lower().encode('utf-8')).hexdigest()\n",
      "You need to call the function as follows:\n",
      "print(compute_hash('YOUR_EMAIL_HERE'))\n",
      "The quotes are required to denote that your email is a string.\n",
      "(By Wesley Barreto)\n",
      "You can also use this website directly by entering your email: http://www.sha1-online.com. Then, you just have to copy and paste your hashed email in the “research” bar of the leaderboard to get your scores.\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: Setting up an environment using VS Code\n",
      "A: I found this video quite helpful: Creating Virtual Environment for Python from VS Code\n",
      "[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\n",
      "[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\n",
      "(Added by Ivan Brigida)\n",
      "\n",
      "Q: Setting up an environment using VS Code\n",
      "A: I found this video quite helpful: Creating Virtual Environment for Python from VS Code\n",
      "[Native Jupiter Notebooks support in VS Code] In VS Code you can also have a native Jupiter Notebooks support, i.e. you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled + run a ‘jupyter notebook ‘ command from a remote machine + have a remote connection configured in .ssh/config (as Alexey’s video suggests) - VS Code can execute remote Jupyter Notebooks files on a remote server from your local machine: https://code.visualstudio.com/docs/datascience/jupyter-notebooks.\n",
      "[Git support from VS Code] You can work with Github from VSCode - staging and commits are easy from the VS Code’s UI:  https://code.visualstudio.com/docs/sourcecontrol/overview\n",
      "(Added by Ivan Brigida)\n",
      "\n",
      "Q: When does the next iteration start?\n",
      "A: The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).\n",
      "\n",
      "Q: When does the next iteration start?\n",
      "A: The course is available in the self-paced mode too, so you can go through the materials at any time. But if you want to do it as a cohort with other students, the next iterations will happen in September 2023, September 2024 (and potentially other Septembers as well).\n",
      "\n",
      "Q: Does the actual values matter after predicting with a neural network or it should be treated as like hood of falling in a class?\n",
      "A: It's fine, some small changes are expected\n",
      "Alexey Grigorev\n",
      "\n",
      "Q: Does the actual values matter after predicting with a neural network or it should be treated as like hood of falling in a class?\n",
      "A: It's fine, some small changes are expected\n",
      "Alexey Grigorev\n",
      "\n",
      "Q: How to Install Xgboost\n",
      "A: To install Xgboost, use the code below directly in your jupyter notebook:\n",
      "(Pip 21.3+ is required)\n",
      "pip install xgboost\n",
      "You can update your pip by using the code below:\n",
      "pip install --upgrade pip\n",
      "For more about xgbboost and installation, check here:\n",
      "https://xgboost.readthedocs.io/en/stable/install.html\n",
      "Aminat Abolade\n",
      "\n",
      "Q: How to Install Xgboost\n",
      "A: To install Xgboost, use the code below directly in your jupyter notebook:\n",
      "(Pip 21.3+ is required)\n",
      "pip install xgboost\n",
      "You can update your pip by using the code below:\n",
      "pip install --upgrade pip\n",
      "For more about xgbboost and installation, check here:\n",
      "https://xgboost.readthedocs.io/en/stable/install.html\n",
      "Aminat Abolade\n",
      "\n",
      "Q: Windows WSL and VS Code\n",
      "If you have a Windows 11 device and would like to use the built in WSL to access linux you can use the Microsoft Learn link Set up a WSL development environment | Microsoft Learn. To connect this to VS Code download the Microsoft verified VS Code extension ‘WSL’ this will allow you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine.\n",
      "A: (Tyler Simpson)\n",
      "\n",
      "Q: Windows WSL and VS Code\n",
      "If you have a Windows 11 device and would like to use the built in WSL to access linux you can use the Microsoft Learn link Set up a WSL development environment | Microsoft Learn. To connect this to VS Code download the Microsoft verified VS Code extension ‘WSL’ this will allow you to remotely connect to your WSL Ubuntu instance as if it was a virtual machine.\n",
      "A: (Tyler Simpson)\n",
      "\n",
      "Q: Your Pipfile.lock (221d14) is out of date (during Docker build)\n",
      "A: If during running the  docker build command, you get an error like this:\n",
      "Your Pipfile.lock (221d14) is out of date. Expected: (939fe0).\n",
      "Usage: pipenv install [OPTIONS] [PACKAGES]...\n",
      "ERROR:: Aborting deploy\n",
      "Option 1: Delete the pipfile.lock via rm Pipfile, and then rebuild the lock via  pipenv lock from the terminal before retrying the docker build command.\n",
      "Option 2:  If it still doesn’t work, remove the pipenv environment, Pipfile and Pipfile.lock, and create a new one before building docker again. Commands to remove pipenv environment and removing pipfiles:\n",
      "pipenv  --rm\n",
      "rm Pipfile*\n",
      "\n",
      "Q: Your Pipfile.lock (221d14) is out of date (during Docker build)\n",
      "A: If during running the  docker build command, you get an error like this:\n",
      "Your Pipfile.lock (221d14) is out of date. Expected: (939fe0).\n",
      "Usage: pipenv install [OPTIONS] [PACKAGES]...\n",
      "ERROR:: Aborting deploy\n",
      "Option 1: Delete the pipfile.lock via rm Pipfile, and then rebuild the lock via  pipenv lock from the terminal before retrying the docker build command.\n",
      "Option 2:  If it still doesn’t work, remove the pipenv environment, Pipfile and Pipfile.lock, and create a new one before building docker again. Commands to remove pipenv environment and removing pipfiles:\n",
      "pipenv  --rm\n",
      "rm Pipfile*\n",
      "\n",
      "Q: How to upload kaggle data to Saturn Cloud?\n",
      "A: Problem description: Uploading the data to saturn cloud from kaggle can be time saving, specially if the dataset is large.\n",
      "You can just download to your local machine and then upload to a folder on saturn cloud, but there is a better solution that needs to be set once and you have access to all kaggle datasets in saturn cloud.\n",
      "On your notebook run:\n",
      "!pip install -q kaggle\n",
      "Go to Kaggle website (you need to have an account for this):\n",
      "Click on your profile image -> Account\n",
      "Scroll down to the API box\n",
      "Click on Create New API token\n",
      "It will download a json file with the name kaggle.json store on your local computer. We need to upload this file in the .kaggle folder\n",
      "On the notebook click on folder icon on the left upper corner\n",
      "This will take you to the root folder\n",
      "Click on the .kaggle folder\n",
      "Once inside of the .kaggle folder upload the kaggle.json file that you downloaded\n",
      "Run this command on your notebook:\n",
      "!chmod 600 /home/jovyan/.kaggle/kaggle.json\n",
      "Download the data using this command:\n",
      "!kaggle datasets download -d agrigorev/dino-or-dragon\n",
      "Create a folder to unzip your files:\n",
      "!mkdir data\n",
      "Unzip your files inside that folder\n",
      "!unzip dino-or-dragon.zip -d data\n",
      "Pastor Soto\n",
      "\n",
      "Q: How to upload kaggle data to Saturn Cloud?\n",
      "A: Problem description: Uploading the data to saturn cloud from kaggle can be time saving, specially if the dataset is large.\n",
      "You can just download to your local machine and then upload to a folder on saturn cloud, but there is a better solution that needs to be set once and you have access to all kaggle datasets in saturn cloud.\n",
      "On your notebook run:\n",
      "!pip install -q kaggle\n",
      "Go to Kaggle website (you need to have an account for this):\n",
      "Click on your profile image -> Account\n",
      "Scroll down to the API box\n",
      "Click on Create New API token\n",
      "It will download a json file with the name kaggle.json store on your local computer. We need to upload this file in the .kaggle folder\n",
      "On the notebook click on folder icon on the left upper corner\n",
      "This will take you to the root folder\n",
      "Click on the .kaggle folder\n",
      "Once inside of the .kaggle folder upload the kaggle.json file that you downloaded\n",
      "Run this command on your notebook:\n",
      "!chmod 600 /home/jovyan/.kaggle/kaggle.json\n",
      "Download the data using this command:\n",
      "!kaggle datasets download -d agrigorev/dino-or-dragon\n",
      "Create a folder to unzip your files:\n",
      "!mkdir data\n",
      "Unzip your files inside that folder\n",
      "!unzip dino-or-dragon.zip -d data\n",
      "Pastor Soto\n",
      "\n",
      "Q: What does KFold do?\n",
      "A: What does this line do?\n",
      "KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
      "If I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\n",
      "Did you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\n",
      "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
      "In my case changing random state changed results\n",
      "(Arthur Minakhmetov)\n",
      "Changing the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\n",
      "(Bhaskar Sarma)\n",
      "In case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\n",
      "(Ani Mkrtumyan)\n",
      "\n",
      "Q: What does KFold do?\n",
      "A: What does this line do?\n",
      "KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
      "If I do it inside the loop [0.01, 0.1, 1, 10] or outside the loop in Q6, HW04 it doesn't make any difference to my answers. I am wondering why and what is the right way, although it doesn't make a difference!\n",
      "Did you try using a different random_state? From my understanding, KFold just makes N (which is equal to n_splits) separate pairs of datasets (train+val).\n",
      "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
      "In my case changing random state changed results\n",
      "(Arthur Minakhmetov)\n",
      "Changing the random state makes a difference in my case too, but not whether it is inside or outside the for loop. I think I have got the answer. kFold = KFold(n_splits=n_splits, shuffle = True, random_state = 1)  is just a generator object and it contains only the information n_splits, shuffle and random_state. The k-fold splitting actually happens in the next for loop for train_idx, val_idx in kFold.split(df_full_train): . So it doesn't matter where we generate the object, before or after the first loop. It will generate the same information. But from the programming point of view, it is better to do it before the loop. No point doing it again and again inside the loop\n",
      "(Bhaskar Sarma)\n",
      "In case of KFold(n_splits=n_splits, shuffle=True, random_state=1) and C= [0.01, 0.1, 1, 10], it is better to loop through the different values of Cs as the video explained. I had separate train() and predict() functions, which were reused after dividing the dataset via KFold. The model ran about 10 minutes and provided a good score.\n",
      "(Ani Mkrtumyan)\n",
      "\n",
      "Q: Could not convert string to float:’Nissan’rt string to float: 'Nissan'\n",
      "A: The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\n",
      "To resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\n",
      "Here’s an example of how you can perform one-hot encoding using pandas:\n",
      "import pandas as pd\n",
      "# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\n",
      "data_encoded = pd.get_dummies(data, columns=['brand'])\n",
      "In this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\n",
      "-Mohammad Emad Sharifi-\n",
      "\n",
      "Q: Could not convert string to float:’Nissan’rt string to float: 'Nissan'\n",
      "A: The error message “could not convert string to float: ‘Nissan’” typically occurs when a machine learning model or function is expecting numerical input, but receives a string instead. In this case, it seems like the model is trying to convert the car brand ‘Nissan’ into a numerical value, which isn’t possible.\n",
      "To resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\n",
      "Here’s an example of how you can perform one-hot encoding using pandas:\n",
      "import pandas as pd\n",
      "# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\n",
      "data_encoded = pd.get_dummies(data, columns=['brand'])\n",
      "In this code, pd.get_dummies() creates a new DataFrame where the ‘brand’ column is replaced with binary columns for each brand (e.g., ‘brand_Nissan’, ‘brand_Toyota’, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns.\n",
      "-Mohammad Emad Sharifi-\n",
      "\n",
      "Q: Will I get a certificate if I missed the midterm project?\n",
      "A: Yes, it's possible. See the previous answer.\n",
      "\n",
      "Q: Will I get a certificate if I missed the midterm project?\n",
      "A: Yes, it's possible. See the previous answer.\n",
      "\n",
      "Q: Features for homework Q5\n",
      "A: Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\n",
      "You need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\n",
      "While calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\n",
      "Since order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)\n",
      "\n",
      "Q: Features for homework Q5\n",
      "A: Do we need to train the model only with the features: total_rooms, total_bedrooms, population and households? or with all the available features and then pop once at a time each of the previous features and train the model to make the accuracy comparison?\n",
      "You need to create a list of all features in this question and evaluate the model one time to obtain the accuracy, this will be the original accuracy, and then remove one feature each time, and in each time, train the model, find the accuracy and the difference between the original accuracy and the found accuracy. Finally, find out which feature has the smallest absolute accuracy difference.\n",
      "While calculating differences between accuracy scores while training on the whole model, versus dropping one feature at a time and comparing its accuracy to the model to judge impact of the feature on the accuracy of the model, do we take the smallest difference or smallest absolute difference?\n",
      "Since order of subtraction between the two accuracy scores can result in a negative number, we will take its absolute value as we are interested in the smallest value difference, not the lowest difference value. Case in point, if difference is -4 and -2, the smallest difference is abs(-2), and not abs(-4)\n",
      "\n",
      "Q: Cannot connect to the docker daemon. Is the Docker daemon running?\n",
      "A: Working on getting Docker installed - when I try running hello-world I am getting the error.\n",
      "Docker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?\n",
      "Solution description\n",
      "If you’re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).\n",
      "On Linux, start the docker daemon with either of these commands:\n",
      "sudo dockerd\n",
      "sudo service docker start\n",
      "Added by Ugochukwu Onyebuchi\n",
      "\n",
      "Q: Cannot connect to the docker daemon. Is the Docker daemon running?\n",
      "A: Working on getting Docker installed - when I try running hello-world I am getting the error.\n",
      "Docker: Cannot connect to the docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running ?\n",
      "Solution description\n",
      "If you’re getting this error on WSL, re-install your docker: remove the docker installation from WSL and install Docker Desktop on your host machine (Windows).\n",
      "On Linux, start the docker daemon with either of these commands:\n",
      "sudo dockerd\n",
      "sudo service docker start\n",
      "Added by Ugochukwu Onyebuchi\n",
      "\n",
      "Q: Useful Resource for Missing Data Treatment\n",
      "https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook\n",
      "A: (Hrithik Kumar Advani)\n",
      "\n",
      "Q: Useful Resource for Missing Data Treatment\n",
      "https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook\n",
      "A: (Hrithik Kumar Advani)\n",
      "\n",
      "Q: Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well)\n",
      "A: Similar to the one above but with a different solution the main reason is that emacski doesn’t seem to maintain the repo any more, the latest image is from 2 years ago at the time of writing (December 2023)\n",
      "Problem:\n",
      "While trying to run the docker code on Mac M2 apple silicon:\n",
      "docker run --platform linux/amd64 -it --rm \\\n",
      "-p 8500:8500 \\\n",
      "-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n",
      "-e MODEL_NAME=\"clothing-model\" \\\n",
      "tensorflow/serving\n",
      "You get an error:\n",
      "/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\n",
      "Solution:\n",
      "Use bitnami/tensorflow-serving base image\n",
      "Launch it either using docker run\n",
      "docker run -d \\\n",
      "--name tf_serving \\\n",
      "-p 8500:8500 \\\n",
      "-p 8501:8501 \\\n",
      "-v $(pwd)/clothing-model:/bitnami/model-data/1 \\\n",
      "-e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \\\n",
      "bitnami/tensorflow-serving:2\n",
      "Or the following docker-compose.yaml\n",
      "version: '3'\n",
      "services:\n",
      "tf_serving:\n",
      "image: bitnami/tensorflow-serving:2\n",
      "volumes:\n",
      "- ${PWD}/clothing-model:/bitnami/model-data/1\n",
      "ports:\n",
      "- 8500:8500\n",
      "- 8501:8501\n",
      "environment:\n",
      "- TENSORFLOW_SERVING_MODEL_NAME=clothing-model\n",
      "And run it with\n",
      "docker compose up\n",
      "Added by Alex Litvinov\n",
      "\n",
      "Q: Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well)\n",
      "A: Similar to the one above but with a different solution the main reason is that emacski doesn’t seem to maintain the repo any more, the latest image is from 2 years ago at the time of writing (December 2023)\n",
      "Problem:\n",
      "While trying to run the docker code on Mac M2 apple silicon:\n",
      "docker run --platform linux/amd64 -it --rm \\\n",
      "-p 8500:8500 \\\n",
      "-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n",
      "-e MODEL_NAME=\"clothing-model\" \\\n",
      "tensorflow/serving\n",
      "You get an error:\n",
      "/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\n",
      "Solution:\n",
      "Use bitnami/tensorflow-serving base image\n",
      "Launch it either using docker run\n",
      "docker run -d \\\n",
      "--name tf_serving \\\n",
      "-p 8500:8500 \\\n",
      "-p 8501:8501 \\\n",
      "-v $(pwd)/clothing-model:/bitnami/model-data/1 \\\n",
      "-e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \\\n",
      "bitnami/tensorflow-serving:2\n",
      "Or the following docker-compose.yaml\n",
      "version: '3'\n",
      "services:\n",
      "tf_serving:\n",
      "image: bitnami/tensorflow-serving:2\n",
      "volumes:\n",
      "- ${PWD}/clothing-model:/bitnami/model-data/1\n",
      "ports:\n",
      "- 8500:8500\n",
      "- 8501:8501\n",
      "environment:\n",
      "- TENSORFLOW_SERVING_MODEL_NAME=clothing-model\n",
      "And run it with\n",
      "docker compose up\n",
      "Added by Alex Litvinov\n",
      "\n",
      "Q: CUDA toolkit and cuDNN Install for Tensorflow\n",
      "A: Install Nvidia drivers: https://www.nvidia.com/download/index.aspx.\n",
      "Windows:\n",
      "Install Anaconda prompt https://www.anaconda.com/\n",
      "Two options:\n",
      "Install package ‘tensorflow-gpu’ in Anaconda\n",
      "Install the Tensorflow way https://www.tensorflow.org/install/pip#windows-native\n",
      "WSL/Linux:\n",
      "WSL: Use the Windows Nvida drivers, do not touch that.\n",
      "Two options:\n",
      "Install the Tensorflow way https://www.tensorflow.org/install/pip#linux_1\n",
      "Make sure to follow step 4 to install CUDA by environment\n",
      "Also run:\n",
      "echo ‘export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n",
      "Install CUDA toolkit 11.x.x https://developer.nvidia.com/cuda-toolkit-archive\n",
      "Install https://developer.nvidia.com/rdp/cudnn-download\n",
      "Now you should be able to do training/inference with GPU in Tensorflow\n",
      "(Learning in public links Links to social media posts where you share your progress with others (LinkedIn, Twitter, etc). Use #mlzoomcamp tag. The scores for this part will be capped at 7 points. Please make sure the posts are valid URLs starting with \"https://\" Does it mean that I should provide my linkedin link? or it means that I should write a post that I have completed my first assignement? (\n",
      "ANS (by ezehcp7482@gmail.com): Yes, provide the linkedIN link to where you posted.\n",
      "ezehcp7482@gmail.com:\n",
      "PROBLEM: Since I had to put up a link to a public repository, I had to use Kaggle and uploading the dataset therein was a bit difficult; but I had to ‘google’ my way out.\n",
      "ANS: See this link for a guide (https://www.kaggle.com/code/dansbecker/finding-your-files-in-kaggle-kernels/notebook)\n",
      "\n",
      "Q: I just joined. What should I do next? How can I access course materials?\n",
      "A: Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\n",
      "Click on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\n",
      "Or you can just use this link: http://mlzoomcamp.com/#syllabus\n",
      "\n",
      "Q: I just joined. What should I do next? How can I access course materials?\n",
      "A: Welcome to the course! Go to the course page (http://mlzoomcamp.com/), scroll down and start going through the course materials. Then read everything in the cohort folder for your cohort’s year.\n",
      "Click on the links and start watching the videos. Also watch office hours from previous cohorts. Go to DTC youtube channel and click on Playlists and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\n",
      "Or you can just use this link: http://mlzoomcamp.com/#syllabus\n",
      "\n",
      "Q: Installing waitress on Windows via GitBash: “waitress-serve” command not found\n",
      "A: Running 'pip install waitress' as a command on GitBash was not downloading the executable file 'waitress-serve.exe'. You need this file to be able to run commands with waitress in Git Bash. To solve this:\n",
      "open a Jupyter notebook and run the same command ' pip install waitress'. This way the executable file will be downloaded. The notebook may give you this warning : 'WARNING: The script waitress-serve.exe is installed in 'c:\\Users\\....\\anaconda3\\Scripts' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.'\n",
      "Add the path where 'waitress-serve.exe' is installed into gitbash's PATH as such:\n",
      "enter the following command in gitbash: nano ~/.bashrc\n",
      "add the path to 'waitress-serve.exe' to PATH using this command: export PATH=\"/path/to/waitress:$PATH\"\n",
      "close gitbash and open it again and you should be good to go\n",
      "Added by Bachar Kabalan\n",
      "\n",
      "Q: Installing waitress on Windows via GitBash: “waitress-serve” command not found\n",
      "A: Running 'pip install waitress' as a command on GitBash was not downloading the executable file 'waitress-serve.exe'. You need this file to be able to run commands with waitress in Git Bash. To solve this:\n",
      "open a Jupyter notebook and run the same command ' pip install waitress'. This way the executable file will be downloaded. The notebook may give you this warning : 'WARNING: The script waitress-serve.exe is installed in 'c:\\Users\\....\\anaconda3\\Scripts' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.'\n",
      "Add the path where 'waitress-serve.exe' is installed into gitbash's PATH as such:\n",
      "enter the following command in gitbash: nano ~/.bashrc\n",
      "add the path to 'waitress-serve.exe' to PATH using this command: export PATH=\"/path/to/waitress:$PATH\"\n",
      "close gitbash and open it again and you should be good to go\n",
      "Added by Bachar Kabalan\n",
      "\n",
      "Q: How to install WSL on Windows 10 and 11 ?\n",
      "A: It is quite simple, and you can follow these instructions here:\n",
      "https://www.youtube.com/watch?v=qYlgUDKKK5A&ab_channel=NeuralNine\n",
      "Make sure that you have “Virtual Machine Platform” feature activated in your Windows “Features”. To do that, search “features” in the research bar and see if the checkbox is selected. You also need to make sure that your system (in the bios) is able to virtualize. This is usually the case.\n",
      "In the Microsoft Store: look for ‘Ubuntu’ or ‘Debian’ (or any linux distribution you want) and install it\n",
      "Once it is downloaded, open the app and choose a username and a password (secured one). When you type your password, nothing will show in the window, which is normal: the writing is invisible.\n",
      "You are now inside of your linux system. You can test some commands such as “pwd”. You are not in your Windows system.\n",
      "To go to your windows system: you need to go back two times with cd ../.. And then go to the “mnt” directory with cd mnt. If you list here your files, you will see your disks. You can move to the desired folder, for example here I moved to the ML_Zoomcamp folder:\n",
      "Python should be already installed but you can check it by running sudo apt install python3 command.\n",
      "You can make your actual folder your default folder when you open your Ubuntu terminal with this command : echo \"cd ../../mnt/your/folder/path\" >> ~/.bashrc\n",
      "You can disable bell sounds (when you type something that does not exist for example) by modifying the inputrc file with this command: sudo vim /etc/inputrc\n",
      "You have to uncomment the set bell-style none line -> to do that, press the “i” keyboard letter (for insert) and go with your keyboard to this line. Delete the # and then press the Escape keyboard touch and finally press “:wq” to write (it saves your modifications) then quit.\n",
      "You can check that your modifications are taken into account by opening a new terminal (you can pin it to your task bar so you do not have to go to the Microsoft app each time).\n",
      "You will need to install pip by running this command sudo apt install python3-pip\n",
      "NB: I had this error message when trying to install pipenv (https://github.com/microsoft/WSL/issues/5663):\n",
      "/sbin/ldconfig.real: Can't link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1\n",
      "/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link\n",
      "So I had to create the following symbolic link:\n",
      "sudo ln -s /usr/lib/wsl/lib/libcuda.so.1 /usr/lib64/libcuda.so\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: How to install WSL on Windows 10 and 11 ?\n",
      "A: It is quite simple, and you can follow these instructions here:\n",
      "https://www.youtube.com/watch?v=qYlgUDKKK5A&ab_channel=NeuralNine\n",
      "Make sure that you have “Virtual Machine Platform” feature activated in your Windows “Features”. To do that, search “features” in the research bar and see if the checkbox is selected. You also need to make sure that your system (in the bios) is able to virtualize. This is usually the case.\n",
      "In the Microsoft Store: look for ‘Ubuntu’ or ‘Debian’ (or any linux distribution you want) and install it\n",
      "Once it is downloaded, open the app and choose a username and a password (secured one). When you type your password, nothing will show in the window, which is normal: the writing is invisible.\n",
      "You are now inside of your linux system. You can test some commands such as “pwd”. You are not in your Windows system.\n",
      "To go to your windows system: you need to go back two times with cd ../.. And then go to the “mnt” directory with cd mnt. If you list here your files, you will see your disks. You can move to the desired folder, for example here I moved to the ML_Zoomcamp folder:\n",
      "Python should be already installed but you can check it by running sudo apt install python3 command.\n",
      "You can make your actual folder your default folder when you open your Ubuntu terminal with this command : echo \"cd ../../mnt/your/folder/path\" >> ~/.bashrc\n",
      "You can disable bell sounds (when you type something that does not exist for example) by modifying the inputrc file with this command: sudo vim /etc/inputrc\n",
      "You have to uncomment the set bell-style none line -> to do that, press the “i” keyboard letter (for insert) and go with your keyboard to this line. Delete the # and then press the Escape keyboard touch and finally press “:wq” to write (it saves your modifications) then quit.\n",
      "You can check that your modifications are taken into account by opening a new terminal (you can pin it to your task bar so you do not have to go to the Microsoft app each time).\n",
      "You will need to install pip by running this command sudo apt install python3-pip\n",
      "NB: I had this error message when trying to install pipenv (https://github.com/microsoft/WSL/issues/5663):\n",
      "/sbin/ldconfig.real: Can't link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1\n",
      "/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link\n",
      "So I had to create the following symbolic link:\n",
      "sudo ln -s /usr/lib/wsl/lib/libcuda.so.1 /usr/lib64/libcuda.so\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: Fitting DictVectorizer on validation\n",
      "A: Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\n",
      "Edidiong Esu\n",
      "Below is an extract of Alexey's book explaining this point. Hope is useful\n",
      "When we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\n",
      "With this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven't seen. By not applying the fit to the validation model we can know how well it was trained.\n",
      "Below is an extract of Alexey's book explaining this point.\n",
      "Humberto Rodriguez\n",
      "There is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\n",
      "The correct way is to fit_transform the train set, and only transform the validation and test sets.\n",
      "Memoona Tahira\n",
      "\n",
      "Q: Fitting DictVectorizer on validation\n",
      "A: Validation dataset helps to validate models and prediction on unseen data. This helps get an estimate on its performance on fresh data. It helps optimize the model.\n",
      "Edidiong Esu\n",
      "Below is an extract of Alexey's book explaining this point. Hope is useful\n",
      "When we apply the fit method, this method is looking at the content of the df_train dictionaries we are passing to the DictVectorizer instance, and fit is figuring out (training) how to map the values of these dictionaries. If categorical, applies one-hot encoding, if numerical it will leave it as it is.\n",
      "With this context, if we apply the fit to the validation model, we are \"giving the answers\" and we are not letting the \"fit\" do its job for data that we haven't seen. By not applying the fit to the validation model we can know how well it was trained.\n",
      "Below is an extract of Alexey's book explaining this point.\n",
      "Humberto Rodriguez\n",
      "There is no need to initialize another instance of dictvectorizer after fitting it on the train set as it will overwrite what it learnt from being fit on the train data.\n",
      "The correct way is to fit_transform the train set, and only transform the validation and test sets.\n",
      "Memoona Tahira\n",
      "\n",
      "Q: How to use pandas to find standard deviation\n",
      "A: If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\n",
      "(Quinn Avila)\n",
      "\n",
      "Q: How to use pandas to find standard deviation\n",
      "A: If we have a list or series of data for example x = [1,2,3,4,5]. We can use pandas to find the standard deviation. We can pass our list into panda series and call standard deviation directly on the series pandas.Series(x).std().\n",
      "(Quinn Avila)\n",
      "\n",
      "Q: How to get all classification metrics?\n",
      "A: How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\n",
      "Use classification_report from sklearn. For more info check here.\n",
      "Abhishek N\n",
      "\n",
      "Q: How to get all classification metrics?\n",
      "A: How to get classification metrics - precision, recall, f1 score, accuracy simultaneously\n",
      "Use classification_report from sklearn. For more info check here.\n",
      "Abhishek N\n",
      "\n",
      "Q: I filled the form, but haven't received a confirmation email. Is it normal?\n",
      "A: The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\n",
      "If you unsubscribed from our newsletter, you won't get course related updates too.\n",
      "But don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\n",
      "\n",
      "Q: Error: Could not find a version that satisfies the requirement tflite_runtime (from versions:none)\n",
      "A: Problem: When trying to install tflite_runtime with\n",
      "!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\n",
      "one gets an error message above.\n",
      "Solution:\n",
      "fflite_runtime is only available for the os-python version combinations that can be found here: https://google-coral.github.io/py-repo/tflite-runtime/\n",
      "your combination must be missing here\n",
      "you can see if any of these work for you https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\n",
      "and install the needed one using pip\n",
      "eg\n",
      "pip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\n",
      "as it is done in the lectures code:\n",
      "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4\n",
      "Alternatively, use a virtual machine (with VM VirtualBox, for example) with a Linux system. The other way is to run a code at a virtual machine within cloud service, for example you can use Vertex AI Workbench at GCP (notebooks and terminals are provided there, so all tasks may be performed).\n",
      "Added by Alena Kniazeva, modified by Alex Litvinov\n",
      "\n",
      "Q: I filled the form, but haven't received a confirmation email. Is it normal?\n",
      "A: The process is automated now, so you should receive the email eventually. If you haven’t, check your promotions tab in Gmail as well as spam.\n",
      "If you unsubscribed from our newsletter, you won't get course related updates too.\n",
      "But don't worry, it’s not a problem. To make sure you don’t miss anything, join the #course-ml-zoomcamp channel in Slack and our telegram channel with announcements. This is enough to follow the course.\n",
      "\n",
      "Q: Error: Could not find a version that satisfies the requirement tflite_runtime (from versions:none)\n",
      "A: Problem: When trying to install tflite_runtime with\n",
      "!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\n",
      "one gets an error message above.\n",
      "Solution:\n",
      "fflite_runtime is only available for the os-python version combinations that can be found here: https://google-coral.github.io/py-repo/tflite-runtime/\n",
      "your combination must be missing here\n",
      "you can see if any of these work for you https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite\n",
      "and install the needed one using pip\n",
      "eg\n",
      "pip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\n",
      "as it is done in the lectures code:\n",
      "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4\n",
      "Alternatively, use a virtual machine (with VM VirtualBox, for example) with a Linux system. The other way is to run a code at a virtual machine within cloud service, for example you can use Vertex AI Workbench at GCP (notebooks and terminals are provided there, so all tasks may be performed).\n",
      "Added by Alena Kniazeva, modified by Alex Litvinov\n",
      "\n",
      "Q: How are numeric class labels determined in flow_from_directroy using binary class mode and what is meant by the single probability predicted by a binary Keras model:\n",
      "A: The command to read folders in the dataset in the tensorflow source code is:\n",
      "for subdir in sorted(os.listdir(directory)):\n",
      "…\n",
      "Reference: https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py, line 563\n",
      "This means folders will be read in alphabetical order. For example, in the case of a folder named dino, and another named dragon, dino will read first and will have class label 0, whereas dragon will be read in next and will have class label 1.\n",
      "When a Keras model predicts binary labels, it will only return one value, and this is the probability of class 1 in case of sigmoid activation function in the last dense layer with 2 neurons. The probability of class 0 can be found out by:\n",
      "prob(class(0)) = 1- prob(class(1))\n",
      "In case of using from_logits to get results, you will get two values for each of the labels.\n",
      "A prediction of 0.8 is saying the probability that the image has class label 1 (in this case dragon), is 0.8, and conversely we can infer the probability that the image has class label 0 is 0.2.\n",
      "(Added by Memoona Tahira)\n",
      "\n",
      "Q: How are numeric class labels determined in flow_from_directroy using binary class mode and what is meant by the single probability predicted by a binary Keras model:\n",
      "A: The command to read folders in the dataset in the tensorflow source code is:\n",
      "for subdir in sorted(os.listdir(directory)):\n",
      "…\n",
      "Reference: https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py, line 563\n",
      "This means folders will be read in alphabetical order. For example, in the case of a folder named dino, and another named dragon, dino will read first and will have class label 0, whereas dragon will be read in next and will have class label 1.\n",
      "When a Keras model predicts binary labels, it will only return one value, and this is the probability of class 1 in case of sigmoid activation function in the last dense layer with 2 neurons. The probability of class 0 can be found out by:\n",
      "prob(class(0)) = 1- prob(class(1))\n",
      "In case of using from_logits to get results, you will get two values for each of the labels.\n",
      "A prediction of 0.8 is saying the probability that the image has class label 1 (in this case dragon), is 0.8, and conversely we can infer the probability that the image has class label 0 is 0.2.\n",
      "(Added by Memoona Tahira)\n",
      "\n",
      "Q: wget is not recognized as an internal or external command\n",
      "A: If you get “wget is not recognized as an internal or external command”, you need to install it.\n",
      "On Ubuntu, run\n",
      "sudo apt-get install wget\n",
      "On Windows, the easiest way to install wget is to use Chocolatey:\n",
      "choco install wget\n",
      "Or you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\n",
      "On Mac, the easiest way to install wget is to use brew.\n",
      "Brew install wget\n",
      "Alternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\n",
      "python -m wget\n",
      "You need to install it with pip first:\n",
      "pip install wget\n",
      "And then in your python code, for example in your jupyter notebook, use:\n",
      "import wget\n",
      "wget.download(\"URL\")\n",
      "This should download whatever is at the URL in the same directory as your code.\n",
      "(Memoona Tahira)\n",
      "Alternatively, you can read a CSV file from a URL directly with pandas:\n",
      "url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\n",
      "df = pd.read_csv(url)\n",
      "Valid URL schemes include http, ftp, s3, gs, and file.\n",
      "In some cases you might need to bypass https checks:\n",
      "import ssl\n",
      "ssl._create_default_https_context = ssl._create_unverified_context\n",
      "Or you can use the built-in Python functionality for downloading the files:\n",
      "import urllib.request\n",
      "url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\n",
      "urllib.request.urlretrieve(url, \"housing.csv\")\n",
      "Urllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\n",
      "The urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\n",
      "On any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\n",
      "(Mohammad Emad Sharifi)\n",
      "\n",
      "Q: wget is not recognized as an internal or external command\n",
      "A: If you get “wget is not recognized as an internal or external command”, you need to install it.\n",
      "On Ubuntu, run\n",
      "sudo apt-get install wget\n",
      "On Windows, the easiest way to install wget is to use Chocolatey:\n",
      "choco install wget\n",
      "Or you can download a binary from here and put it to any location in your PATH (e.g. C:/tools/)\n",
      "On Mac, the easiest way to install wget is to use brew.\n",
      "Brew install wget\n",
      "Alternatively, you can use a Python wget library, but instead of simply using “wget” you’ll need eeeto use\n",
      "python -m wget\n",
      "You need to install it with pip first:\n",
      "pip install wget\n",
      "And then in your python code, for example in your jupyter notebook, use:\n",
      "import wget\n",
      "wget.download(\"URL\")\n",
      "This should download whatever is at the URL in the same directory as your code.\n",
      "(Memoona Tahira)\n",
      "Alternatively, you can read a CSV file from a URL directly with pandas:\n",
      "url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\n",
      "df = pd.read_csv(url)\n",
      "Valid URL schemes include http, ftp, s3, gs, and file.\n",
      "In some cases you might need to bypass https checks:\n",
      "import ssl\n",
      "ssl._create_default_https_context = ssl._create_unverified_context\n",
      "Or you can use the built-in Python functionality for downloading the files:\n",
      "import urllib.request\n",
      "url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\n",
      "urllib.request.urlretrieve(url, \"housing.csv\")\n",
      "Urllib.request.urlretrieve() is a standard Python library function available on all devices and platforms. URL requests and URL data retrieval are done with the urllib.request module.\n",
      "The urlretrieve() function allows you to download files from URLs and save them locally. Python programs use it to download files from the internet.\n",
      "On any Python-enabled device or platform, you can use the urllib.request.urlretrieve() function to download the file.\n",
      "(Mohammad Emad Sharifi)\n",
      "\n",
      "Q: Testing HTTP POST requests from command line using curl\n",
      "A: I wanted to have a fast and simple way to check if the HTTP POST requests are working just running a request from command line. This can be done running ‘curl’. \n",
      "(Used with WSL2 on Windows, should also work on Linux and MacOS)\n",
      "curl --json '<json data>' <url>\n",
      "# piping the structure to the command\n",
      "cat <json file path> | curl --json @- <url>\n",
      "echo '<json data>' | curl --json @- <url>\n",
      "# example using piping\n",
      "echo '{\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}'\\\n",
      "| curl --json @- http://localhost:9696/predict\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: Testing HTTP POST requests from command line using curl\n",
      "A: I wanted to have a fast and simple way to check if the HTTP POST requests are working just running a request from command line. This can be done running ‘curl’. \n",
      "(Used with WSL2 on Windows, should also work on Linux and MacOS)\n",
      "curl --json '<json data>' <url>\n",
      "# piping the structure to the command\n",
      "cat <json file path> | curl --json @- <url>\n",
      "echo '<json data>' | curl --json @- <url>\n",
      "# example using piping\n",
      "echo '{\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}'\\\n",
      "| curl --json @- http://localhost:9696/predict\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: [Errno 12] Cannot allocate memory in AWS Elastic Container Service\n",
      "A: In the Elastic Container Service task log, error “[Errno 12] Cannot allocate memory” showed up.\n",
      "Just increase the RAM and CPU in your task definition.\n",
      "Humberto Rodriguez\n",
      "\n",
      "Q: [Errno 12] Cannot allocate memory in AWS Elastic Container Service\n",
      "A: In the Elastic Container Service task log, error “[Errno 12] Cannot allocate memory” showed up.\n",
      "Just increase the RAM and CPU in your task definition.\n",
      "Humberto Rodriguez\n",
      "\n",
      "Q: In case you are using mac os and having trouble with WGET\n",
      "A: Wget doesn't ship with macOS, so there are other alternatives to use.\n",
      "No worries, we got curl:\n",
      "example:\n",
      "curl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "Explanations:\n",
      "curl: a utility for retrieving information from the internet.\n",
      "-o: Tell it to store the result as a file.\n",
      "filename: You choose the file's name.\n",
      "Links: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\n",
      "More about it at:\n",
      "Curl Documentation\n",
      "Added by David Espejo\n",
      "\n",
      "Q: In case you are using mac os and having trouble with WGET\n",
      "A: Wget doesn't ship with macOS, so there are other alternatives to use.\n",
      "No worries, we got curl:\n",
      "example:\n",
      "curl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "Explanations:\n",
      "curl: a utility for retrieving information from the internet.\n",
      "-o: Tell it to store the result as a file.\n",
      "filename: You choose the file's name.\n",
      "Links: Put the web address (URL) here, and cURL will extract data from it and save it under the name you provide.\n",
      "More about it at:\n",
      "Curl Documentation\n",
      "Added by David Espejo\n",
      "\n",
      "Q: Shuffling the initial dataset using pandas built-in function\n",
      "A: It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\n",
      "Setting frac=1 will result in returning a shuffled version of the complete Dataset.\n",
      "Setting random_state=seed will result in the same randomization as used in the course resources.\n",
      "df_shuffled = df.sample(frac=1, random_state=seed)\n",
      "df_shuffled.reset_index(drop=True, inplace=True)\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: Shuffling the initial dataset using pandas built-in function\n",
      "A: It is possible to do the shuffling of the dataset with the pandas built-in function pandas.DataFrame.sample.The complete dataset can be shuffled including resetting the index with the following commands:\n",
      "Setting frac=1 will result in returning a shuffled version of the complete Dataset.\n",
      "Setting random_state=seed will result in the same randomization as used in the course resources.\n",
      "df_shuffled = df.sample(frac=1, random_state=seed)\n",
      "df_shuffled.reset_index(drop=True, inplace=True)\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: How to combine train and validation datasets\n",
      "A: Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\n",
      "df_train_combined = pd.concat([df_train, df_val])\n",
      "y_train = np.concatenate((y_train, y_val), axis=0)\n",
      "(George Chizhmak)\n",
      "\n",
      "Q: How to calculate Root Mean Squared Error?\n",
      "A: We can use sklearn & numpy packages to calculate Root Mean Squared Error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "Rmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\n",
      "Added by Radikal Lukafiardi\n",
      "You can also refer to Alexey’s notebook for Week 2:\n",
      "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\n",
      "which includes the following code:\n",
      "def rmse(y, y_pred):\n",
      "error = y_pred - y\n",
      "mse = (error ** 2).mean()\n",
      "return np.sqrt(mse)\n",
      "(added by Rileen Sinha)\n",
      "\n",
      "Q: How to get started with Week 6?\n",
      "A: Week 6 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/06-trees/homework.md\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "HW 4 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/04-evaluation/homework_4.ipynb\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "YouTube Link: 6.X --- https://www.youtube.com/watch?v=GJGmlfZoCoU&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57\n",
      "FAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\n",
      "~~~Nukta Bhatia~~~\n",
      "\n",
      "Q: How to get started with Week 8?\n",
      "A: TODO\n",
      "\n",
      "Q: How to use Kaggle for Deep Learning?\n",
      "A: Create or import your notebook into Kaggle.\n",
      "Click on the Three dots at the top right hand side\n",
      "Click on Accelerator\n",
      "Choose T4 GPU\n",
      "Khurram Majeed\n",
      "\n",
      "Q: How to get started with Week 9?\n",
      "A: TODO\n",
      "\n",
      "Q: How to get started with Week 10?\n",
      "A: TODO\n",
      "\n",
      "Q: How to install Tensorflow in Ubuntu WSL2\n",
      "A: Running a CNN on your CPU can take a long time and once you’ve run out of free time on some cloud providers, it’s time to pay up. Both can be tackled by installing tensorflow with CUDA support on your local machine if you have the right hardware.\n",
      "I was able to get it working by using the following resources:\n",
      "CUDA on WSL :: CUDA Toolkit Documentation (nvidia.com)\n",
      "Install TensorFlow with pip\n",
      "Start Locally | PyTorch\n",
      "I included the link to PyTorch so that you can get that one installed and working too while everything is fresh on your mind. Just select your options, and for Computer Platform, I chose CUDA 11.7 and it worked for me.\n",
      "Added by Martin Uribe\n",
      "\n",
      "Q: How to conduct peer reviews for projects?\n",
      "A: Answer: Previous cohorts projects page has instructions (youtube).\n",
      "https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project\n",
      "Alexey and his team will compile a g-sheet with links to submitted projects with our hashed emails (just like when we check leaderboard for homework) that are ours to review within the evaluation deadline.\n",
      "~~~ Added by Nukta Bhatia ~~~\n",
      "\n",
      "Q: How to use wget with Google Colab?\n",
      "A: Install w get:\n",
      "!which wget\n",
      "Download data:\n",
      "!wget -P /content/drive/My\\ Drive/Downloads/ URL\n",
      "(added by Paulina Hernandez)\n",
      "\n",
      "Q: How to combine train and validation datasets\n",
      "A: Use ‘pandas.concat’ function (https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to combine two dataframes. To combine two numpy arrays use numpy.concatenate (https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) function. So the code would be as follows:\n",
      "df_train_combined = pd.concat([df_train, df_val])\n",
      "y_train = np.concatenate((y_train, y_val), axis=0)\n",
      "(George Chizhmak)\n",
      "\n",
      "Q: How to calculate Root Mean Squared Error?\n",
      "A: We can use sklearn & numpy packages to calculate Root Mean Squared Error\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "Rmse = np.sqrt(mean_squared_error(y_pred, y_val/ytest)\n",
      "Added by Radikal Lukafiardi\n",
      "You can also refer to Alexey’s notebook for Week 2:\n",
      "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/chapter-02-car-price/02-carprice.ipynb\n",
      "which includes the following code:\n",
      "def rmse(y, y_pred):\n",
      "error = y_pred - y\n",
      "mse = (error ** 2).mean()\n",
      "return np.sqrt(mse)\n",
      "(added by Rileen Sinha)\n",
      "\n",
      "Q: How to get started with Week 6?\n",
      "A: Week 6 HW: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/06-trees/homework.md\n",
      "All HWs: https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2023/\n",
      "HW 4 Solution: https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/04-evaluation/homework_4.ipynb\n",
      "Evaluation Matrix: https://docs.google.com/spreadsheets/d/e/2PACX-1vQCwqAtkjl07MTW-SxWUK9GUvMQ3Pv_fF8UadcuIYLgHa0PlNu9BRWtfLgivI8xSCncQs82HDwGXSm3/pubhtml\n",
      "GitHub for theory: https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp\n",
      "YouTube Link: 6.X --- https://www.youtube.com/watch?v=GJGmlfZoCoU&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57\n",
      "FAQs: https://docs.google.com/document/d/1LpPanc33QJJ6BSsyxVg-pWNMplal84TdZtq10naIhD8/edit#heading=h.lpz96zg7l47j\n",
      "~~~Nukta Bhatia~~~\n",
      "\n",
      "Q: How to get started with Week 8?\n",
      "A: TODO\n",
      "\n",
      "Q: How to use Kaggle for Deep Learning?\n",
      "A: Create or import your notebook into Kaggle.\n",
      "Click on the Three dots at the top right hand side\n",
      "Click on Accelerator\n",
      "Choose T4 GPU\n",
      "Khurram Majeed\n",
      "\n",
      "Q: How to get started with Week 9?\n",
      "A: TODO\n",
      "\n",
      "Q: How to get started with Week 10?\n",
      "A: TODO\n",
      "\n",
      "Q: How to install Tensorflow in Ubuntu WSL2\n",
      "A: Running a CNN on your CPU can take a long time and once you’ve run out of free time on some cloud providers, it’s time to pay up. Both can be tackled by installing tensorflow with CUDA support on your local machine if you have the right hardware.\n",
      "I was able to get it working by using the following resources:\n",
      "CUDA on WSL :: CUDA Toolkit Documentation (nvidia.com)\n",
      "Install TensorFlow with pip\n",
      "Start Locally | PyTorch\n",
      "I included the link to PyTorch so that you can get that one installed and working too while everything is fresh on your mind. Just select your options, and for Computer Platform, I chose CUDA 11.7 and it worked for me.\n",
      "Added by Martin Uribe\n",
      "\n",
      "Q: How to conduct peer reviews for projects?\n",
      "A: Answer: Previous cohorts projects page has instructions (youtube).\n",
      "https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project\n",
      "Alexey and his team will compile a g-sheet with links to submitted projects with our hashed emails (just like when we check leaderboard for homework) that are ours to review within the evaluation deadline.\n",
      "~~~ Added by Nukta Bhatia ~~~\n",
      "\n",
      "Q: Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=)\n",
      "A: If you’re getting TypeError:\n",
      "“TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>”,\n",
      "probably you’ve done this:\n",
      "features = dv.get_feature_names_out()\n",
      "It gets you np.ndarray instead of list. Converting to list list(features) will not fix this, read below.\n",
      "If you’re getting ValueError:\n",
      "“ValueError: feature_names must be string, and may not contain [, ] or <”,\n",
      "probably you’ve either done:\n",
      "features = list(dv.get_feature_names_out())\n",
      "or:\n",
      "features = dv.feature_names_\n",
      "reason is what you get from DictVectorizer here looks like this:\n",
      "['households',\n",
      "'housing_median_age',\n",
      "'latitude',\n",
      "'longitude',\n",
      "'median_income',\n",
      "'ocean_proximity=<1H OCEAN',\n",
      "'ocean_proximity=INLAND',\n",
      "'population',\n",
      "'total_bedrooms',\n",
      "'total_rooms']\n",
      "it has symbols XGBoost doesn’t like ([, ] or <).\n",
      "What you can do, is either do not specify “feature_names=” while creating xgb.DMatrix or:\n",
      "import re\n",
      "features = dv.feature_names_\n",
      "pattern = r'[\\[\\]<>]'\n",
      "features = [re.sub(pattern, '  ', f) for f in features]\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=)\n",
      "A: If you’re getting TypeError:\n",
      "“TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>”,\n",
      "probably you’ve done this:\n",
      "features = dv.get_feature_names_out()\n",
      "It gets you np.ndarray instead of list. Converting to list list(features) will not fix this, read below.\n",
      "If you’re getting ValueError:\n",
      "“ValueError: feature_names must be string, and may not contain [, ] or <”,\n",
      "probably you’ve either done:\n",
      "features = list(dv.get_feature_names_out())\n",
      "or:\n",
      "features = dv.feature_names_\n",
      "reason is what you get from DictVectorizer here looks like this:\n",
      "['households',\n",
      "'housing_median_age',\n",
      "'latitude',\n",
      "'longitude',\n",
      "'median_income',\n",
      "'ocean_proximity=<1H OCEAN',\n",
      "'ocean_proximity=INLAND',\n",
      "'population',\n",
      "'total_bedrooms',\n",
      "'total_rooms']\n",
      "it has symbols XGBoost doesn’t like ([, ] or <).\n",
      "What you can do, is either do not specify “feature_names=” while creating xgb.DMatrix or:\n",
      "import re\n",
      "features = dv.feature_names_\n",
      "pattern = r'[\\[\\]<>]'\n",
      "features = [re.sub(pattern, '  ', f) for f in features]\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: You are using windows. Conda environment. You then use waitress instead of gunicorn. After a few runs, suddenly mlflow server fails to run.\n",
      "A: Ans: Pip uninstall waitress mflow. Then reinstall just mlflow. By this time you should have successfully built your docker image so you dont need to reinstall waitress. All good. Happy learning.\n",
      "Added by 🅱🅻🅰🆀\n",
      "\n",
      "Q: You are using windows. Conda environment. You then use waitress instead of gunicorn. After a few runs, suddenly mlflow server fails to run.\n",
      "A: Ans: Pip uninstall waitress mflow. Then reinstall just mlflow. By this time you should have successfully built your docker image so you dont need to reinstall waitress. All good. Happy learning.\n",
      "Added by 🅱🅻🅰🆀\n",
      "\n",
      "Q: Docker Temporary failure in name resolution\n",
      "A: Add the next lines to vim /etc/docker/daemon.json\n",
      "{\n",
      "\"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\n",
      "}\n",
      "Then, restart docker:  sudo service docker restart\n",
      "Ibai Irastorza\n",
      "\n",
      "Q: Docker Temporary failure in name resolution\n",
      "A: Add the next lines to vim /etc/docker/daemon.json\n",
      "{\n",
      "\"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\n",
      "}\n",
      "Then, restart docker:  sudo service docker restart\n",
      "Ibai Irastorza\n",
      "\n",
      "Q: Object of type float32 is not JSON serializable\n",
      "A: Problem:\n",
      "While passing local testing of the lambda function without issues, trying to test the same input with a running docker instance results in an error message like\n",
      "{‘errorMessage’: ‘Unable to marshal response: Object of type float32 is not JSON serializable’, ‘errorType’: ‘Runtime.MarshalError’, ‘requestId’: ‘f155492c-9af2-4d04-b5a4-639548b7c7ac’, ‘stackTrace’: []}\n",
      "This happens when a model (in this case the dino vs dragon model) returns individual estimation values as numpy float32 values (arrays). They need to be converted individually to base-Python floats in order to become “serializable”.\n",
      "Solution:\n",
      "In my particular case, I set up the dino vs dragon model in such a way as to return a label + predicted probability for each class as follows (below is a two-line extract of function predict() in the lambda_function.py):\n",
      "preds = [interpreter.get_tensor(output_index)[0][0], \\\n",
      "1-interpreter.get_tensor(output_index)[0][0]]\n",
      "In which case the above described solution will look like this:\n",
      "preds = [float(interpreter.get_tensor(output_index)[0][0]), \\\n",
      "float(1-interpreter.get_tensor(output_index)[0][0])]\n",
      "The rest can be made work by following the chapter 9 (and/or chapter 5!) lecture videos step by step.\n",
      "Added by Konrad Muehlberg\n",
      "\n",
      "Q: Object of type float32 is not JSON serializable\n",
      "A: Problem:\n",
      "While passing local testing of the lambda function without issues, trying to test the same input with a running docker instance results in an error message like\n",
      "{‘errorMessage’: ‘Unable to marshal response: Object of type float32 is not JSON serializable’, ‘errorType’: ‘Runtime.MarshalError’, ‘requestId’: ‘f155492c-9af2-4d04-b5a4-639548b7c7ac’, ‘stackTrace’: []}\n",
      "This happens when a model (in this case the dino vs dragon model) returns individual estimation values as numpy float32 values (arrays). They need to be converted individually to base-Python floats in order to become “serializable”.\n",
      "Solution:\n",
      "In my particular case, I set up the dino vs dragon model in such a way as to return a label + predicted probability for each class as follows (below is a two-line extract of function predict() in the lambda_function.py):\n",
      "preds = [interpreter.get_tensor(output_index)[0][0], \\\n",
      "1-interpreter.get_tensor(output_index)[0][0]]\n",
      "In which case the above described solution will look like this:\n",
      "preds = [float(interpreter.get_tensor(output_index)[0][0]), \\\n",
      "float(1-interpreter.get_tensor(output_index)[0][0])]\n",
      "The rest can be made work by following the chapter 9 (and/or chapter 5!) lecture videos step by step.\n",
      "Added by Konrad Muehlberg\n",
      "\n",
      "Q: Save Docker Image to local machine and view contents\n",
      "A: The docker image can be saved/exported to tar format in local machine using the below command:\n",
      "docker image save <image-name> -o <name-of-tar-file.tar>\n",
      "The individual layers of the docker image for the filesystem content can be viewed by extracting the layer.tar present in the <name-of-tar-file.tar> created from above.\n",
      "Sumeet Lalla\n",
      "\n",
      "Q: Save Docker Image to local machine and view contents\n",
      "A: The docker image can be saved/exported to tar format in local machine using the below command:\n",
      "docker image save <image-name> -o <name-of-tar-file.tar>\n",
      "The individual layers of the docker image for the filesystem content can be viewed by extracting the layer.tar present in the <name-of-tar-file.tar> created from above.\n",
      "Sumeet Lalla\n",
      "\n",
      "Q: Does the github repository need to be public?\n",
      "A: Yes. Whoever corrects the homework will only be able to access the link if the repository is public.\n",
      "(added by Tano Bugelli)\n",
      "How to install Conda environment in my local machine?\n",
      "Which ide is recommended for machine learning?\n",
      "\n",
      "Q: What does pandas.DataFrame.info() do?\n",
      "A: Answer:\n",
      "It prints the information about the dataset like:\n",
      "Index datatype\n",
      "No. of entries\n",
      "Column information with not-null count and datatype\n",
      "Memory usage by dataset\n",
      "We use it as:\n",
      "df.info()\n",
      "(Added by Aadarsha Shrestha & Emoghena Itakpe)\n",
      "\n",
      "Q: What does pandas.DataFrame.info() do?\n",
      "A: Answer:\n",
      "It prints the information about the dataset like:\n",
      "Index datatype\n",
      "No. of entries\n",
      "Column information with not-null count and datatype\n",
      "Memory usage by dataset\n",
      "We use it as:\n",
      "df.info()\n",
      "(Added by Aadarsha Shrestha & Emoghena Itakpe)\n",
      "\n",
      "Q: Free cloud alternatives\n",
      "A: Q: Hii folks, I tried deploying my docker image on Render, but it won't I get SIGTERM everytime.\n",
      "I think .5GB RAM is not enough, is there any other free alternative available ?\n",
      "A: aws (amazon), gcp (google), saturn.\n",
      "Both aws and gcp give microinstance for free for a VERY long time, and a bunch more free stuff.\n",
      "Saturn even provides free GPU instances. Recent promo link from mlzoomcamp for Saturn:\n",
      "“You can sign up here: https://bit.ly/saturn-mlzoomcamp\n",
      "When you sign up, write in the chat box that you're an ML Zoomcamp student and you should get extra GPU hours (something like 150)”\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Free cloud alternatives\n",
      "A: Q: Hii folks, I tried deploying my docker image on Render, but it won't I get SIGTERM everytime.\n",
      "I think .5GB RAM is not enough, is there any other free alternative available ?\n",
      "A: aws (amazon), gcp (google), saturn.\n",
      "Both aws and gcp give microinstance for free for a VERY long time, and a bunch more free stuff.\n",
      "Saturn even provides free GPU instances. Recent promo link from mlzoomcamp for Saturn:\n",
      "“You can sign up here: https://bit.ly/saturn-mlzoomcamp\n",
      "When you sign up, write in the chat box that you're an ML Zoomcamp student and you should get extra GPU hours (something like 150)”\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Do we need to apply regularization techniques always? Or only in certain scenarios?\n",
      "A: The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\n",
      "(Daniel Muñoz Viveros)\n",
      "\n",
      "Q: Do we need to apply regularization techniques always? Or only in certain scenarios?\n",
      "A: The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed.\n",
      "(Daniel Muñoz Viveros)\n",
      "\n",
      "Q: Bind for 127.0.0.1:5000 showing error\n",
      "A: I was getting the error on client side with this\n",
      "Client Side:\n",
      "File \"C:\\python\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen …………………..\n",
      "raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Sevrer Side:\n",
      "It showed error for gunicorn\n",
      "The waitress  cmd was running smoothly from server side\n",
      "Solution:\n",
      "Use the ip-address as 0.0.0.0:8000 or 0.0.0.0:9696.They are the ones which do work max times\n",
      "Aamir Wani\n",
      "\n",
      "Q: Bind for 127.0.0.1:5000 showing error\n",
      "A: I was getting the error on client side with this\n",
      "Client Side:\n",
      "File \"C:\\python\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen …………………..\n",
      "raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Sevrer Side:\n",
      "It showed error for gunicorn\n",
      "The waitress  cmd was running smoothly from server side\n",
      "Solution:\n",
      "Use the ip-address as 0.0.0.0:8000 or 0.0.0.0:9696.They are the ones which do work max times\n",
      "Aamir Wani\n",
      "\n",
      "Q: Understanding RMSE and how to calculate RMSE score\n",
      "A: The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model's predicted values and the actual observed values, offering insight into the model's ability to accurately forecast the target variable. To calculate RMSE score:\n",
      "Libraries needed\n",
      "import numpy as np\n",
      "from sklearn.metrics import mean_squared_error\n",
      "mse = mean_squared_error(actual_values, predicted_values)\n",
      "rmse = np.sqrt(mse)\n",
      "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
      "(Aminat Abolade)\n",
      "\n",
      "Q: How  to Disable/avoid Warnings in Jupyter Notebooks\n",
      "A: The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\n",
      "Import warnings\n",
      "warnings.filterwarnings(“ignore”)\n",
      "Krishna Anand\n",
      "\n",
      "Q: How to use Google Colab for Deep Learning?\n",
      "A: Create or import your notebook into Google Colab.\n",
      "Click on the Drop Down at the top right hand side\n",
      "Click on “Change runtime type”\n",
      "Choose T4 GPU\n",
      "Khurram Majeed\n",
      "\n",
      "Q: How to install CUDA & cuDNN on Ubuntu 22.04\n",
      "A: In order to run tensorflow with gpu on your local machine you’ll need to setup cuda and cudnn.\n",
      "The process can be overwhelming. Here’s a simplified guide\n",
      "Osman Ali\n",
      "\n",
      "Q: How to get feature importance for XGboost model\n",
      "A: Using model.feature_importances_ can gives you an error:\n",
      "AttributeError: 'Booster' object has no attribute 'feature_importances_'\n",
      "Answer: if you train the model like this: model = xgb.train you should use get_score() instead\n",
      "Ekaterina Kutovaia\n",
      "\n",
      "Q: Understanding RMSE and how to calculate RMSE score\n",
      "A: The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model's predicted values and the actual observed values, offering insight into the model's ability to accurately forecast the target variable. To calculate RMSE score:\n",
      "Libraries needed\n",
      "import numpy as np\n",
      "from sklearn.metrics import mean_squared_error\n",
      "mse = mean_squared_error(actual_values, predicted_values)\n",
      "rmse = np.sqrt(mse)\n",
      "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
      "(Aminat Abolade)\n",
      "\n",
      "Q: How  to Disable/avoid Warnings in Jupyter Notebooks\n",
      "A: The warnings on the jupyter notebooks can be disabled/ avoided with the following comments:\n",
      "Import warnings\n",
      "warnings.filterwarnings(“ignore”)\n",
      "Krishna Anand\n",
      "\n",
      "Q: How to use Google Colab for Deep Learning?\n",
      "A: Create or import your notebook into Google Colab.\n",
      "Click on the Drop Down at the top right hand side\n",
      "Click on “Change runtime type”\n",
      "Choose T4 GPU\n",
      "Khurram Majeed\n",
      "\n",
      "Q: How to install CUDA & cuDNN on Ubuntu 22.04\n",
      "A: In order to run tensorflow with gpu on your local machine you’ll need to setup cuda and cudnn.\n",
      "The process can be overwhelming. Here’s a simplified guide\n",
      "Osman Ali\n",
      "\n",
      "Q: How to get feature importance for XGboost model\n",
      "A: Using model.feature_importances_ can gives you an error:\n",
      "AttributeError: 'Booster' object has no attribute 'feature_importances_'\n",
      "Answer: if you train the model like this: model = xgb.train you should use get_score() instead\n",
      "Ekaterina Kutovaia\n",
      "\n",
      "Q: Deep dive into normal equation for regression\n",
      "A: I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression\n",
      "\n",
      "Q: Deep dive into normal equation for regression\n",
      "A: I found this video pretty usual for understanding how we got the normal form with linear regression Normal Equation Derivation for Regression\n",
      "\n",
      "Q: `TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> ` when training xgboost model.\n",
      "A: If you’re getting this error, It is likely because the feature names in dv.get_feature_names_out() are a np.ndarray instead of a list so you have to convert them into a list by using the to_list() method.\n",
      "Ali Osman\n",
      "\n",
      "Q: `TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'> ` when training xgboost model.\n",
      "A: If you’re getting this error, It is likely because the feature names in dv.get_feature_names_out() are a np.ndarray instead of a list so you have to convert them into a list by using the to_list() method.\n",
      "Ali Osman\n",
      "\n",
      "Q: Error UnidentifiedImageError: cannot identify image file\n",
      "A: In deploying model part, I wanted to test my model locally on a test-image data and I had this silly error after the following command:\n",
      "url = 'https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg'\n",
      "X = preprocessor.from_url(url)\n",
      "I got the error:\n",
      "UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f797010a590>\n",
      "Solution:\n",
      "Add ?raw=true after .jpg in url. E.g. as below\n",
      "url = ‘https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true’\n",
      "Bhaskar Sarma\n",
      "\n",
      "Q: Error UnidentifiedImageError: cannot identify image file\n",
      "A: In deploying model part, I wanted to test my model locally on a test-image data and I had this silly error after the following command:\n",
      "url = 'https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg'\n",
      "X = preprocessor.from_url(url)\n",
      "I got the error:\n",
      "UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f797010a590>\n",
      "Solution:\n",
      "Add ?raw=true after .jpg in url. E.g. as below\n",
      "url = ‘https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true’\n",
      "Bhaskar Sarma\n",
      "\n",
      "Q: Can I do the course in other languages, like R or Scala?\n",
      "A: Technically, yes. Advisable? Not really. Reasons:\n",
      "Some homework(s) asks for specific python library versions.\n",
      "Answers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)\n",
      "And as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?\n",
      "You can create a separate repo using course’s lessons but written in other languages for your own learnings, but not advisable for submissions.\n",
      "tx[source]\n",
      "\n",
      "Q: TypeError: Descriptors cannot not be created directly.\n",
      "A: Problem description\n",
      "I was getting the below error message when I run gateway.py after modifying the code & creating virtual environment in  video 10.3 :\n",
      "File \"C:\\Users\\Asia\\Data_Science_Code\\Zoompcamp\\Kubernetes\\gat.py\", line 9, in <module>\n",
      "from tensorflow_serving.apis import predict_pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow_serving\\apis\\predict_pb2.py\", line 14, in <module>\n",
      "from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py\", line 14, in <module>\n",
      "from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py\", line 14, in <module>\n",
      "from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py\", line 36, in <module>\n",
      "_descriptor.FieldDescriptor(\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 560, in __new__\n",
      "_message.Message._CheckCalledFromGeneratedFile()\n",
      "TypeError: Descriptors cannot not be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      "1. Downgrade the protobuf package to 3.20.x or lower.\n",
      "2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "Solution description:\n",
      "Issue has been resolved by downgrading protobuf to version 3.20.1.\n",
      "pipenv install protobuf==3.20.1\n",
      "Asia Saeed\n",
      "\n",
      "Q: TypeError: Descriptors cannot not be created directly.\n",
      "A: Problem description\n",
      "I was getting the below error message when I run gateway.py after modifying the code & creating virtual environment in  video 10.3 :\n",
      "File \"C:\\Users\\Asia\\Data_Science_Code\\Zoompcamp\\Kubernetes\\gat.py\", line 9, in <module>\n",
      "from tensorflow_serving.apis import predict_pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow_serving\\apis\\predict_pb2.py\", line 14, in <module>\n",
      "from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py\", line 14, in <module>\n",
      "from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py\", line 14, in <module>\n",
      "from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py\", line 36, in <module>\n",
      "_descriptor.FieldDescriptor(\n",
      "File \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 560, in __new__\n",
      "_message.Message._CheckCalledFromGeneratedFile()\n",
      "TypeError: Descriptors cannot not be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      "1. Downgrade the protobuf package to 3.20.x or lower.\n",
      "2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "Solution description:\n",
      "Issue has been resolved by downgrading protobuf to version 3.20.1.\n",
      "pipenv install protobuf==3.20.1\n",
      "Asia Saeed\n",
      "\n",
      "Q: Getting day of the year from day and month column\n",
      "A: Problem description: I have one column day_of_the_month . It has values 1, 2, 20, 25 etc. and int . I have a second column month_of_the_year. It has values jan, feb, ..dec. and are string. I want to convert these two columns into one column day_of_the_year and I want them to be int. 2 and jan should give me 2, i.e. 2nd day of the year, 1 and feb should give me 32, i.e. 32 nd day of the year. What is the simplest pandas-way to do it?\n",
      "Solution description:\n",
      "convert dtype in day_of_the_month column from int to str with df['day_of_the_month'] = df['day_of_the_month'].map(str)\n",
      "convert month_of_the_year column in jan, feb ...,dec into 1,2, ..,12 string using map()\n",
      "convert day and month into a datetime object with:\n",
      "df['date_formatted'] = pd.to_datetime(\n",
      "dict(\n",
      "year='2055',\n",
      "month=df['month'],\n",
      "day=df['day']\n",
      ")\n",
      ")\n",
      "get day of year with: df['day_of_year']=df['date_formatted'].dt.dayofyear\n",
      "(Bhaskar Sarma)\n",
      "\n",
      "Q: How to setup TensorFlow with GPU support on Ubuntu?\n",
      "A: Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/\n",
      "\n",
      "Q: How to identify the shape of dataset in Pandas\n",
      "A: There are many ways to identify the shape of dataset, one of them is using .shape attribute!\n",
      "df.shape\n",
      "df.shape[0] # for identify the number of rows\n",
      "df.shape[1] # for identify the number of columns\n",
      "Added by Radikal Lukafiardi\n",
      "\n",
      "Q: How to setup TensorFlow with GPU support on Ubuntu?\n",
      "A: Here is an article that worked for me: https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/\n",
      "\n",
      "Q: How to identify the shape of dataset in Pandas\n",
      "A: There are many ways to identify the shape of dataset, one of them is using .shape attribute!\n",
      "df.shape\n",
      "df.shape[0] # for identify the number of rows\n",
      "df.shape[1] # for identify the number of columns\n",
      "Added by Radikal Lukafiardi\n",
      "\n",
      "Q: [pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies\n",
      "A: Problem: If you run pipenv install and get this message. Maybe manually change Pipfile and Pipfile.lock.\n",
      "Solution: Run: ` pipenv lock` for fix this problem and dependency files\n",
      "Alejandro Aponte\n",
      "\n",
      "Q: [pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies\n",
      "A: Problem: If you run pipenv install and get this message. Maybe manually change Pipfile and Pipfile.lock.\n",
      "Solution: Run: ` pipenv lock` for fix this problem and dependency files\n",
      "Alejandro Aponte\n",
      "\n",
      "Q: Uploading the homework to Github\n",
      "A: This is my first time using Github to upload a code. I was getting the below error message when I type\n",
      "git push -u origin master:\n",
      "error: src refspec master does not match any\n",
      "error: failed to push some refs to 'https://github.com/XXXXXX/1st-Homework.git'\n",
      "Solution:\n",
      "The error message got fixed by running below commands:\n",
      "git commit -m \"initial commit\"\n",
      "git push origin main\n",
      "If this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\n",
      "(Asia Saeed)\n",
      "You can also use the “upload file” functionality from GitHub for that\n",
      "If you write your code on Google colab you can also directly share it on your Github.\n",
      "(By Pranab Sarma)\n",
      "\n",
      "Q: Uploading the homework to Github\n",
      "A: This is my first time using Github to upload a code. I was getting the below error message when I type\n",
      "git push -u origin master:\n",
      "error: src refspec master does not match any\n",
      "error: failed to push some refs to 'https://github.com/XXXXXX/1st-Homework.git'\n",
      "Solution:\n",
      "The error message got fixed by running below commands:\n",
      "git commit -m \"initial commit\"\n",
      "git push origin main\n",
      "If this is your first time to use Github, you will find a great & straightforward tutorial in this link https://dennisivy.com/github-quickstart\n",
      "(Asia Saeed)\n",
      "You can also use the “upload file” functionality from GitHub for that\n",
      "If you write your code on Google colab you can also directly share it on your Github.\n",
      "(By Pranab Sarma)\n",
      "\n",
      "Q: Version-conflict in pipenv\n",
      "A: Problem description:\n",
      "In video 5.5 when I do pipenv shell and then pipenv run gunicorn --bind 0.0.0.0:9696 predict:app, I get the following warning:\n",
      "UserWarning: Trying to unpickle estimator DictVectorizer from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Solution description:\n",
      "When you create a virtual env, you should use the same version of Scikit-Learn that you used for training the model on this case it's 1.1.1. There is version conflicts so we need to make sure our model and dv files are created from the version we are using for the project.\n",
      "Bhaskar Sarma\n",
      "\n",
      "Q: Version-conflict in pipenv\n",
      "A: Problem description:\n",
      "In video 5.5 when I do pipenv shell and then pipenv run gunicorn --bind 0.0.0.0:9696 predict:app, I get the following warning:\n",
      "UserWarning: Trying to unpickle estimator DictVectorizer from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "Solution description:\n",
      "When you create a virtual env, you should use the same version of Scikit-Learn that you used for training the model on this case it's 1.1.1. There is version conflicts so we need to make sure our model and dv files are created from the version we are using for the project.\n",
      "Bhaskar Sarma\n",
      "\n",
      "Q: Requests Error: No connection adapters were found for 'localhost:9696/predict'.\n",
      "A: You need to include the protocol scheme: 'http://localhost:9696/predict'.\n",
      "Without the http:// part, requests has no idea how to connect to the remote server.\n",
      "Note that the protocol scheme must be all lowercase; if your URL starts with HTTP:// for example, it won’t find the http:// connection adapter either.\n",
      "Added by George Chizhmak\n",
      "\n",
      "Q: Requests Error: No connection adapters were found for 'localhost:9696/predict'.\n",
      "A: You need to include the protocol scheme: 'http://localhost:9696/predict'.\n",
      "Without the http:// part, requests has no idea how to connect to the remote server.\n",
      "Note that the protocol scheme must be all lowercase; if your URL starts with HTTP:// for example, it won’t find the http:// connection adapter either.\n",
      "Added by George Chizhmak\n",
      "\n",
      "Q: Keras model *.h5 doesn’t load. Error: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`\n",
      "A: Solution: add compile = False to the load_model function\n",
      "keras.models.load_model('model_name.h5', compile=False)\n",
      "Nadia Paz\n",
      "\n",
      "Q: Keras model *.h5 doesn’t load. Error: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`\n",
      "A: Solution: add compile = False to the load_model function\n",
      "keras.models.load_model('model_name.h5', compile=False)\n",
      "Nadia Paz\n",
      "\n",
      "Q: How to avoid Value errors with array shapes in homework?\n",
      "A: First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\n",
      "Dimension Mismatch\n",
      "To perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\n",
      "Added by Leah Gotladera\n",
      "\n",
      "Q: How to evaluate feature importance for numerical variables with AUC?\n",
      "A: You can use roc_auc_score function from sklearn.metrics module and pass the vector of the target variable (‘above_average’) as the first argument and the vector of feature values as the second one. This function will return AUC score for the feature that was passed as a second argument.\n",
      "(Denys Soloviov)\n",
      "\n",
      "Q: How to get the training and validation metrics from XGBoost?\n",
      "A: During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.\n",
      "We can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\n",
      "Added by Daniel Coronel\n",
      "\n",
      "Q: How to avoid Value errors with array shapes in homework?\n",
      "A: First of all use np.dot for matrix multiplication. When you compute matrix-matrix multiplication you should understand that order of multiplying is crucial and affects the result of the multiplication!\n",
      "Dimension Mismatch\n",
      "To perform matrix multiplication, the number of columns in the 1st matrix should match the number of rows in the 2nd matrix. You can rearrange the order to make sure that this satisfies the condition.\n",
      "Added by Leah Gotladera\n",
      "\n",
      "Q: How to evaluate feature importance for numerical variables with AUC?\n",
      "A: You can use roc_auc_score function from sklearn.metrics module and pass the vector of the target variable (‘above_average’) as the first argument and the vector of feature values as the second one. This function will return AUC score for the feature that was passed as a second argument.\n",
      "(Denys Soloviov)\n",
      "\n",
      "Q: How to get the training and validation metrics from XGBoost?\n",
      "A: During the XGBoost lesson, we created a parser to extract the training and validation auc from the standard output. However, we can accomplish that in a more straightforward way.\n",
      "We can use the evals_result  parameters, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\n",
      "Added by Daniel Coronel\n",
      "\n",
      "Q: Why do we use cross validation?\n",
      "A: Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\n",
      "\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\n",
      "Smaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\n",
      "Larger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\n",
      "Aminat Abolade\n",
      "\n",
      "Q: Why do we use cross validation?\n",
      "A: Cross-validation evaluates the performance of a model and chooses the best hyperparameters. Cross-validation does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\n",
      "\"C\" is a hyperparameter that is typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\n",
      "Smaller \"C\" values: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\n",
      "Larger \"C\" values: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct.\n",
      "Aminat Abolade\n",
      "\n",
      "Q: The command '/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache' returned a non-zero code: 1\n",
      "A: After using the command “docker build -t churn-prediction .” to build the Docker image, the above error is raised and the image is not created.\n",
      "In your Dockerfile, change the Python version in the first line the Python version installed in your system:\n",
      "FROM python:3.7.5-slim\n",
      "To find your python version, use the command python --version. For example:\n",
      "python --version\n",
      ">> Python 3.9.7\n",
      "Then, change it on your Dockerfile:\n",
      "FROM python:3.9.7-slim\n",
      "Added by Filipe Melo\n",
      "\n",
      "Q: The command '/bin/sh -c pipenv install --deploy --system &&  rm -rf /root/.cache' returned a non-zero code: 1\n",
      "A: After using the command “docker build -t churn-prediction .” to build the Docker image, the above error is raised and the image is not created.\n",
      "In your Dockerfile, change the Python version in the first line the Python version installed in your system:\n",
      "FROM python:3.7.5-slim\n",
      "To find your python version, use the command python --version. For example:\n",
      "python --version\n",
      ">> Python 3.9.7\n",
      "Then, change it on your Dockerfile:\n",
      "FROM python:3.9.7-slim\n",
      "Added by Filipe Melo\n",
      "\n",
      "Q: How long is the course?\n",
      "A: Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)\n",
      "\n",
      "Q: How long is the course?\n",
      "A: Approximately 4 months, but may take more if you want to do some extra activities (an extra project, an article, etc)\n",
      "\n",
      "Q: I cannot pull the image with docker pull command\n",
      "A: Problem: When I am trying to pull the image with the docker pull svizor/zoomcamp-model command I am getting an error:\n",
      "Using default tag: latest\n",
      "Error response from daemon: manifest for svizor/zoomcamp-model:latest not found: manifest unknown: manifest unknown\n",
      "Solution: The docker by default uses the latest tag to avoid this use the correct tag from image description. In our case use command:\n",
      "docker pull svizor/zoomcamp-model:3.10.12-slim\n",
      "Added by Vladimir Yesipov\n",
      "\n",
      "Q: I cannot pull the image with docker pull command\n",
      "A: Problem: When I am trying to pull the image with the docker pull svizor/zoomcamp-model command I am getting an error:\n",
      "Using default tag: latest\n",
      "Error response from daemon: manifest for svizor/zoomcamp-model:latest not found: manifest unknown: manifest unknown\n",
      "Solution: The docker by default uses the latest tag to avoid this use the correct tag from image description. In our case use command:\n",
      "docker pull svizor/zoomcamp-model:3.10.12-slim\n",
      "Added by Vladimir Yesipov\n",
      "\n",
      "Q: Compute Recall, Precision, and F1 Score using scikit-learn library\n",
      "A: In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\n",
      "from sklearn.metrics import precision_score, recall_score, f1_score\n",
      "precision_score(y_true, y_pred, average='binary')\n",
      "recall_score(y_true, y_pred, average='binary')\n",
      "f1_score(y_true, y_pred, average='binary')\n",
      "Radikal Lukafiardi\n",
      "\n",
      "Q: Compute Recall, Precision, and F1 Score using scikit-learn library\n",
      "A: In the demonstration video, we are shown how to calculate the precision and recall manually. You can use the Scikit Learn library to calculate the confusion matrix. precision, recall, f1_score without having to first define true positive, true negative, false positive, and false negative.\n",
      "from sklearn.metrics import precision_score, recall_score, f1_score\n",
      "precision_score(y_true, y_pred, average='binary')\n",
      "recall_score(y_true, y_pred, average='binary')\n",
      "f1_score(y_true, y_pred, average='binary')\n",
      "Radikal Lukafiardi\n",
      "\n",
      "Q: How to solve regression problems with random forest in scikit-learn?\n",
      "A: You should create sklearn.ensemble.RandomForestRegressor object. It’s rather similar to sklearn.ensemble.RandomForestClassificator for classification problems. Check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html for more information.\n",
      "Alena Kniazeva\n",
      "\n",
      "Q: How to solve regression problems with random forest in scikit-learn?\n",
      "A: You should create sklearn.ensemble.RandomForestRegressor object. It’s rather similar to sklearn.ensemble.RandomForestClassificator for classification problems. Check https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html for more information.\n",
      "Alena Kniazeva\n",
      "\n",
      "Q: How much theory will you cover?\n",
      "A: The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\n",
      "For example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\n",
      "\n",
      "Q: How much theory will you cover?\n",
      "A: The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.: https://mlbookcamp.com/article/python\n",
      "For example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results.\n",
      "\n",
      "Q: Running out of space for AWS instance.\n",
      "A: Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance. It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run docker system prune\n",
      "\n",
      "Q: Running out of space for AWS instance.\n",
      "A: Due to experimenting back and forth so much without care for storage, I just ran out of it on my 30-GB AWS instance. It turns out that deleting docker images does not actually free up any space as you might expect. After removing images, you also need to run docker system prune\n",
      "\n",
      "Q: Basic Ubuntu Commands:\n",
      "A: Cd .. (go back)\n",
      "Ls (see current folders)\n",
      "Cd ‘path’/ (go to this path)\n",
      "Pwd (home)\n",
      "Cat “file name’ --edit txt file in ubuntu\n",
      "Aileah Gotladera\n",
      "\n",
      "Q: Basic Ubuntu Commands:\n",
      "A: Cd .. (go back)\n",
      "Ls (see current folders)\n",
      "Cd ‘path’/ (go to this path)\n",
      "Pwd (home)\n",
      "Cat “file name’ --edit txt file in ubuntu\n",
      "Aileah Gotladera\n",
      "\n",
      "Q: Bind for 0.0.0.0:9696 failed: port is already allocated\n",
      "A: I was getting the below error when I rebuilt the docker image although the port was not allocated, and it was working fine.\n",
      "Error message:\n",
      "Error response from daemon: driver failed programming external connectivity on endpoint beautiful_tharp (875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): Bind for 0.0.0.0:9696 failed: port is already allocated.\n",
      "Solution description\n",
      "Issue has been resolved running the following command:\n",
      "docker kill $(docker ps -q)\n",
      "https://github.com/docker/for-win/issues/2722\n",
      "Asia Saeed\n",
      "\n",
      "Q: Getting  ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8\n",
      "A: This error is produced sometimes when building your docker image from the Amazon python base image.\n",
      "Solution description: The following could solve the problem.\n",
      "Update your docker desktop if you haven’t done so.\n",
      "Or restart docker desktop and terminal and then build the image all over again.\n",
      "Or if all else fails, first run the following command: DOCKER_BUILDKIT=0  docker build .  then build your image.\n",
      "(optional) Added by Odimegwu David\n",
      "\n",
      "Q: Bind for 0.0.0.0:9696 failed: port is already allocated\n",
      "A: I was getting the below error when I rebuilt the docker image although the port was not allocated, and it was working fine.\n",
      "Error message:\n",
      "Error response from daemon: driver failed programming external connectivity on endpoint beautiful_tharp (875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): Bind for 0.0.0.0:9696 failed: port is already allocated.\n",
      "Solution description\n",
      "Issue has been resolved running the following command:\n",
      "docker kill $(docker ps -q)\n",
      "https://github.com/docker/for-win/issues/2722\n",
      "Asia Saeed\n",
      "\n",
      "Q: Getting  ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8\n",
      "A: This error is produced sometimes when building your docker image from the Amazon python base image.\n",
      "Solution description: The following could solve the problem.\n",
      "Update your docker desktop if you haven’t done so.\n",
      "Or restart docker desktop and terminal and then build the image all over again.\n",
      "Or if all else fails, first run the following command: DOCKER_BUILDKIT=0  docker build .  then build your image.\n",
      "(optional) Added by Odimegwu David\n",
      "\n",
      "Q: The connection to the server localhost:8080 was refused - did you specify the right host or port?\n",
      "A: I ran into an issue where kubectl wasn't working.\n",
      "I kept getting the following error:\n",
      "kubectl get service\n",
      "The connection to the server localhost:8080 was refused - did you specify the right host or port?\n",
      "I searched online for a resolution, but everyone kept talking about creating an environment variable and creating some admin.config file in my home directory.\n",
      "All hogwash.\n",
      "The solution to my problem was to just start over.\n",
      "kind delete cluster\n",
      "rm -rf ~/.kube\n",
      "kind create cluster\n",
      "Now when I try the same command again:\n",
      "kubectl get service\n",
      "NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
      "kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   53s\n",
      "Added by Martin Uribe\n",
      "\n",
      "Q: The connection to the server localhost:8080 was refused - did you specify the right host or port?\n",
      "A: I ran into an issue where kubectl wasn't working.\n",
      "I kept getting the following error:\n",
      "kubectl get service\n",
      "The connection to the server localhost:8080 was refused - did you specify the right host or port?\n",
      "I searched online for a resolution, but everyone kept talking about creating an environment variable and creating some admin.config file in my home directory.\n",
      "All hogwash.\n",
      "The solution to my problem was to just start over.\n",
      "kind delete cluster\n",
      "rm -rf ~/.kube\n",
      "kind create cluster\n",
      "Now when I try the same command again:\n",
      "kubectl get service\n",
      "NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n",
      "kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   53s\n",
      "Added by Martin Uribe\n",
      "\n",
      "Q: Problem with recent version of protobuf\n",
      "A: In session 10.3, when creating the virtual environment with pipenv and trying to run the script gateway.py, you might get this error:\n",
      "TypeError: Descriptors cannot not be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      "1. Downgrade the protobuf package to 3.20.x or lower.\n",
      "2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n",
      "This will happen if your version of protobuf is one of the newer ones. As a workaround, you can fix the protobuf version to an older one. In my case I got around the issue by creating the environment with:\n",
      "pipenv install --python 3.9.13 requests grpcio==1.42.0 flask gunicorn \\\n",
      "keras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19.6\n",
      "Added by Ángel de Vicente\n",
      "\n",
      "Q: Problem with recent version of protobuf\n",
      "A: In session 10.3, when creating the virtual environment with pipenv and trying to run the script gateway.py, you might get this error:\n",
      "TypeError: Descriptors cannot not be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      "1. Downgrade the protobuf package to 3.20.x or lower.\n",
      "2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n",
      "This will happen if your version of protobuf is one of the newer ones. As a workaround, you can fix the protobuf version to an older one. In my case I got around the issue by creating the environment with:\n",
      "pipenv install --python 3.9.13 requests grpcio==1.42.0 flask gunicorn \\\n",
      "keras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19.6\n",
      "Added by Ángel de Vicente\n",
      "\n",
      "Q: Error: (ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.) when loading model.\n",
      "A: Problem description:\n",
      "When loading saved model getting error: ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\n",
      "Solution description:\n",
      "Before loading model need to evaluate the model on input data: model.evaluate(train_ds)\n",
      "Added by Vladimir Yesipov\n",
      "\n",
      "Q: Error: (ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.) when loading model.\n",
      "A: Problem description:\n",
      "When loading saved model getting error: ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\n",
      "Solution description:\n",
      "Before loading model need to evaluate the model on input data: model.evaluate(train_ds)\n",
      "Added by Vladimir Yesipov\n",
      "\n",
      "Q: Does it matter if we let the Python file create the server or if we run gunicorn directly?\n",
      "A: They both do the same, it's just less typing from the script.\n",
      "\n",
      "Q: Does it matter if we let the Python file create the server or if we run gunicorn directly?\n",
      "A: They both do the same, it's just less typing from the script.\n",
      "\n",
      "Q: What is standard deviation?\n",
      "A: In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\n",
      "(Aadarsha Shrestha)\n",
      "\n",
      "Q: What is standard deviation?\n",
      "A: In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia] The formula to calculate standard deviation is:\n",
      "(Aadarsha Shrestha)\n",
      "\n",
      "Q: 'kind' is not recognized as an internal or external command, operable program or batch file. (In Windows)\n",
      "A: Problem: I download kind from the next command:\n",
      "curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64\n",
      "When I try\n",
      "kind --version\n",
      "I get: 'kind' is not recognized as an internal or external command, operable program or batch file\n",
      "Solution: The default name of executable is kind-windows-amd64.exe, so that you have to rename this file to  kind.exe. Put this file in specific folder, and add it to PATH\n",
      "Alejandro Aponte\n",
      "\n",
      "Q: 'kind' is not recognized as an internal or external command, operable program or batch file. (In Windows)\n",
      "A: Problem: I download kind from the next command:\n",
      "curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64\n",
      "When I try\n",
      "kind --version\n",
      "I get: 'kind' is not recognized as an internal or external command, operable program or batch file\n",
      "Solution: The default name of executable is kind-windows-amd64.exe, so that you have to rename this file to  kind.exe. Put this file in specific folder, and add it to PATH\n",
      "Alejandro Aponte\n",
      "\n",
      "Q: HPA instance doesn’t run properly (easier solution)\n",
      "A: In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\n",
      ">>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n",
      "And the targets still appear as <unknown>\n",
      "Run the following command:\n",
      "kubectl apply -f https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml\n",
      "Which uses a metrics server deployment file already embedding the - --kubelet-insecure-tls option.\n",
      "Added by Giovanni Pecoraro\n",
      "\n",
      "Q: HPA instance doesn’t run properly (easier solution)\n",
      "A: In case the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\n",
      ">>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n",
      "And the targets still appear as <unknown>\n",
      "Run the following command:\n",
      "kubectl apply -f https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml\n",
      "Which uses a metrics server deployment file already embedding the - --kubelet-insecure-tls option.\n",
      "Added by Giovanni Pecoraro\n",
      "\n",
      "Q: How to find the intercept between precision and recall curves by using numpy?\n",
      "A: You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\n",
      "I suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\n",
      "You want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\n",
      "idx = np.argwhere(\n",
      "np.diff(\n",
      "np.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\n",
      ")\n",
      ").flatten()\n",
      "You can print the result to easily read it:\n",
      "print(\n",
      "f\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx]['threshold']}.\"\n",
      ")\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: How to find the intercept between precision and recall curves by using numpy?\n",
      "A: You can find the intercept between these two curves using numpy diff (https://numpy.org/doc/stable/reference/generated/numpy.diff.html ) and sign (https://numpy.org/doc/stable/reference/generated/numpy.sign.html):\n",
      "I suppose here that you have your df_scores ready with your three columns ‘threshold’, ‘precision’ and ‘recall’:\n",
      "You want to know at which index (or indices) you have your intercept between precision and recall (namely: where the sign of the difference between precision and recall changes):\n",
      "idx = np.argwhere(\n",
      "np.diff(\n",
      "np.sign(np.array(df_scores[\"precision\"]) - np.array(df_scores[\"recall\"]))\n",
      ")\n",
      ").flatten()\n",
      "You can print the result to easily read it:\n",
      "print(\n",
      "f\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx]['threshold']}.\"\n",
      ")\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: DictVectorizer feature names\n",
      "A: The DictVectorizer has a function to get the feature names get_feature_names_out(). This is helpful for example if you need to analyze feature importance but use the dict vectorizer for one hot encoding. Just keep in mind it does return a numpy array so you may need to convert this to a list depending on your usage for example dv.get_feature_names_out() will return a ndarray array of string objects. list(dv.get_feature_names_out()) will convert to a standard list of strings. Also keep in mind that you first need to fit the predictor and response arrays before you have access to the feature names.\n",
      "Quinn Avila\n",
      "\n",
      "Q: DictVectorizer feature names\n",
      "A: The DictVectorizer has a function to get the feature names get_feature_names_out(). This is helpful for example if you need to analyze feature importance but use the dict vectorizer for one hot encoding. Just keep in mind it does return a numpy array so you may need to convert this to a list depending on your usage for example dv.get_feature_names_out() will return a ndarray array of string objects. list(dv.get_feature_names_out()) will convert to a standard list of strings. Also keep in mind that you first need to fit the predictor and response arrays before you have access to the feature names.\n",
      "Quinn Avila\n",
      "\n",
      "Q: Install docker on MacOS\n",
      "A: Refer to the page https://docs.docker.com/desktop/install/mac-install/ remember to check if you have apple chip or intel chip.\n",
      "\n",
      "Q: Install docker on MacOS\n",
      "A: Refer to the page https://docs.docker.com/desktop/install/mac-install/ remember to check if you have apple chip or intel chip.\n",
      "\n",
      "Q: What data should we use for correlation matrix\n",
      "A: Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\n",
      "Yes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\n",
      "Pastor Soto\n",
      "\n",
      "Q: What data should we use for correlation matrix\n",
      "A: Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\n",
      "Yes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\n",
      "Pastor Soto\n",
      "\n",
      "Q: When do I use ROC vs Precision-Recall curves?\n",
      "A: - ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n",
      "- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\n",
      "-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\n",
      "- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\n",
      "(Anudeep Vanjavakam)\n",
      "\n",
      "Q: When do I use ROC vs Precision-Recall curves?\n",
      "A: - ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n",
      "- The reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance.\n",
      "-This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\n",
      "- If the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance.\n",
      "(Anudeep Vanjavakam)\n",
      "\n",
      "Q: Does it matter if we let the Python file create the server or if we run gunicorn directly?\n",
      "A: They both do the same, it's just less typing from the script.\n",
      "Asked by Andrew Katoch, Added by Edidiong Esu\n",
      "\n",
      "Q: Does it matter if we let the Python file create the server or if we run gunicorn directly?\n",
      "A: They both do the same, it's just less typing from the script.\n",
      "Asked by Andrew Katoch, Added by Edidiong Esu\n",
      "\n",
      "Q: Root Mean Squared Error\n",
      "A: To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
      "See details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\n",
      "Ahmed Okka\n",
      "\n",
      "Q: Root Mean Squared Error\n",
      "A: To use RMSE without math or numpy, ‘sklearn.metrics’ has a mean_squared_error function with a squared kwarg (defaults to True). Setting squared to False will return the RMSE.\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rms = mean_squared_error(y_actual, y_predicted, squared=False)\n",
      "See details: https://stackoverflow.com/questions/17197492/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\n",
      "Ahmed Okka\n",
      "\n",
      "Q: Meaning of mean in homework 2, question 3\n",
      "A: In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\n",
      "It means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\n",
      "df_train['column_name'].mean( )\n",
      "Another option:\n",
      "df_train[‘column_name’].describe()\n",
      "(Bhaskar Sarma)\n",
      "\n",
      "Q: Meaning of mean in homework 2, question 3\n",
      "A: In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\n",
      "It means that you should use only the training data set for computing the mean, not validation or  test data set. This is how you can calculate the mean\n",
      "df_train['column_name'].mean( )\n",
      "Another option:\n",
      "df_train[‘column_name’].describe()\n",
      "(Bhaskar Sarma)\n",
      "\n",
      "Q: Model training very slow in google colab with T4 GPU\n",
      "A: When training the models, in the fit function, you can specify the number of workers/threads.\n",
      "The number of threads apparently also works for GPUs, and came very handy in google colab for the T4 GPU, since it was very very slow, and workers default value is 1.\n",
      "I changed the workers variable to 2560, following this thread in stackoverflow. I am using the free T4 GPU.  (https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu)\n",
      "Added by Ibai Irastorza\n",
      "\n",
      "Q: Model training very slow in google colab with T4 GPU\n",
      "A: When training the models, in the fit function, you can specify the number of workers/threads.\n",
      "The number of threads apparently also works for GPUs, and came very handy in google colab for the T4 GPU, since it was very very slow, and workers default value is 1.\n",
      "I changed the workers variable to 2560, following this thread in stackoverflow. I am using the free T4 GPU.  (https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu)\n",
      "Added by Ibai Irastorza\n",
      "\n",
      "Q: BentoML not working with –production flag at any stage: e.g. with bentoml serve and while running the bentoml container\n",
      "A: You might see a long error message with something about sparse matrices, and in the swagger UI, you get a code 500 error with “” (empty string) as output.\n",
      "Potential reason: Setting DictVectorizer or OHE to sparse while training, and then storing this in a pipeline or custom object in the benotml model saving stage in train.py. This means that when the custom object is called in service.py, it will convert each input to a different sized sparse matrix, and this can't be batched due to inconsistent length. In this case, bentoml model signatures should have batchable set to False for production during saving the bentoml mode in train.py.\n",
      "(Memoona Tahira)\n",
      "\n",
      "Q: BentoML not working with –production flag at any stage: e.g. with bentoml serve and while running the bentoml container\n",
      "A: You might see a long error message with something about sparse matrices, and in the swagger UI, you get a code 500 error with “” (empty string) as output.\n",
      "Potential reason: Setting DictVectorizer or OHE to sparse while training, and then storing this in a pipeline or custom object in the benotml model saving stage in train.py. This means that when the custom object is called in service.py, it will convert each input to a different sized sparse matrix, and this can't be batched due to inconsistent length. In this case, bentoml model signatures should have batchable set to False for production during saving the bentoml mode in train.py.\n",
      "(Memoona Tahira)\n",
      "\n",
      "Q: How many models should I train?\n",
      "A: Regarding Point 4 in the midterm deliverables, which states, \"Train multiple models, tune their performance, and select the best model,\" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term \"multiple\" implies having more than one model, so as long as you have more than one, you're on the right track.\n",
      "\n",
      "Q: How many models should I train?\n",
      "A: Regarding Point 4 in the midterm deliverables, which states, \"Train multiple models, tune their performance, and select the best model,\" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term \"multiple\" implies having more than one model, so as long as you have more than one, you're on the right track.\n",
      "\n",
      "Q: Features in scikit-learn?\n",
      "A: Features (X) must always be formatted as a 2-D array to be accepted by scikit-learn.\n",
      "Use reshape to reshape a 1D array to a 2D.\n",
      "\t\t\t\t\t\t\t(-Aileah) :>\n",
      "(added by Tano\n",
      "filtered_df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n",
      "# Select only the desired columns\n",
      "selected_columns = [\n",
      "'latitude',\n",
      "'longitude',\n",
      "'housing_median_age',\n",
      "'total_rooms',\n",
      "'total_bedrooms',\n",
      "'population',\n",
      "'households',\n",
      "'median_income',\n",
      "'median_house_value'\n",
      "]\n",
      "filtered_df = filtered_df[selected_columns]\n",
      "# Display the first few rows of the filtered DataFrame\n",
      "print(filtered_df.head())\n",
      "\n",
      "Q: Fatal: Authentication failed for 'https://github.com/username\n",
      "A: I had a problem when I tried to push my code from Git Bash:\n",
      "remote: Support for password authentication was removed on August 13, 2021.\n",
      "remote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\n",
      "fatal: Authentication failed for 'https://github.com/username\n",
      "Solution:\n",
      "Create a personal access token from your github account and use it when you make a push of your last changes.\n",
      "https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n",
      "Bruno Bedón\n",
      "\n",
      "Q: Fatal: Authentication failed for 'https://github.com/username\n",
      "A: I had a problem when I tried to push my code from Git Bash:\n",
      "remote: Support for password authentication was removed on August 13, 2021.\n",
      "remote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\n",
      "fatal: Authentication failed for 'https://github.com/username\n",
      "Solution:\n",
      "Create a personal access token from your github account and use it when you make a push of your last changes.\n",
      "https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n",
      "Bruno Bedón\n",
      "\n",
      "Q: What is eta in XGBoost\n",
      "A: Sometimes someone might wonder what eta means in the tunable hyperparameters of XGBoost and how it helps the model.\n",
      "ETA is the learning rate of the model. XGBoost uses gradient descent to calculate and update the model. In gradient descent, we are looking for the minimum weights that help the model to learn the data very well. This minimum weights for the features is updated each time the model passes through the features and learns the features during training. Tuning the learning rate helps you tell the model what speed it would use in deriving the minimum for the weights.\n",
      "\n",
      "Q: What is eta in XGBoost\n",
      "A: Sometimes someone might wonder what eta means in the tunable hyperparameters of XGBoost and how it helps the model.\n",
      "ETA is the learning rate of the model. XGBoost uses gradient descent to calculate and update the model. In gradient descent, we are looking for the minimum weights that help the model to learn the data very well. This minimum weights for the features is updated each time the model passes through the features and learns the features during training. Tuning the learning rate helps you tell the model what speed it would use in deriving the minimum for the weights.\n",
      "\n",
      "Q: Why do we sometimes use random_state and not at other times?\n",
      "A: Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\n",
      "Refer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\n",
      "~~Ella Sahnan~~\n",
      "\n",
      "Q: Running “pipenv install sklearn==1.0.2” gives errors. What should I do?\n",
      "A: When the facilitator was adding sklearn to the virtual environment in the lectures, he used sklearn==0.24.1 and it ran smoothly. But while doing the homework and you are asked to use the 1.0.2 version of sklearn, it gives errors.\n",
      "The solution is to use the full name of sklearn. That is, run it as “pipenv install scikit-learn==1.0.2” and the error will go away, allowing you to install sklearn for the version in your virtual environment.\n",
      "Odimegwu David\n",
      "Homework asks you to install 1.3.1\n",
      "Pipenv install scikit-learn==1.3.1\n",
      "Use Pipenv to install Scikit-Learn version 1.3.1\n",
      "Gopakumar Gopinathan\n",
      "\n",
      "Q: Why do we sometimes use random_state and not at other times?\n",
      "A: Ie particularly in module-04 homework Qn2 vs Qn5. https://datatalks-club.slack.com/archives/C0288NJ5XSA/p1696760905214979\n",
      "Refer to the sklearn docs, random_state is to ensure the “randomness” that is used to shuffle dataset is reproducible, and it usually requires both random_state and shuffle params to be set accordingly.\n",
      "~~Ella Sahnan~~\n",
      "\n",
      "Q: Running “pipenv install sklearn==1.0.2” gives errors. What should I do?\n",
      "A: When the facilitator was adding sklearn to the virtual environment in the lectures, he used sklearn==0.24.1 and it ran smoothly. But while doing the homework and you are asked to use the 1.0.2 version of sklearn, it gives errors.\n",
      "The solution is to use the full name of sklearn. That is, run it as “pipenv install scikit-learn==1.0.2” and the error will go away, allowing you to install sklearn for the version in your virtual environment.\n",
      "Odimegwu David\n",
      "Homework asks you to install 1.3.1\n",
      "Pipenv install scikit-learn==1.3.1\n",
      "Use Pipenv to install Scikit-Learn version 1.3.1\n",
      "Gopakumar Gopinathan\n",
      "\n",
      "Q: What is the difference between pandas get_dummies and sklearn OnehotEncoder?\n",
      "A: They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]\n",
      "\n",
      "Q: What is the difference between pandas get_dummies and sklearn OnehotEncoder?\n",
      "A: They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc, but they are both techniques to one-hot-encode categorical variables with identical results. The biggest difference is get_dummies are a convenient choice when you are working with Pandas Dataframes, while if you are building a scikit-learn-based machine learning pipeline and need to handle categorical data as part of that pipeline, OneHotEncoder is a more suitable choice. [Abhirup Ghosh]\n",
      "\n",
      "Q: Installing md5sum on Macos\n",
      "A: Install it by using command\n",
      "% brew install md5sha1sum\n",
      "Then run command to check hash for file to check if they the same with the provided\n",
      "% md5sum model1.bin dv.bin\n",
      "Olga Rudakova\n",
      "\n",
      "Q: Installing md5sum on Macos\n",
      "A: Install it by using command\n",
      "% brew install md5sha1sum\n",
      "Then run command to check hash for file to check if they the same with the provided\n",
      "% md5sum model1.bin dv.bin\n",
      "Olga Rudakova\n",
      "\n",
      "Q: Get_feature_names() not found\n",
      "A: Problem: In the course this function worked to get the features from the dictVectorizer instance: dv.get_feature_names(). But in my computer did not work. I think it has to do with library versions and but apparently that function will be deprecated soon:\n",
      "Old: https://scikit-learn.org/0.22/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\n",
      "New: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\n",
      "Solution: change the line dv.get_feature_names() to list(dv.get_feature_names_out))\n",
      "Ibai Irastorza\n",
      "\n",
      "Q: Get_feature_names() not found\n",
      "A: Problem: In the course this function worked to get the features from the dictVectorizer instance: dv.get_feature_names(). But in my computer did not work. I think it has to do with library versions and but apparently that function will be deprecated soon:\n",
      "Old: https://scikit-learn.org/0.22/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\n",
      "New: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.get_feature_names\n",
      "Solution: change the line dv.get_feature_names() to list(dv.get_feature_names_out))\n",
      "Ibai Irastorza\n",
      "\n",
      "Q: Identifying highly correlated feature pairs easily through unstack\n",
      "A: data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\n",
      "data_corr.head(10)\n",
      "Added by Harish Balasundaram\n",
      "You can also use seaborn to create a heatmap with the correlation. The code for doing that:\n",
      "sns.heatmap(df[numerical_features].corr(),\n",
      "annot=True,\n",
      "square=True,\n",
      "fmt=\".2g\",\n",
      "cmap=\"crest\")\n",
      "Added by Cecile Guillot\n",
      "You can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\n",
      "Which outputs, in the case of churn dataset:\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: What is the better option FeatureHasher or DictVectorizer\n",
      "A: These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\n",
      "When you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\n",
      "You can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\n",
      "Olga Rudakova\n",
      "\n",
      "Q: Identifying highly correlated feature pairs easily through unstack\n",
      "A: data_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\n",
      "data_corr.head(10)\n",
      "Added by Harish Balasundaram\n",
      "You can also use seaborn to create a heatmap with the correlation. The code for doing that:\n",
      "sns.heatmap(df[numerical_features].corr(),\n",
      "annot=True,\n",
      "square=True,\n",
      "fmt=\".2g\",\n",
      "cmap=\"crest\")\n",
      "Added by Cecile Guillot\n",
      "You can refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information with this function:\n",
      "Which outputs, in the case of churn dataset:\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: What is the better option FeatureHasher or DictVectorizer\n",
      "A: These both methods receive the dictionary as an input. While the DictVectorizer will store the big vocabulary and takes more memory. FeatureHasher create a vectors with predefined length. They are both used for categorical features.\n",
      "When you have a high cardinality for categorical features better to use FeatureHasher. If you want to preserve feature names in transformed data and have a small number of unique values is DictVectorizer. But your choice will dependence on your data.\n",
      "You can read more by follow the link https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html\n",
      "Olga Rudakova\n",
      "\n",
      "Q: Error launching Jupyter notebook\n",
      "A: If you face an error kind of ImportError: cannot import name 'contextfilter' from 'jinja2' (anaconda\\lib\\site-packages\\jinja2\\__init__.py) when launching a new notebook for a brand new environment.\n",
      "Switch to the main environment and run \"pip install nbconvert --upgrade\".\n",
      "Added by George Chizhmak\n",
      "\n",
      "Q: Error launching Jupyter notebook\n",
      "A: If you face an error kind of ImportError: cannot import name 'contextfilter' from 'jinja2' (anaconda\\lib\\site-packages\\jinja2\\__init__.py) when launching a new notebook for a brand new environment.\n",
      "Switch to the main environment and run \"pip install nbconvert --upgrade\".\n",
      "Added by George Chizhmak\n",
      "\n",
      "Q: Adding community notes\n",
      "A: You can create your own github repository for the course with your notes, homework, projects, etc.\n",
      "Then fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\n",
      "After that's done, create a pull request to sync your fork with the original course repo.\n",
      "(By Wesley Barreto)\n",
      "\n",
      "Q: Error with scipy missing module in SaturnCloud\n",
      "A: Problem:\n",
      "I created a new environment in SaturnCloud and chose the image corresponding to Saturn with Tensorflow, but when I tried to fit the model it showed an error about the missing module: scipy\n",
      "Solution:\n",
      "Install the module in a new cell: !pip install scipy\n",
      "Restart the kernel and fit the model again\n",
      "Added by Erick Calderin\n",
      "\n",
      "Q: Adding community notes\n",
      "A: You can create your own github repository for the course with your notes, homework, projects, etc.\n",
      "Then fork the original course repo and add a link under the 'Community Notes' section to the notes that are in your own repo.\n",
      "After that's done, create a pull request to sync your fork with the original course repo.\n",
      "(By Wesley Barreto)\n",
      "\n",
      "Q: Error with scipy missing module in SaturnCloud\n",
      "A: Problem:\n",
      "I created a new environment in SaturnCloud and chose the image corresponding to Saturn with Tensorflow, but when I tried to fit the model it showed an error about the missing module: scipy\n",
      "Solution:\n",
      "Install the module in a new cell: !pip install scipy\n",
      "Restart the kernel and fit the model again\n",
      "Added by Erick Calderin\n",
      "\n",
      "Q: Running kind on Linux with Rootless Docker or Rootless Podman\n",
      "A: Using kind with Rootless Docker or Rootless Podman requires some changes on the system (Linux), see kind – Rootless (k8s.io).\n",
      "Sylvia Schmitt\n",
      "\n",
      "Q: Running kind on Linux with Rootless Docker or Rootless Podman\n",
      "A: Using kind with Rootless Docker or Rootless Podman requires some changes on the system (Linux), see kind – Rootless (k8s.io).\n",
      "Sylvia Schmitt\n",
      "\n",
      "Q: Reproducibility\n",
      "A: Problem description:\n",
      "Do we have to run everything?\n",
      "You are encouraged, if you can, to run them. As this provides another opportunity to learn from others.\n",
      "Not everyone will be able to run all the files, in particular the neural networks.\n",
      "Solution description:\n",
      "Alternatively, can you see that everything you need to reproduce is there: the dataset is there, the instructions are there, are there any obvious errors and so on.\n",
      "Related slack conversation here.\n",
      "(Gregory Morris)\n",
      "\n",
      "Q: Reproducibility\n",
      "A: Problem description:\n",
      "Do we have to run everything?\n",
      "You are encouraged, if you can, to run them. As this provides another opportunity to learn from others.\n",
      "Not everyone will be able to run all the files, in particular the neural networks.\n",
      "Solution description:\n",
      "Alternatively, can you see that everything you need to reproduce is there: the dataset is there, the instructions are there, are there any obvious errors and so on.\n",
      "Related slack conversation here.\n",
      "(Gregory Morris)\n",
      "\n",
      "Q: The course has already started. Can I still join it?\n",
      "A: Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\n",
      "In order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.\n",
      "\n",
      "Q: The course has already started. Can I still join it?\n",
      "A: Yes, you can. You won’t be able to submit some of the homeworks, but you can still take part in the course.\n",
      "In order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers’ Projects by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate.\n",
      "\n",
      "Q: 'pipenv' is not recognized as an internal or external command, operable program or batch file.\n",
      "A: This error happens because pipenv is already installed but you can't access it from the path.\n",
      "This error comes out if you run.\n",
      "pipenv  --version\n",
      "pipenv shell\n",
      "Solution for Windows\n",
      "Open this option\n",
      "Click here\n",
      "Click in Edit Button\n",
      "Make sure the next two locations are on the PATH, otherwise, add it.\n",
      "C:\\Users\\AppData\\....\\Python\\PythonXX\\\n",
      "C:\\Users\\AppData\\....\\Python\\PythonXX\\Scripts\\\n",
      "Added by Alejandro Aponte\n",
      "Note: this answer assumes you don’t use Anaconda. For Windows, using Anaconda would be a better choice and less prone to errors.\n",
      "\n",
      "Q: 'pipenv' is not recognized as an internal or external command, operable program or batch file.\n",
      "A: This error happens because pipenv is already installed but you can't access it from the path.\n",
      "This error comes out if you run.\n",
      "pipenv  --version\n",
      "pipenv shell\n",
      "Solution for Windows\n",
      "Open this option\n",
      "Click here\n",
      "Click in Edit Button\n",
      "Make sure the next two locations are on the PATH, otherwise, add it.\n",
      "C:\\Users\\AppData\\....\\Python\\PythonXX\\\n",
      "C:\\Users\\AppData\\....\\Python\\PythonXX\\Scripts\\\n",
      "Added by Alejandro Aponte\n",
      "Note: this answer assumes you don’t use Anaconda. For Windows, using Anaconda would be a better choice and less prone to errors.\n",
      "\n",
      "Q: Where is the model for week 9?\n",
      "A: The week 9 uses a link to github to fetch the models.\n",
      "The original link was moved to here:\n",
      "https://github.com/DataTalksClub/machine-learning-zoomcamp/releases\n",
      "\n",
      "Q: Where is the model for week 9?\n",
      "A: The week 9 uses a link to github to fetch the models.\n",
      "The original link was moved to here:\n",
      "https://github.com/DataTalksClub/machine-learning-zoomcamp/releases\n",
      "\n",
      "Q: Dealing with Convergence in Week 3 q6\n",
      "A: When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\n",
      "Feature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \tsimilar scale, preventing convergence issues.\n",
      "Categorical Feature Encoding: If your dataset includes categorical features, apply \tcategorical encoding techniques such as OneHotEncoder (OHE) to \tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\n",
      "Combine Features: After \tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\n",
      "By following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\n",
      "You can find an example here.\n",
      " \t\t\t\t\t\t\t\t\t\t\t\tOsman Ali\n",
      "\n",
      "Q: Dealing with Convergence in Week 3 q6\n",
      "A: When encountering convergence errors during the training of a Ridge regression model, consider the following steps:\n",
      "Feature Normalization: Normalize your numerical features using techniques like MinMaxScaler or StandardScaler. This ensures that numerical features are on a \tsimilar scale, preventing convergence issues.\n",
      "Categorical Feature Encoding: If your dataset includes categorical features, apply \tcategorical encoding techniques such as OneHotEncoder (OHE) to \tconvert them into a numerical format. OHE is commonly used to represent categorical variables as binary vectors, making them compatible with regression models like Ridge.\n",
      "Combine Features: After \tnormalizing numerical features and encoding categorical features using OneHotEncoder, combine them to form a single feature matrix (X_train). This combined dataset serves as the input for training the Ridge regression model.\n",
      "By following these steps, you can address convergence errors and enhance the stability of your Ridge model training process. It's important to note that the choice of encoding method, such as OneHotEncoder, is appropriate for handling categorical features in this context.\n",
      "You can find an example here.\n",
      " \t\t\t\t\t\t\t\t\t\t\t\tOsman Ali\n",
      "\n",
      "Q: No module named ‘ping’?\n",
      "A: When I tried to run example from the video using function ping and can not import it. I use import ping and it was unsuccessful. To fix it I use the statement:\n",
      "\n",
      "from [file name] import ping\n",
      "Olga Rudakova\n",
      "\n",
      "Q: No module named ‘ping’?\n",
      "A: When I tried to run example from the video using function ping and can not import it. I use import ping and it was unsuccessful. To fix it I use the statement:\n",
      "\n",
      "from [file name] import ping\n",
      "Olga Rudakova\n",
      "\n",
      "Q: Python_version and Python_full_version error after running pipenv install:\n",
      "A: If you install packages via pipenv install, and get an error that ends like this:\n",
      "pipenv.vendor.plette.models.base.ValidationError: {'python_version': '3.9', 'python_full_version': '3.9.13'}\n",
      "python_full_version: 'python_version' must not be present with 'python_full_version'\n",
      "python_version: 'python_full_version' must not be present with 'python_version'\n",
      "Do this:\n",
      "open Pipfile in nano editor, and remove either the python_version or python_full_version line, press CTRL+X, type Y and click Enter to save changed\n",
      "Type pipenv lock to create the Pipfile.lock.\n",
      "Done. Continue what you were doing\n",
      "\n",
      "Q: Python_version and Python_full_version error after running pipenv install:\n",
      "A: If you install packages via pipenv install, and get an error that ends like this:\n",
      "pipenv.vendor.plette.models.base.ValidationError: {'python_version': '3.9', 'python_full_version': '3.9.13'}\n",
      "python_full_version: 'python_version' must not be present with 'python_full_version'\n",
      "python_version: 'python_full_version' must not be present with 'python_version'\n",
      "Do this:\n",
      "open Pipfile in nano editor, and remove either the python_version or python_full_version line, press CTRL+X, type Y and click Enter to save changed\n",
      "Type pipenv lock to create the Pipfile.lock.\n",
      "Done. Continue what you were doing\n",
      "\n",
      "Q: Convert dictionary values to Dataframe table\n",
      "A: You can convert the prediction output values to a datafarme using \n",
      "df = pd.DataFrame.from_dict(dict, orient='index' , columns=[\"Prediction\"])\n",
      "Edidiong Esu\n",
      "\n",
      "Q: Problem: 'ls' is not recognized as an internal or external command, operable program or batch file.\n",
      "A: When trying to run the command  !ls -lh in windows jupyter notebook  , I was getting an error message that says “'ls' is not recognized as an internal or external command,operable program or batch file.\n",
      "Solution description :\n",
      "Instead of !ls -lh , you can use this command !dir , and you will get similar output\n",
      "Asia Saeed\n",
      "\n",
      "Q: Problem: 'ls' is not recognized as an internal or external command, operable program or batch file.\n",
      "A: When trying to run the command  !ls -lh in windows jupyter notebook  , I was getting an error message that says “'ls' is not recognized as an internal or external command,operable program or batch file.\n",
      "Solution description :\n",
      "Instead of !ls -lh , you can use this command !dir , and you will get similar output\n",
      "Asia Saeed\n",
      "\n",
      "Q: Different values of auc, each time code is re-run\n",
      "A: When I run dt = DecisionTreeClassifier() in jupyter in same laptop, each time I re-run it or do (restart kernel + run) I get different values of auc. Some of them are 0.674, 0.652, 0.642, 0.669 and so on.  Anyone knows why it could be? I am referring to 7:40-7:45 of video 6.3.\n",
      "Solution: try setting the random seed e.g\n",
      "dt = DecisionTreeClassifier(random_state=22)\n",
      "Bhaskar Sarma\n",
      "\n",
      "Q: Different values of auc, each time code is re-run\n",
      "A: When I run dt = DecisionTreeClassifier() in jupyter in same laptop, each time I re-run it or do (restart kernel + run) I get different values of auc. Some of them are 0.674, 0.652, 0.642, 0.669 and so on.  Anyone knows why it could be? I am referring to 7:40-7:45 of video 6.3.\n",
      "Solution: try setting the random seed e.g\n",
      "dt = DecisionTreeClassifier(random_state=22)\n",
      "Bhaskar Sarma\n",
      "\n",
      "Q: Why do I have different values of accuracy than the options in the homework?\n",
      "A: One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\n",
      "Although the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\n",
      "1)\n",
      "df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
      "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
      "2)\n",
      "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
      "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n",
      "Therefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\n",
      "Ibraheem Taha\n",
      "\n",
      "Q: Why do I have different values of accuracy than the options in the homework?\n",
      "A: One main reason behind that, is the way of splitting data. For example, we want to split data into train/validation/test with the ratios 60%/20%/20% respectively.\n",
      "Although the following two options end up with the same ratio, the data itself is a bit different and not 100% matching in each case.\n",
      "1)\n",
      "df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
      "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n",
      "2)\n",
      "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
      "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n",
      "Therefore, I would recommend using the second method which is more consistent with the lessons and thus the homeworks.\n",
      "Ibraheem Taha\n",
      "\n",
      "Q: Random seed 42\n",
      "A: One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\n",
      "The purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.\n",
      "\n",
      "Q: Random seed 42\n",
      "A: One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\n",
      "The purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes.\n",
      "\n",
      "Q: Learning in public links for the projects\n",
      "A: For the learning in public for this midterm project it seems that has a total value of 14!, Does this mean that we need make 14 posts?, Or the regular seven posts for each module and each one with a value of 2?, Or just one with a total value of 14?\n",
      "14 posts, one for each day\n",
      "\n",
      "Q: Learning in public links for the projects\n",
      "A: For the learning in public for this midterm project it seems that has a total value of 14!, Does this mean that we need make 14 posts?, Or the regular seven posts for each module and each one with a value of 2?, Or just one with a total value of 14?\n",
      "14 posts, one for each day\n",
      "\n",
      "Q: Method to get beautiful classification report\n",
      "A: Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\n",
      "Krishna Annad\n",
      "\n",
      "Q: Method to get beautiful classification report\n",
      "A: Use Yellowbrick. Yellowbrick in a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports.\n",
      "Krishna Annad\n",
      "\n",
      "Q: Understanding Ridge\n",
      "A: Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\n",
      "sag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It's particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\n",
      "Alpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\n",
      "from sklearn.linear_model import Ridge\n",
      "ridge = Ridge(alpha=alpha, solver='sag', random_state=42)\n",
      "ridge.fit(X_train, y_train)\n",
      "Aminat Abolade\n",
      "\n",
      "Q: Understanding Ridge\n",
      "A: Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\n",
      "sag Solver: The sag solver stands for \"Stochastic Average Gradient.\" It's particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\n",
      "Alpha: The alpha parameter  controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\n",
      "from sklearn.linear_model import Ridge\n",
      "ridge = Ridge(alpha=alpha, solver='sag', random_state=42)\n",
      "ridge.fit(X_train, y_train)\n",
      "Aminat Abolade\n",
      "\n",
      "Q: Visualize Feature Importance by using horizontal bar chart\n",
      "A: To make it easier for us to determine which features are important, we can use a horizontal bar chart to illustrate feature importance sorted by value.\n",
      "1. # extract the feature importances from the model\n",
      "feature_importances = list(zip(features_names, rdr_model.feature_importances_))\n",
      "importance_df = pd.DataFrame(feature_importances, columns=['feature_names', 'feature_importances'])\n",
      "2. # sort descending the dataframe by using feature_importances value\n",
      "importance_df = importance_df.sort_values(by='feature_importances', ascending=False)\n",
      "3. # create a horizontal bar chart\n",
      "plt.figure(figsize=(8, 6))\n",
      "sns.barplot(x='feature_importances', y='feature_names', data=importance_df, palette='Blues_r')\n",
      "plt.xlabel('Feature Importance')\n",
      "plt.ylabel('Feature Names')\n",
      "plt.title('Feature Importance Chart')\n",
      "Radikal Lukafiardi\n",
      "\n",
      "Q: Visualize Feature Importance by using horizontal bar chart\n",
      "A: To make it easier for us to determine which features are important, we can use a horizontal bar chart to illustrate feature importance sorted by value.\n",
      "1. # extract the feature importances from the model\n",
      "feature_importances = list(zip(features_names, rdr_model.feature_importances_))\n",
      "importance_df = pd.DataFrame(feature_importances, columns=['feature_names', 'feature_importances'])\n",
      "2. # sort descending the dataframe by using feature_importances value\n",
      "importance_df = importance_df.sort_values(by='feature_importances', ascending=False)\n",
      "3. # create a horizontal bar chart\n",
      "plt.figure(figsize=(8, 6))\n",
      "sns.barplot(x='feature_importances', y='feature_names', data=importance_df, palette='Blues_r')\n",
      "plt.xlabel('Feature Importance')\n",
      "plt.ylabel('Feature Names')\n",
      "plt.title('Feature Importance Chart')\n",
      "Radikal Lukafiardi\n",
      "\n",
      "Q: ValueError: Unknown label type: 'continuous'\n",
      "A: Solution: This problem happens because you use DecisionTreeClassifier instead of DecisionTreeRegressor. You should check if you want to use a Decision tree for classification or regression.\n",
      "Alejandro Aponte\n",
      "\n",
      "Q: ValueError: Unknown label type: 'continuous'\n",
      "A: Solution: This problem happens because you use DecisionTreeClassifier instead of DecisionTreeRegressor. You should check if you want to use a Decision tree for classification or regression.\n",
      "Alejandro Aponte\n",
      "\n",
      "Q: None of the videos have how to install the environment in Mac, does someone have instructions for Mac with M1 chip?\n",
      "A: Refer to https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/01-intro/06-environment.md\n",
      "(added by Rileen Sinha)\n",
      "\n",
      "Q: What if your accuracy and std training loss don’t match HW?\n",
      "A: Problem:\n",
      "I found running the wasp/bee model on my mac laptop had higher reported accuracy and lower std deviation than the HW answers. This may be because of the SGD optimizer. Running this on my mac printed a message about a new and legacy version that could be used.\n",
      "Solution:\n",
      "Try running the same code on google collab or another way. The answers were closer for me on collab. Another tip is to change the runtime to use T4 and the model run’s faster than just CPU\n",
      "Added by Quinn Avila\n",
      "\n",
      "Q: What if your accuracy and std training loss don’t match HW?\n",
      "A: Problem:\n",
      "I found running the wasp/bee model on my mac laptop had higher reported accuracy and lower std deviation than the HW answers. This may be because of the SGD optimizer. Running this on my mac printed a message about a new and legacy version that could be used.\n",
      "Solution:\n",
      "Try running the same code on google collab or another way. The answers were closer for me on collab. Another tip is to change the runtime to use T4 and the model run’s faster than just CPU\n",
      "Added by Quinn Avila\n",
      "\n",
      "Q: When should we transform the target variable to logarithm distribution?\n",
      "A: When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work\n",
      "\n",
      "Q: When should we transform the target variable to logarithm distribution?\n",
      "A: When the target variable has a long tail distribution, like in prices, with a wide range, you can transform the target variable with np.log1p() method, but be aware if your target variable has negative values, this method will not work\n",
      "\n",
      "Q: wget: unable to resolve host address 'raw.githubusercontent.com'\n",
      "A: In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\n",
      "Getting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address 'raw.githubusercontent.com'\n",
      "Solution:\n",
      "In your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\n",
      "\n",
      "Q: wget: unable to resolve host address 'raw.githubusercontent.com'\n",
      "A: In Kaggle, when you are trying to !wget a dataset from github (or any other public repository/location), you get the following error:\n",
      "Getting  this error while trying to import data- !wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address 'raw.githubusercontent.com'\n",
      "Solution:\n",
      "In your Kaggle notebook settings, turn on the Internet for your session. It's on the settings panel, on the right hand side of the Kaggle screen. You'll be asked to verify your phone number so Kaggle knows you are not a bot.\n",
      "\n",
      "Q: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named 'sklearn'\n",
      "A: I was getting the below error message when I was trying to create docker image using bentoml\n",
      "[bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named 'sklearn'\n",
      "Solution description\n",
      "The cause was because , in bentofile.yaml, I wrote sklearn instead of scikit-learn. Issue was fixed after I modified the packages list as below.\n",
      "packages: # Additional pip packages required by the service\n",
      "- xgboost\n",
      "- scikit-learn\n",
      "- pydantic\n",
      "Asia Saeed\n",
      "\n",
      "Q: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named 'sklearn'\n",
      "A: I was getting the below error message when I was trying to create docker image using bentoml\n",
      "[bentoml-cli] `serve` failed: Failed loading Bento from directory /home/bentoml/bento: Failed to import module \"service\": No module named 'sklearn'\n",
      "Solution description\n",
      "The cause was because , in bentofile.yaml, I wrote sklearn instead of scikit-learn. Issue was fixed after I modified the packages list as below.\n",
      "packages: # Additional pip packages required by the service\n",
      "- xgboost\n",
      "- scikit-learn\n",
      "- pydantic\n",
      "Asia Saeed\n",
      "\n",
      "Q: Why are FPR and TPR equal to 0.0, when threshold = 1.0?\n",
      "A: For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\n",
      "The threshold is 1.0\n",
      "FPR is 0.0\n",
      "And TPR is 0.0\n",
      "When the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\n",
      "That is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\n",
      "Alena Kniazeva\n",
      "\n",
      "Q: Why are FPR and TPR equal to 0.0, when threshold = 1.0?\n",
      "A: For churn/not churn predictions, I need help to interpret the following scenario please, what is happening when:\n",
      "The threshold is 1.0\n",
      "FPR is 0.0\n",
      "And TPR is 0.0\n",
      "When the threshold is 1.0, the condition for belonging to the positive class (churn class) is g(x)>=1.0 But g(x) is a sigmoid function for a binary classification problem. It has values between 0 and 1. This function  never becomes equal to outermost values, i.e. 0 and 1.\n",
      "That is why there is no object, for which churn-condition could be satisfied. And that is why there is no any positive  (churn) predicted value (neither true positive, nor false positive), if threshold is equal to 1.0\n",
      "Alena Kniazeva\n",
      "\n",
      "Q: Completed creating the environment locally but could not find the environment on AWS.\n",
      "A: Ans: so you have created the env. You need to make sure you're in eu-west-1 (ireland) when you check the EB environments. Maybe you're in a different region in your console.\n",
      "Added by Edidiong Esu\n",
      "\n",
      "Q: Completed creating the environment locally but could not find the environment on AWS.\n",
      "A: Ans: so you have created the env. You need to make sure you're in eu-west-1 (ireland) when you check the EB environments. Maybe you're in a different region in your console.\n",
      "Added by Edidiong Esu\n",
      "\n",
      "Q: Any particular hardware requirements for the course or everything is mostly cloud? TIA! Couldn't really find this in the FAQ.\n",
      "A: For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\n",
      "(Rileen Sinha; based on response by Alexey on Slack)\n",
      "\n",
      "Q: Any particular hardware requirements for the course or everything is mostly cloud? TIA! Couldn't really find this in the FAQ.\n",
      "A: For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud (we use Saturn cloud but can be anything else).\n",
      "(Rileen Sinha; based on response by Alexey on Slack)\n",
      "\n",
      "Q: How keras flow_from_directory know the names of classes in images?\n",
      "A: Problem:\n",
      "When we run train_gen.flow_from_directory() as in video 8.5, it finds images belonging to 10 classes. Does it understand the names of classes from the names of folders? Or, there is already something going on deep behind?\n",
      "Solution:\n",
      "The name of class is the folder name\n",
      "If you just create some random folder with the name \"xyz\", it will also be considered as a class!! The name itself is saying flow_from_directory\n",
      "a clear explanation below:\n",
      "https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\n",
      "Added by Bhaskar Sarma\n",
      "\n",
      "Q: How keras flow_from_directory know the names of classes in images?\n",
      "A: Problem:\n",
      "When we run train_gen.flow_from_directory() as in video 8.5, it finds images belonging to 10 classes. Does it understand the names of classes from the names of folders? Or, there is already something going on deep behind?\n",
      "Solution:\n",
      "The name of class is the folder name\n",
      "If you just create some random folder with the name \"xyz\", it will also be considered as a class!! The name itself is saying flow_from_directory\n",
      "a clear explanation below:\n",
      "https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720\n",
      "Added by Bhaskar Sarma\n",
      "\n",
      "Q: Install kind through choco library\n",
      "A: First you need to launch a powershell terminal with administrator privilege.\n",
      "For this we need to install choco library first through the following syntax in powershell:\n",
      "Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\n",
      "Krishna Anand\n",
      "\n",
      "Q: Install kind through choco library\n",
      "A: First you need to launch a powershell terminal with administrator privilege.\n",
      "For this we need to install choco library first through the following syntax in powershell:\n",
      "Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\n",
      "Krishna Anand\n",
      "\n",
      "Q: Kitchenware Classification Competition Dataset Generator\n",
      "A: The image dataset for the competition was in a different layout from what we used in the dino vs dragon lesson. Since that’s what was covered, some folks were more comfortable with that setup, so I wrote a script that would generate it for them\n",
      "It can be found here: kitchenware-dataset-generator | Kaggle\n",
      "Martin Uribe\n",
      "\n",
      "Q: Dependence of the F-score on class imbalance\n",
      "A: Precision-recall curve, and thus the score, explicitly depends on the ratio  of positive to negative test cases. This means that comparison of the F-score across different problems with differing class ratios is problematic. One way to address this issue is to use a standard class ratio  when making such comparisons.\n",
      "(George Chizhmak)\n",
      "\n",
      "Q: Dependence of the F-score on class imbalance\n",
      "A: Precision-recall curve, and thus the score, explicitly depends on the ratio  of positive to negative test cases. This means that comparison of the F-score across different problems with differing class ratios is problematic. One way to address this issue is to use a standard class ratio  when making such comparisons.\n",
      "(George Chizhmak)\n",
      "\n",
      "Q: Model too big\n",
      "A: If your model is too big for github one option is to try and compress the model using joblib. For example joblib.dump(model, model_filename, compress=('zlib', 6) will use zlib to compress the model. Just note this could take a few moments as the model is being compressed.\n",
      "Quinn Avila\n",
      "\n",
      "Q: Model too big\n",
      "A: If your model is too big for github one option is to try and compress the model using joblib. For example joblib.dump(model, model_filename, compress=('zlib', 6) will use zlib to compress the model. Just note this could take a few moments as the model is being compressed.\n",
      "Quinn Avila\n",
      "\n",
      "Q: LinAlgError: Singular matrix\n",
      "A: It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\n",
      "You can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\n",
      "(Added by Cécile Guillot)\n",
      "\n",
      "Q: LinAlgError: Singular matrix\n",
      "A: It’s possible that when you follow the videos, you’ll get a Singular Matrix error. We will explain why it happens in the Regularization video. Don’t worry, it’s normal that you have it.\n",
      "You can also have an error because you did the inverse of X once in your code and you’re doing it a second time.\n",
      "(Added by Cécile Guillot)\n",
      "\n",
      "Q: Any advice for adding the Machine Learning Zoomcamp experience to your LinkedIn profile?\n",
      "A: I’ve seen LinkedIn users list DataTalksClub as Experience with titles as:\n",
      "Machine Learning Fellow\n",
      "Machine Learning Student\n",
      "Machine Learning Participant\n",
      "Machine Learning Trainee\n",
      "Please note it is best advised that you do not list the experience as an official “job” or “internship” experience since DataTalksClub did not hire you, nor financially compensate you.\n",
      "Other ways you can incorporate the experience in the following sections:\n",
      "Organizations\n",
      "Projects\n",
      "Skills\n",
      "Featured\n",
      "Original posts\n",
      "Certifications\n",
      "Courses\n",
      "By Annaliese Bronz\n",
      "Interesting question, I put the link of my project into my CV as showcase and make posts to show my progress.\n",
      "By Ani Mkrtumyan\n",
      "\n",
      "Q: California housing dataset\n",
      "A: You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\n",
      "KS\n",
      "\n",
      "Q: California housing dataset\n",
      "A: You can find a detailed description of the dataset ere https://inria.github.io/scikit-learn-mooc/python_scripts/datasets_california_housing.html\n",
      "KS\n",
      "\n",
      "Q: Help with understanding: “For each numerical value, use it as score and compute AUC”\n",
      "A: When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\n",
      "Sylvia Schmitt\n",
      "\n",
      "Q: Help with understanding: “For each numerical value, use it as score and compute AUC”\n",
      "A: When calculating the ROC AUC score using sklearn.metrics.roc_auc_score the function expects two parameters “y_true” and “y_score”. So for each numerical value in the dataframe it will be passed as the “y_score” to the function and the target variable will get passed a “y_true” each time.\n",
      "Sylvia Schmitt\n",
      "\n",
      "Q: About getting the wrong result when multiplying matrices\n",
      "A: When multiplying matrices, the order of multiplication is important.\n",
      "For example:\n",
      "A (m x n) * B (n x p) = C (m x p)\n",
      "B (n x p) * A (m x n) = D (n x n)\n",
      "C and D are matrices of different sizes and usually have different values. Therefore the order is important in matrix multiplication and changing the order changes the result.\n",
      "Baran Akın\n",
      "\n",
      "Q: Question 7: Mathematical formula for linear regression\n",
      "A: In Question 7 we are asked to calculate\n",
      "The initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\n",
      "Additional reading and videos:\n",
      "Ordinary least squares\n",
      "Multiple Linear Regression in Matrix Form\n",
      "Pseudoinverse Solution to OLS\n",
      "Added by Sylvia Schmitt\n",
      "with commends from Dmytro Durach\n",
      "\n",
      "Q: Question 7: Mathematical formula for linear regression\n",
      "A: In Question 7 we are asked to calculate\n",
      "The initial problem  can be solved by this, where a Matrix X is multiplied by some unknown weights w resulting in the target y.\n",
      "Additional reading and videos:\n",
      "Ordinary least squares\n",
      "Multiple Linear Regression in Matrix Form\n",
      "Pseudoinverse Solution to OLS\n",
      "Added by Sylvia Schmitt\n",
      "with commends from Dmytro Durach\n",
      "\n",
      "Q: What is the use of inverting or negating the variables less than the threshold?\n",
      "A: Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms.\n",
      "Aileah Gotladera\n",
      "\n",
      "Q: Features Importance graph\n",
      "A: I like this visual implementation of features importance in scikit-learn library:\n",
      "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
      "It actually adds std.errors to features importance -> so that you can trace stability of features (important for a model’s explainability) over the different params of the model.\n",
      "Ivan Brigida\n",
      "\n",
      "Q: What is the use of inverting or negating the variables less than the threshold?\n",
      "A: Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms.\n",
      "Aileah Gotladera\n",
      "\n",
      "Q: Features Importance graph\n",
      "A: I like this visual implementation of features importance in scikit-learn library:\n",
      "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
      "It actually adds std.errors to features importance -> so that you can trace stability of features (important for a model’s explainability) over the different params of the model.\n",
      "Ivan Brigida\n",
      "\n",
      "Q: I didn’t fully understand the ROC curve. Can I move on?\n",
      "A: It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\n",
      "Nonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\n",
      "\n",
      "Q: I didn’t fully understand the ROC curve. Can I move on?\n",
      "A: It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\n",
      "Nonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models.\n",
      "\n",
      "Q: The answer I get for one of the homework questions doesn't match any of the options. What should I do?\n",
      "A: That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\n",
      "If it’s the case, just select the option that’s closest to your answer\n",
      "\n",
      "Q: The answer I get for one of the homework questions doesn't match any of the options. What should I do?\n",
      "A: That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\n",
      "If it’s the case, just select the option that’s closest to your answer\n",
      "\n",
      "Q: Sparse matrix compared dense matrix\n",
      "A: A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\n",
      "The default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\n",
      " \t\t\t\t\t\t\t\t\t\t\t\tQuinn Avila\n",
      "\n",
      "Q: Sparse matrix compared dense matrix\n",
      "A: A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\n",
      "The default DictVectorizer configuration is a sparse matrix. For week3 Q6 using the default sparse is an interesting option because of the size of the matrix. Training the model was also more performant and didn’t give an error message like dense mode.\n",
      " \t\t\t\t\t\t\t\t\t\t\t\tQuinn Avila\n",
      "\n",
      "Q: Where does pipenv create environments and how does it name them?\n",
      "A: It creates them in\n",
      "OSX/Linux: ~/.local/share/virtualenvs/folder-name_cyrptic-hash\n",
      "Windows: C:\\Users\\<USERNAME>\\.virtualenvs\\folder-name_cyrptic-hash\n",
      "Eg: C:\\Users\\Ella\\.virtualenvs\\code-qsdUdabf (for module-05 lesson)\n",
      "The environment name is the name of the last folder in the folder directory where we used the pipenv install command (or any other pipenv command). E.g. If you run any pipenv command in folder path ~/home/user/Churn-Flask-app, it will create an environment named Churn-Flask-app-some_random_characters, and it's path will be like this: /home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX.\n",
      "All libraries of this environment will be installed inside this folder. To activate this environment, I will need to cd into the project folder again, and type pipenv shell. In short, the location of the project folder acts as an identifier for an environment, in place of any name.\n",
      "(Memoona Tahira)\n",
      "\n",
      "Q: Where does pipenv create environments and how does it name them?\n",
      "A: It creates them in\n",
      "OSX/Linux: ~/.local/share/virtualenvs/folder-name_cyrptic-hash\n",
      "Windows: C:\\Users\\<USERNAME>\\.virtualenvs\\folder-name_cyrptic-hash\n",
      "Eg: C:\\Users\\Ella\\.virtualenvs\\code-qsdUdabf (for module-05 lesson)\n",
      "The environment name is the name of the last folder in the folder directory where we used the pipenv install command (or any other pipenv command). E.g. If you run any pipenv command in folder path ~/home/user/Churn-Flask-app, it will create an environment named Churn-Flask-app-some_random_characters, and it's path will be like this: /home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX.\n",
      "All libraries of this environment will be installed inside this folder. To activate this environment, I will need to cd into the project folder again, and type pipenv shell. In short, the location of the project folder acts as an identifier for an environment, in place of any name.\n",
      "(Memoona Tahira)\n",
      "\n",
      "Q: What data should be used for EDA?\n",
      "A: Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\n",
      "It's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\n",
      "Alena Kniazeva\n",
      "\n",
      "Q: What is the difference between OneHotEncoder and DictVectorizer?\n",
      "A: Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\n",
      "Both will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\n",
      "Tanya Mard\n",
      "\n",
      "Q: What data should be used for EDA?\n",
      "A: Should we perform EDA on the base of train or train+validation or train+validation+test dataset?\n",
      "It's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset, even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data\n",
      "Alena Kniazeva\n",
      "\n",
      "Q: What is the difference between OneHotEncoder and DictVectorizer?\n",
      "A: Both work in similar ways. That is, to convert categorical features to numerical variables for use in training the model. But the difference lies in the input. OneHotEncoder uses an array as input while DictVectorizer uses a dictionary.\n",
      "Both will produce the same result. But when we use OneHotEncoder, features are sorted alphabetically. When you use DictVectorizer you stack features that you want.\n",
      "Tanya Mard\n",
      "\n",
      "Q: Errors with istio during installation\n",
      "A: Problem description:\n",
      "Running this:\n",
      "curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\" | bash\n",
      "Fails with errors because of istio failing to update resources, and you are on kubectl > 1.25.0.\n",
      "Check kubectl version with kubectl version\n",
      "Solution description\n",
      "Edit the file “quick_install.bash” by downloading it with curl without running bash. Edit the versions of Istio and Knative as per the matrix on the KServe website.\n",
      "Run the bash script now.\n",
      "Added by Andrew Katoch\n",
      "\n",
      "Q: Errors with istio during installation\n",
      "A: Problem description:\n",
      "Running this:\n",
      "curl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\" | bash\n",
      "Fails with errors because of istio failing to update resources, and you are on kubectl > 1.25.0.\n",
      "Check kubectl version with kubectl version\n",
      "Solution description\n",
      "Edit the file “quick_install.bash” by downloading it with curl without running bash. Edit the versions of Istio and Knative as per the matrix on the KServe website.\n",
      "Run the bash script now.\n",
      "Added by Andrew Katoch\n",
      "\n",
      "Q: Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc.\n",
      "A: While weeks 1-4 can relatively easily be followed and the associated homework completed with just about any default environment / local setup, week 5 introduces several layers of abstraction and dependencies.\n",
      "It is advised to prepare your “homework environment” with a cloud provider of your choice. A thorough step-by-step guide for doing so for an AWS EC2 instance is provided in an introductory video taken from the MLOPS course here:\n",
      "https://www.youtube.com/watch?v=IXSiYkP23zo\n",
      "Note that (only) small AWS instances can be run for free, and that larger ones will be billed hourly based on usage (but can and should be stopped when not in use).\n",
      "Alternative ways are sketched here:\n",
      "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md\n",
      "\n",
      "Q: Getting: Allocator ran out of memory errors?\n",
      "A: If you are running tensorflow on your own machine and you start getting the following errors:\n",
      "Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "Try adding this code in a cell at the beginning of your notebook:\n",
      "config = tf.compat.v1.ConfigProto()\n",
      "config.gpu_options.allow_growth = True\n",
      "session = tf.compat.v1.Session(config=config)\n",
      "After doing this most of my issues went away. I say most because there was one instance when I still got the error once more, but only during one epoch. I ran the code again, right after it finished, and I never saw the error again.\n",
      "Added by Martin Uribe\n",
      "\n",
      "Q: Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc.\n",
      "A: While weeks 1-4 can relatively easily be followed and the associated homework completed with just about any default environment / local setup, week 5 introduces several layers of abstraction and dependencies.\n",
      "It is advised to prepare your “homework environment” with a cloud provider of your choice. A thorough step-by-step guide for doing so for an AWS EC2 instance is provided in an introductory video taken from the MLOPS course here:\n",
      "https://www.youtube.com/watch?v=IXSiYkP23zo\n",
      "Note that (only) small AWS instances can be run for free, and that larger ones will be billed hourly based on usage (but can and should be stopped when not in use).\n",
      "Alternative ways are sketched here:\n",
      "https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md\n",
      "\n",
      "Q: Getting: Allocator ran out of memory errors?\n",
      "A: If you are running tensorflow on your own machine and you start getting the following errors:\n",
      "Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "Try adding this code in a cell at the beginning of your notebook:\n",
      "config = tf.compat.v1.ConfigProto()\n",
      "config.gpu_options.allow_growth = True\n",
      "session = tf.compat.v1.Session(config=config)\n",
      "After doing this most of my issues went away. I say most because there was one instance when I still got the error once more, but only during one epoch. I ran the code again, right after it finished, and I never saw the error again.\n",
      "Added by Martin Uribe\n",
      "\n",
      "Q: Module5 HW Question 6\n",
      "A: The provided image FROM svizor/zoomcamp-model:3.10.12-slim has a model and dictvectorizer that should be used for question 6. \"model2.bin\", \"dv.bin\"\n",
      "Added by Quinn Avila\n",
      "\n",
      "Q: Module5 HW Question 6\n",
      "A: The provided image FROM svizor/zoomcamp-model:3.10.12-slim has a model and dictvectorizer that should be used for question 6. \"model2.bin\", \"dv.bin\"\n",
      "Added by Quinn Avila\n",
      "\n",
      "Q: Retrieving csv inside notebook\n",
      "A: You can use\n",
      "!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "To download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\n",
      "For instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\n",
      "!mkdir -p ../data/\n",
      "!mv housing.csv ../data/\n",
      "\n",
      "Q: Retrieving csv inside notebook\n",
      "A: You can use\n",
      "!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "To download the data too. The exclamation mark !, lets you execute shell commands inside your notebooks. This works generally for shell commands such as ls, cp, mkdir, mv etc . . .\n",
      "For instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\n",
      "!mkdir -p ../data/\n",
      "!mv housing.csv ../data/\n",
      "\n",
      "Q: What is the difference between bagging and boosting?\n",
      "A: For ensemble algorithms, during the week 6, one bagging algorithm and one boosting algorithm were presented: Random Forest and XGBoost, respectively.\n",
      "Random Forest trains several models in parallel. The output can be, for example, the average value of all the outputs of each model. This is called bagging.\n",
      "XGBoost trains several models sequentially: the previous model error is used to train the following model. Weights are used to ponderate the models such as the best models have higher weights and are therefore favored for the final output. This method is called boosting.\n",
      "Note that boosting is not necessarily better than bagging.\n",
      "Mélanie Fouesnard\n",
      "Bagging stands for “Bootstrap Aggregation” - it involves taking multiple samples with replacement to derive multiple training datasets from the original training dataset (bootstrapping), training a classifier (e.g. decision trees or stumps for Random Forests) on each such training dataset, and then combining the the predictions (aggregation) to obtain the final prediction. For classification, predictions are combined via voting; for regression, via averaging. Bagging can be done in parallel, since the various classifiers are independent. Bagging decreases variance (but not bias) and is robust against overfitting.\n",
      "Boosting, on the other hand, is sequential - each model learns from the mistakes of its predecessor. Observations are given different weights - observations/samples misclassified by the previous classifier are given a higher weight, and this process is continued until a stopping condition is reached (e.g. max. No. of models is reached, or error is acceptably small, etc.). Boosting reduces bias & is generally more accurate than bagging, but can be prone to overfitting.\n",
      "Rileen\n",
      "\n",
      "Q: What is the difference between bagging and boosting?\n",
      "A: For ensemble algorithms, during the week 6, one bagging algorithm and one boosting algorithm were presented: Random Forest and XGBoost, respectively.\n",
      "Random Forest trains several models in parallel. The output can be, for example, the average value of all the outputs of each model. This is called bagging.\n",
      "XGBoost trains several models sequentially: the previous model error is used to train the following model. Weights are used to ponderate the models such as the best models have higher weights and are therefore favored for the final output. This method is called boosting.\n",
      "Note that boosting is not necessarily better than bagging.\n",
      "Mélanie Fouesnard\n",
      "Bagging stands for “Bootstrap Aggregation” - it involves taking multiple samples with replacement to derive multiple training datasets from the original training dataset (bootstrapping), training a classifier (e.g. decision trees or stumps for Random Forests) on each such training dataset, and then combining the the predictions (aggregation) to obtain the final prediction. For classification, predictions are combined via voting; for regression, via averaging. Bagging can be done in parallel, since the various classifiers are independent. Bagging decreases variance (but not bias) and is robust against overfitting.\n",
      "Boosting, on the other hand, is sequential - each model learns from the mistakes of its predecessor. Observations are given different weights - observations/samples misclassified by the previous classifier are given a higher weight, and this process is continued until a stopping condition is reached (e.g. max. No. of models is reached, or error is acceptably small, etc.). Boosting reduces bias & is generally more accurate than bagging, but can be prone to overfitting.\n",
      "Rileen\n",
      "\n",
      "Q: Caution for applying log transformation in Week-2 2023 cohort homework\n",
      "A: The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\n",
      "However, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\n",
      "(Added by Soham Mundhada)\n",
      "\n",
      "Q: Caution for applying log transformation in Week-2 2023 cohort homework\n",
      "A: The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\n",
      "However, this instruction is absent in the subsequent questions of the homework, and I got stuck with Q5 for a long time, trying to figure out why my RMSE was so huge, when it clicked to me that I forgot to apply log transformation to the target variable. Please remember to apply log transformation to the target variable for each question.\n",
      "(Added by Soham Mundhada)\n",
      "\n",
      "Q: Logistic regression crashing Jupyter kernel\n",
      "A: Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\n",
      "Make sure that the target variable for the logistic regression is binary.\n",
      "Konrad Muehlberg\n",
      "\n",
      "Q: Logistic regression crashing Jupyter kernel\n",
      "A: Fitting the logistic regression takes a long time / kernel crashes when calling predict() with the fitted model.\n",
      "Make sure that the target variable for the logistic regression is binary.\n",
      "Konrad Muehlberg\n",
      "\n",
      "Q: What does ‘long tail’ mean?\n",
      "A: One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\n",
      "(Tatiana Dávila)\n",
      "\n",
      "Q: What does ‘long tail’ mean?\n",
      "A: One of the most important characteristics of the normal distribution is that mean=median=mode, this means that the most popular value, the mean of the distribution and 50% of the sample are under the same value, this is equivalent to say that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) is the result of having a few observations with high values, now the behaviour of the distribution changes, first of all, the area is different on each side and now the mean, median and mode are different. As a consequence, the mean is no longer representative, the range is larger than before and the probability of being on the left or on the right is not the same.\n",
      "(Tatiana Dávila)\n",
      "\n",
      "Q: HW3Q4 I am getting 1.0 as accuracy. Should I use the closest option?\n",
      "A: If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\n",
      "Added by Akshar Goyal\n",
      "\n",
      "Q: HW3Q4 I am getting 1.0 as accuracy. Should I use the closest option?\n",
      "A: If you are getting 1.0 as accuracy then there is a possibility you have overfitted the model. Dropping the column msrp/price can help you solve this issue.\n",
      "Added by Akshar Goyal\n",
      "\n",
      "Q: Warning: the environment variable LANG is not set!\n",
      "A: Q2.1: Use Pipenv to install Scikit-Learn version 1.3.1\n",
      "This is an error I got while executing the above step in the ml-zoomcamp conda environment. The error is not fatal and just warns you that explicit language specifications are not set out in our bash profile. A quick-fix is here:\n",
      "https://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma\n",
      "But one can proceed without addressing it.\n",
      "Added by Abhirup Ghosh\n",
      "\n",
      "Q: Warning: the environment variable LANG is not set!\n",
      "A: Q2.1: Use Pipenv to install Scikit-Learn version 1.3.1\n",
      "This is an error I got while executing the above step in the ml-zoomcamp conda environment. The error is not fatal and just warns you that explicit language specifications are not set out in our bash profile. A quick-fix is here:\n",
      "https://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma\n",
      "But one can proceed without addressing it.\n",
      "Added by Abhirup Ghosh\n",
      "\n",
      "Q: Multiple thresholds for Q4\n",
      "A: I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\n",
      "Choose the one closest to any of the options\n",
      "Added by Azeez Enitan Edunwale\n",
      "You can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\n",
      "Added by Rileen Sinha\n",
      "\n",
      "Q: Multiple thresholds for Q4\n",
      "A: I am getting multiple thresholds with the same F1 score, does this indicate I am doing something wrong or is there a method for choosing? I would assume just pick the lowest?\n",
      "Choose the one closest to any of the options\n",
      "Added by Azeez Enitan Edunwale\n",
      "You can always use scikit-learn (or other standard libraries/packages) to verify results obtained using your own code, e.g. you can use  “classification_report” (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to obtain precision, recall and F1-score.\n",
      "Added by Rileen Sinha\n",
      "\n",
      "Q: pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings:\n",
      "A: DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\n",
      "Using “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\n",
      "Larkin Andrii\n",
      "\n",
      "Q: pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings:\n",
      "A: DictVectorizer(sparse=True) produces CSR format, which is both more memory efficient and converges better during fit(). Basically it stores non-zero values and indices instead of adding a column for each class of each feature (models of cars produced 900+ columns alone in the current task).\n",
      "Using “sparse” format like on the picture above, both via pandas.get_dummies() and DictVectorizer(sparse=False) - is slower (around 6-8min for Q6 task - Linear/Ridge Regression) for high amount of classes (like models of cars for eg) and gives a bit “worse” results in both Logistic and Linear/Ridge Regression, while also producing convergence warnings for Linear/Ridge Regression.\n",
      "Larkin Andrii\n",
      "\n",
      "Q: Keras model training fails with “Failed to find data adapter”\n",
      "A: While training a Keras model you get the error “Failed to find data adapter that can handle input: <class 'keras.src.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>” you may have unintentionally passed the image generator instead of the dataset to the model\n",
      "train_gen = ImageDataGenerator(rescale=1./255)\n",
      "train_ds = train_gen.flow_from_directory(…)\n",
      "history_after_augmentation = model.fit(\n",
      "train_gen, # this should be train_ds!!!\n",
      "epochs=10,\n",
      "validation_data=test_gen # this should be test_ds!!!\n",
      ")\n",
      "The fix is simple and probably obvious once pointed out, use the training and validation dataset (train_ds and val_ds) returned from flow_from_directory\n",
      "Added by Tzvi Friedman\n",
      "\n",
      "Q: Keras model training fails with “Failed to find data adapter”\n",
      "A: While training a Keras model you get the error “Failed to find data adapter that can handle input: <class 'keras.src.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>” you may have unintentionally passed the image generator instead of the dataset to the model\n",
      "train_gen = ImageDataGenerator(rescale=1./255)\n",
      "train_ds = train_gen.flow_from_directory(…)\n",
      "history_after_augmentation = model.fit(\n",
      "train_gen, # this should be train_ds!!!\n",
      "epochs=10,\n",
      "validation_data=test_gen # this should be test_ds!!!\n",
      ")\n",
      "The fix is simple and probably obvious once pointed out, use the training and validation dataset (train_ds and val_ds) returned from flow_from_directory\n",
      "Added by Tzvi Friedman\n",
      "\n",
      "Q: Convergence Problems in W3Q6\n",
      "A: Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "Play with different scalers. See notebook-scaling-ohe.ipynb\n",
      "Dmytro Durach\n",
      "(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.\n",
      "\n",
      "Q: Convergence Problems in W3Q6\n",
      "A: Ridge with sag solver requires feature to be of the same scale. You may get the following warning: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "Play with different scalers. See notebook-scaling-ohe.ipynb\n",
      "Dmytro Durach\n",
      "(Oscar Garcia)  Use a StandardScaler for the numeric fields and OneHotEncoder (sparce = False) for the categorical features.  This help with the warning. Separate the features (num/cat) without using the encoder first and see if that helps.\n",
      "\n",
      "Q: Missing channel value error while reloading model:\n",
      "A: While doing:\n",
      "import tensorflow as tf\n",
      "from tensorflow import keras\n",
      "model = tf.keras.models.load_model('model_saved.h5')\n",
      "If you get an error message like this:\n",
      "ValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.\n",
      "Solution:\n",
      "Saving a model (either yourself via model.save() or via checkpoint when save_weights_only = False) saves two things: The trained model weights (for example the best weights found during training) and the model architecture.  If the number of channels is not explicitly specified in the Input layer of the model, and is instead defined as a variable, the model architecture will not have the value in the variable stored. Therefore when the model is reloaded, it will complain about not knowing the number of channels. See the code below, in the first line, you need to specify number of channels explicitly:\n",
      "# model architecture:\n",
      "inputs = keras.Input(shape=(input_size, input_size, 3))\n",
      "base = base_model(inputs, training=False)\n",
      "vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
      "inner = keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
      "drop = keras.layers.Dropout(droprate)(inner)\n",
      "outputs = keras.layers.Dense(10)(drop)\n",
      "model = keras.Model(inputs, outputs)\n",
      "(Memoona Tahira)\n",
      "\n",
      "Q: Missing channel value error while reloading model:\n",
      "A: While doing:\n",
      "import tensorflow as tf\n",
      "from tensorflow import keras\n",
      "model = tf.keras.models.load_model('model_saved.h5')\n",
      "If you get an error message like this:\n",
      "ValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.\n",
      "Solution:\n",
      "Saving a model (either yourself via model.save() or via checkpoint when save_weights_only = False) saves two things: The trained model weights (for example the best weights found during training) and the model architecture.  If the number of channels is not explicitly specified in the Input layer of the model, and is instead defined as a variable, the model architecture will not have the value in the variable stored. Therefore when the model is reloaded, it will complain about not knowing the number of channels. See the code below, in the first line, you need to specify number of channels explicitly:\n",
      "# model architecture:\n",
      "inputs = keras.Input(shape=(input_size, input_size, 3))\n",
      "base = base_model(inputs, training=False)\n",
      "vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
      "inner = keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
      "drop = keras.layers.Dropout(droprate)(inner)\n",
      "outputs = keras.layers.Dense(10)(drop)\n",
      "model = keras.Model(inputs, outputs)\n",
      "(Memoona Tahira)\n",
      "\n",
      "Q: Model breaking after augmentation – high loss + bad accuracy\n",
      "A: Problem:\n",
      "When resuming training after augmentation, the loss skyrockets (1000+ during first epoch) and accuracy settles around 0.5 – i.e. the model becomes as good as a random coin flip.\n",
      "Solution:\n",
      "Check that the augmented ImageDataGenerator still includes the option “rescale” as specified in the preceding step.\n",
      "Added by Konrad Mühlberg\n",
      "\n",
      "Q: Model breaking after augmentation – high loss + bad accuracy\n",
      "A: Problem:\n",
      "When resuming training after augmentation, the loss skyrockets (1000+ during first epoch) and accuracy settles around 0.5 – i.e. the model becomes as good as a random coin flip.\n",
      "Solution:\n",
      "Check that the augmented ImageDataGenerator still includes the option “rescale” as specified in the preceding step.\n",
      "Added by Konrad Mühlberg\n",
      "\n",
      "Q: Question 3 of homework 6 if i see that rmse goes up at a certain number of n_estimators but then goes back down lower than it was before, should the answer be the number of n_estimators after which rmse initially went up, or the number after which it was its overall lowest value?\n",
      "A: When rmse stops improving means, when it stops to decrease or remains almost similar.\n",
      "Pastor Soto\n",
      "\n",
      "Q: Question 3 of homework 6 if i see that rmse goes up at a certain number of n_estimators but then goes back down lower than it was before, should the answer be the number of n_estimators after which rmse initially went up, or the number after which it was its overall lowest value?\n",
      "A: When rmse stops improving means, when it stops to decrease or remains almost similar.\n",
      "Pastor Soto\n",
      "\n",
      "Q: Evaluate the Model using scikit learn metrics\n",
      "A: Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\n",
      "from sklearn.metrics import (accuracy_score,\n",
      "precision_score,\n",
      "recall_score,\n",
      "f1_score,\n",
      "roc_auc_score\n",
      ")\n",
      "accuracy = accuracy_score(y_val, y_pred)\n",
      "precision = precision_score(y_val, y_pred)\n",
      "recall = recall_score(y_val, y_pred)\n",
      "f1 = f1_score(y_val, y_pred)\n",
      "roc_auc = roc_auc_score(y_val, y_pred)\n",
      "print(f'Accuracy: {accuracy}')\n",
      "print(f'Precision: {precision}')\n",
      "print(f'Recall: {recall}')\n",
      "print(f'F1-Score: {f1}')\n",
      "print(f'ROC AUC: {roc_auc}')\n",
      "(Harish Balasundaram)\n",
      "\n",
      "Q: Evaluate the Model using scikit learn metrics\n",
      "A: Model evaluation metrics can be easily computed using off the shelf calculations available in scikit learn library. This saves a lot of time and more precise compared to our own calculations from the scratch using numpy and pandas libraries.\n",
      "from sklearn.metrics import (accuracy_score,\n",
      "precision_score,\n",
      "recall_score,\n",
      "f1_score,\n",
      "roc_auc_score\n",
      ")\n",
      "accuracy = accuracy_score(y_val, y_pred)\n",
      "precision = precision_score(y_val, y_pred)\n",
      "recall = recall_score(y_val, y_pred)\n",
      "f1 = f1_score(y_val, y_pred)\n",
      "roc_auc = roc_auc_score(y_val, y_pred)\n",
      "print(f'Accuracy: {accuracy}')\n",
      "print(f'Precision: {precision}')\n",
      "print(f'Recall: {recall}')\n",
      "print(f'F1-Score: {f1}')\n",
      "print(f'ROC AUC: {roc_auc}')\n",
      "(Harish Balasundaram)\n",
      "\n",
      "Q: Second variable that we need to use to calculate the mutual information score\n",
      "A: Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\n",
      "Answer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\n",
      "Asia Saeed\n",
      "\n",
      "Q: Second variable that we need to use to calculate the mutual information score\n",
      "A: Question: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?\n",
      "Answer: You need to calculate the mutual info score between the binarized price (above_average) variable & ocean_proximity, the only original categorical variable in the dataset.\n",
      "Asia Saeed\n",
      "\n",
      "Q: ValueError: feature_names must be string, and may not contain [, ] or <\n",
      "A: This error occurs because the list of feature names contains some characters like \"<\" that are not supported. To fix this issue, you can replace those problematic characters with supported ones. If you want to create a consistent list of features with no special characters, you can achieve it like this:\n",
      "You can address this error by replacing problematic characters in the feature names with underscores, like so:\n",
      "features = [f.replace('=<', '_').replace('=', '_') for f in features]\n",
      "This code will go through the list of features and replace any instances of \"=<\" with \"\", as well as any \"=\" with \"\", ensuring that the feature names only consist of supported characters.\n",
      "\n",
      "Q: ValueError: feature_names must be string, and may not contain [, ] or <\n",
      "A: This error occurs because the list of feature names contains some characters like \"<\" that are not supported. To fix this issue, you can replace those problematic characters with supported ones. If you want to create a consistent list of features with no special characters, you can achieve it like this:\n",
      "You can address this error by replacing problematic characters in the feature names with underscores, like so:\n",
      "features = [f.replace('=<', '_').replace('=', '_') for f in features]\n",
      "This code will go through the list of features and replace any instances of \"=<\" with \"\", as well as any \"=\" with \"\", ensuring that the feature names only consist of supported characters.\n",
      "\n",
      "Q: Conda is not an internal command\n",
      "A: I have a problem with my terminal. Command\n",
      "conda create -n ml-zoomcamp python=3.9\n",
      "doesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\n",
      "If you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\n",
      "If you don’t have Anaconda or Miniconda, you should install it first\n",
      "(Tatyana Mardvilko)\n",
      "\n",
      "Q: Conda is not an internal command\n",
      "A: I have a problem with my terminal. Command\n",
      "conda create -n ml-zoomcamp python=3.9\n",
      "doesn’t work. Any of 3.8/ 3.9 / 3.10 should be all fine\n",
      "If you’re on Windows and just installed Anaconda, you can use Anaconda’s own terminal called “Anaconda Prompt”.\n",
      "If you don’t have Anaconda or Miniconda, you should install it first\n",
      "(Tatyana Mardvilko)\n",
      "\n",
      "Q: Error with the line “interpreter.set_tensor(input_index, X”)\n",
      "A: I had this error when running the command line : interpreter.set_tensor(input_index, x) that can be seen in the video 9.3 around 12 minutes.\n",
      "ValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\n",
      "This is because the X is an int but a float is expected.\n",
      "Solution:\n",
      "I found this solution from this question here https://stackoverflow.com/questions/76102508/valueerror-cannot-set-tensor-got-value-of-type-float64-but-expected-type-float :\n",
      "# Need to convert to float32 before set_tensor\n",
      "X = np.float32(X)\n",
      "Then, it works. I work with tensorflow 2.15.0, maybe the fact that this version is more recent involves this change ?\n",
      "Added by Mélanie Fouesnard\n",
      "\n",
      "Q: Error with the line “interpreter.set_tensor(input_index, X”)\n",
      "A: I had this error when running the command line : interpreter.set_tensor(input_index, x) that can be seen in the video 9.3 around 12 minutes.\n",
      "ValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\n",
      "This is because the X is an int but a float is expected.\n",
      "Solution:\n",
      "I found this solution from this question here https://stackoverflow.com/questions/76102508/valueerror-cannot-set-tensor-got-value-of-type-float64-but-expected-type-float :\n",
      "# Need to convert to float32 before set_tensor\n",
      "X = np.float32(X)\n",
      "Then, it works. I work with tensorflow 2.15.0, maybe the fact that this version is more recent involves this change ?\n",
      "Added by Mélanie Fouesnard\n",
      "\n",
      "Q: Question 7: Multiplication operators.\n",
      "A: Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\n",
      "numpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\n",
      "If multiplying by a scalar numpy.multiply() or * is preferred.\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Question 7: Multiplication operators.\n",
      "A: Note, that matrix multiplication (matrix-matrix, matrix-vector multiplication) can be written as * operator in some sources, but performed as @ operator or np.matmul() via numpy. * operator performs element-wise multiplication (Hadamard product).\n",
      "numpy.dot() or ndarray.dot() can be used, but for matrix-matrix multiplication @ or np.matmul() is preferred (as per numpy doc).\n",
      "If multiplying by a scalar numpy.multiply() or * is preferred.\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Is it going to be live? When?\n",
      "A: The course videos are pre-recorded, you can start watching the course right now.\n",
      "We will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\n",
      "You can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.\n",
      "\n",
      "Q: Quick way to plot Precision-Recall Curve\n",
      "A: We can import precision_recall_curve from scikit-learn and plot the graph as follows:\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "precision, recall, thresholds = precision_recall_curve(y_val, y_predict)\n",
      "plt.plot(thresholds, precision[:-1], label='Precision')\n",
      "plt.plot(thresholds, recall[:-1], label='Recall')\n",
      "plt.legend()\n",
      "Hrithik Kumar Advani\n",
      "\n",
      "Q: Is it going to be live? When?\n",
      "A: The course videos are pre-recorded, you can start watching the course right now.\n",
      "We will also occasionally have office hours - live sessions where we will answer your questions. The office hours sessions are recorded too.\n",
      "You can see the office hours as well as the pre-recorded course videos in the course playlist on YouTube.\n",
      "\n",
      "Q: Quick way to plot Precision-Recall Curve\n",
      "A: We can import precision_recall_curve from scikit-learn and plot the graph as follows:\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "precision, recall, thresholds = precision_recall_curve(y_val, y_predict)\n",
      "plt.plot(thresholds, precision[:-1], label='Precision')\n",
      "plt.plot(thresholds, recall[:-1], label='Recall')\n",
      "plt.legend()\n",
      "Hrithik Kumar Advani\n",
      "\n",
      "Q: Executing the command echo ${REMOTE_URI} returns nothing.\n",
      "A: Solution description\n",
      "In the unit 9.6, Alexey ran the command echo ${REMOTE_URI} which turned the URI address in the terminal. There workaround is to set a local variable (REMOTE_URI) and assign your URI address in the terminal and use it to login the registry, for instance, REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images (fake address). One caveat is that you will lose this variable once the session is terminated.\n",
      "I also had the same problem on Ubuntu terminal. I executed the following two commands:\n",
      "$ export REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\n",
      "$ echo $REMOTE_URI\n",
      "111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\n",
      "Note: 1. no curly brackets (e.g. echo ${REMOTE_URI}) needed unlike in video 9.6,\n",
      "2. Replace REMOTE_URI with your URI\n",
      "(Bhaskar Sarma)\n",
      "\n",
      "Q: Executing the command echo ${REMOTE_URI} returns nothing.\n",
      "A: Solution description\n",
      "In the unit 9.6, Alexey ran the command echo ${REMOTE_URI} which turned the URI address in the terminal. There workaround is to set a local variable (REMOTE_URI) and assign your URI address in the terminal and use it to login the registry, for instance, REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images (fake address). One caveat is that you will lose this variable once the session is terminated.\n",
      "I also had the same problem on Ubuntu terminal. I executed the following two commands:\n",
      "$ export REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\n",
      "$ echo $REMOTE_URI\n",
      "111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\n",
      "Note: 1. no curly brackets (e.g. echo ${REMOTE_URI}) needed unlike in video 9.6,\n",
      "2. Replace REMOTE_URI with your URI\n",
      "(Bhaskar Sarma)\n",
      "\n",
      "Q: Q: Where does the number of Conv2d layer’s params come from? Where does the number of “features” we get after the Flatten layer come from?\n",
      "A: Let’s say we define our Conv2d layer like this:\n",
      ">> tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))\n",
      "It means our input image is RGB (3 channels, 150 by 150 pixels), kernel is 3x3 and number of filters (layer’s width) is 32.\n",
      "If we check model.summary() we will get this:\n",
      "_________________________________________________________________\n",
      "Layer (type)                Output Shape              Param #\n",
      "=================================================================\n",
      "conv2d (Conv2D)             (None, 148, 148, 32)      896\n",
      "So where does 896 params come from? It’s computed like this:\n",
      ">>> (3*3*3 +1) * 32\n",
      "896\n",
      "# 3x3 kernel, 3 channels RGB, +1 for bias, 32 filters\n",
      "What about the number of “features” we get after the Flatten layer?\n",
      "For our homework model.summary() for last MaxPooling2d and Flatten layers looked like this:\n",
      "_________________________________________________________________\n",
      "Layer (type)                Output Shape              Param #\n",
      "=================================================================\n",
      "max_pooling2d_3       (None, 7, 7, 128)         0\n",
      "flatten (Flatten)           (None, 6272)              0\n",
      "So where do 6272 vectors come from? It’s computed like this:\n",
      ">>> 7*7*128\n",
      "6272\n",
      "# 7x7 “image shape” after several convolutions and poolings, 128 filters\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Q: Where does the number of Conv2d layer’s params come from? Where does the number of “features” we get after the Flatten layer come from?\n",
      "A: Let’s say we define our Conv2d layer like this:\n",
      ">> tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))\n",
      "It means our input image is RGB (3 channels, 150 by 150 pixels), kernel is 3x3 and number of filters (layer’s width) is 32.\n",
      "If we check model.summary() we will get this:\n",
      "_________________________________________________________________\n",
      "Layer (type)                Output Shape              Param #\n",
      "=================================================================\n",
      "conv2d (Conv2D)             (None, 148, 148, 32)      896\n",
      "So where does 896 params come from? It’s computed like this:\n",
      ">>> (3*3*3 +1) * 32\n",
      "896\n",
      "# 3x3 kernel, 3 channels RGB, +1 for bias, 32 filters\n",
      "What about the number of “features” we get after the Flatten layer?\n",
      "For our homework model.summary() for last MaxPooling2d and Flatten layers looked like this:\n",
      "_________________________________________________________________\n",
      "Layer (type)                Output Shape              Param #\n",
      "=================================================================\n",
      "max_pooling2d_3       (None, 7, 7, 128)         0\n",
      "flatten (Flatten)           (None, 6272)              0\n",
      "So where do 6272 vectors come from? It’s computed like this:\n",
      ">>> 7*7*128\n",
      "6272\n",
      "# 7x7 “image shape” after several convolutions and poolings, 128 filters\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Submitting learning in public links\n",
      "A: When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\n",
      "For posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\n",
      "The same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\n",
      "For midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.\n",
      "\n",
      "Q: Submitting learning in public links\n",
      "A: When you post about what you learned from the course on your social media pages, use the tag #mlzoomcamp. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\n",
      "For posting the learning in public links, you get extra scores. But the number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\n",
      "The same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week, see Alexey’s reply. (~ ellacharmed)\n",
      "For midterms/capstones, the awarded points are doubled as the duration is longer. So for projects the points are capped at 14 for 14 URLs.\n",
      "\n",
      "Q: Floating Point Precision\n",
      "A: I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\n",
      "Inverse * Original:\n",
      "[[ 1.00000000e+00 -1.38777878e-16]\n",
      "[ 3.16968674e-13  1.00000000e+00]]\n",
      "Solution:\n",
      "It's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\n",
      "(Added by Wesley Barreto)\n",
      "\n",
      "Q: Floating Point Precision\n",
      "A: I was doing Question 7 from Week1 Homework and with step6: Invert XTX, I created the inverse. Now, an inverse when multiplied by the original matrix should return in an Identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\n",
      "Inverse * Original:\n",
      "[[ 1.00000000e+00 -1.38777878e-16]\n",
      "[ 3.16968674e-13  1.00000000e+00]]\n",
      "Solution:\n",
      "It's because floating point math doesn't work well on computers as shown here: https://stackoverflow.com/questions/588004/is-floating-point-math-broken\n",
      "(Added by Wesley Barreto)\n",
      "\n",
      "Q: Use AUC to evaluate feature importance of numerical variables\n",
      "A: Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.\n",
      "\n",
      "Q: One of the method to visualize the decision trees\n",
      "A: dot_data = tree.export_graphviz(regr, out_file=None,\n",
      "feature_names=boston.feature_names,\n",
      "filled=True)\n",
      "graphviz.Source(dot_data, format=\"png\")\n",
      "Krishna Anand\n",
      "from sklearn import tree\n",
      "tree.plot_tree(dt,feature_names=dv.feature_names_)\n",
      "Added By Ryan Pramana\n",
      "\n",
      "Q: Use AUC to evaluate feature importance of numerical variables\n",
      "A: Check the solutions from the 2021 iteration of the course. You should use roc_auc_score.\n",
      "\n",
      "Q: One of the method to visualize the decision trees\n",
      "A: dot_data = tree.export_graphviz(regr, out_file=None,\n",
      "feature_names=boston.feature_names,\n",
      "filled=True)\n",
      "graphviz.Source(dot_data, format=\"png\")\n",
      "Krishna Anand\n",
      "from sklearn import tree\n",
      "tree.plot_tree(dt,feature_names=dv.feature_names_)\n",
      "Added By Ryan Pramana\n",
      "\n",
      "Q: Is it best to train your model only on the most important features?\n",
      "A: I’m just looking back at the lessons in week 3 (churn prediction project), and lesson 3.6 talks about Feature Importance for categorical values. At 8.12, the mutual info scores show that the some features are more important than others, but then in lesson 3.10 the Logistic Regression model is trained on all of the categorical variables (see 1:35). Once we have done feature importance, is it best to train your model only on the most important features?\n",
      "Not necessarily - rather, any feature that can offer additional predictive value should be included (so, e.g. predict with & without including that feature; if excluding it drops performance, keep it, else drop it). A few individually important features might in fact be highly correlated with others, & dropping some might be fine. There are many feature selection algorithms, it might be interesting to read up on them (among the methods we've learned so far in this course, L1 regularization (Lasso) implicitly does feature selection by shrinking some weights all the way to zero).\n",
      "By Rileen Sinha\n",
      "\n",
      "Q: Installing and updating to the python version 3.10 and higher\n",
      "A: Open terminal and type the code below to check the version on your laptop\n",
      "python3 --version\n",
      "For windows,\n",
      "Visit the official python website at  https://www.python.org/downloads/ to download the python version you need for installation\n",
      "Run the installer and  ensure to check the box that says “Add Python to PATH” during installation and complete the installation by following the prompts\n",
      "Or\n",
      "For Python 3,\n",
      "Open your command prompt or terminal and run the following command:\n",
      "pip install --upgrade python\n",
      "Aminat Abolade\n",
      "\n",
      "Q: Installing and updating to the python version 3.10 and higher\n",
      "A: Open terminal and type the code below to check the version on your laptop\n",
      "python3 --version\n",
      "For windows,\n",
      "Visit the official python website at  https://www.python.org/downloads/ to download the python version you need for installation\n",
      "Run the installer and  ensure to check the box that says “Add Python to PATH” during installation and complete the installation by following the prompts\n",
      "Or\n",
      "For Python 3,\n",
      "Open your command prompt or terminal and run the following command:\n",
      "pip install --upgrade python\n",
      "Aminat Abolade\n",
      "\n",
      "Q: ValueError: feature_names must be string, and may not contain [, ] or <\n",
      "A: In question 6, I was getting ValueError: feature_names must be string, and may not contain [, ] or < when I was creating DMatrix for train and validation\n",
      "Solution description\n",
      "The cause of this error is that some of the features names contain special characters like = and <, and I fixed the error by removing them as  follows:\n",
      "features= [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for i in features]\n",
      "Asia Saeed\n",
      "Alternative Solution:\n",
      "In my case the equal sign “=” was not a problem, so in my opinion the first part of Asias solution features= [i.replace(\"=<\", \"_\") should work as well.\n",
      "For me this works:\n",
      "features = []\n",
      "for f in dv.feature_names_:\n",
      "string = f.replace(“=<”, “-le”)\n",
      "features.append(string)\n",
      "Peter Ernicke\n",
      "\n",
      "Q: ValueError: feature_names must be string, and may not contain [, ] or <\n",
      "A: In question 6, I was getting ValueError: feature_names must be string, and may not contain [, ] or < when I was creating DMatrix for train and validation\n",
      "Solution description\n",
      "The cause of this error is that some of the features names contain special characters like = and <, and I fixed the error by removing them as  follows:\n",
      "features= [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for i in features]\n",
      "Asia Saeed\n",
      "Alternative Solution:\n",
      "In my case the equal sign “=” was not a problem, so in my opinion the first part of Asias solution features= [i.replace(\"=<\", \"_\") should work as well.\n",
      "For me this works:\n",
      "features = []\n",
      "for f in dv.feature_names_:\n",
      "string = f.replace(“=<”, “-le”)\n",
      "features.append(string)\n",
      "Peter Ernicke\n",
      "\n",
      "Q: Are there other ways to compute Precision, Recall and F1 score?\n",
      "A: Scikit-learn offers another way: precision_recall_fscore_support\n",
      "Example:\n",
      "from sklearn.metrics import precision_recall_fscore_support\n",
      "precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\n",
      "(Gopakumar Gopinathan)\n",
      "\n",
      "Q: Loading the Image with PILLOW library and converting to numpy array\n",
      "A: Pip install pillow - install pillow library\n",
      "from PIL import Image\n",
      "img = Image.open('aeroplane.png')\n",
      "From numpy import asarray\n",
      "numdata=asarray(img)\n",
      "Krishna Anand\n",
      "\n",
      "Q: Are there other ways to compute Precision, Recall and F1 score?\n",
      "A: Scikit-learn offers another way: precision_recall_fscore_support\n",
      "Example:\n",
      "from sklearn.metrics import precision_recall_fscore_support\n",
      "precision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\n",
      "(Gopakumar Gopinathan)\n",
      "\n",
      "Q: Loading the Image with PILLOW library and converting to numpy array\n",
      "A: Pip install pillow - install pillow library\n",
      "from PIL import Image\n",
      "img = Image.open('aeroplane.png')\n",
      "From numpy import asarray\n",
      "numdata=asarray(img)\n",
      "Krishna Anand\n",
      "\n",
      "Q: What dataset should I use to compute the metrics in Question 3\n",
      "A: You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\n",
      "Diego Giraldo\n",
      "\n",
      "Q: What dataset should I use to compute the metrics in Question 3\n",
      "A: You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.\n",
      "Diego Giraldo\n",
      "\n",
      "Q: Method to find the version of any install python libraries in jupyter notebook\n",
      "A: Import waitress\n",
      "print(waitress.__version__)\n",
      "Krishna Anand\n",
      "\n",
      "Q: \"Unable to import module 'lambda_function': No module named 'tensorflow'\" when run python test.py\n",
      "A: Make sure all codes in test.py dont have any dependencies with tensorflow library. One of most common reason that lead the this error is tflite still imported from tensorflow. Change import tensorflow.lite as tflite to import tflite_runtime.interpreter as tflite\n",
      "Added by Ryan Pramana\n",
      "\n",
      "Q: Could not install packages due to an OSError: [WinError 5] Access is denied\n",
      "A: When I run pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0 to install the libraries in windows machine,  I was getting the below error :\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Asia\\\\anaconda3\\\\Lib\\\\site-packages\\\\google\\\\protobuf\\\\internal\\\\_api_implementation.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "Solution description :\n",
      "I was able to install the libraries using below command:\n",
      "pip --user install grpcio==1.42.0 tensorflow-serving-api==2.7.0\n",
      "Asia Saeed\n",
      "\n",
      "Q: What If I submitted only two projects and failed to submit the third?\n",
      "A: If you have submitted two projects (and peer-reviewed at least 3 course-mates’ projects for each submission), you will get the certificate for the course. According to the course coordinator, Alexey Grigorev, only two projects are needed to get the course certificate.\n",
      "(optional) David Odimegwu\n",
      "\n",
      "Q: Method to find the version of any install python libraries in jupyter notebook\n",
      "A: Import waitress\n",
      "print(waitress.__version__)\n",
      "Krishna Anand\n",
      "\n",
      "Q: \"Unable to import module 'lambda_function': No module named 'tensorflow'\" when run python test.py\n",
      "A: Make sure all codes in test.py dont have any dependencies with tensorflow library. One of most common reason that lead the this error is tflite still imported from tensorflow. Change import tensorflow.lite as tflite to import tflite_runtime.interpreter as tflite\n",
      "Added by Ryan Pramana\n",
      "\n",
      "Q: Could not install packages due to an OSError: [WinError 5] Access is denied\n",
      "A: When I run pip install grpcio==1.42.0 tensorflow-serving-api==2.7.0 to install the libraries in windows machine,  I was getting the below error :\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Asia\\\\anaconda3\\\\Lib\\\\site-packages\\\\google\\\\protobuf\\\\internal\\\\_api_implementation.cp39-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "Solution description :\n",
      "I was able to install the libraries using below command:\n",
      "pip --user install grpcio==1.42.0 tensorflow-serving-api==2.7.0\n",
      "Asia Saeed\n",
      "\n",
      "Q: What If I submitted only two projects and failed to submit the third?\n",
      "A: If you have submitted two projects (and peer-reviewed at least 3 course-mates’ projects for each submission), you will get the certificate for the course. According to the course coordinator, Alexey Grigorev, only two projects are needed to get the course certificate.\n",
      "(optional) David Odimegwu\n",
      "\n",
      "Q: Why did we change the targets to binary format when calculating mutual information score in the homework?\n",
      "A: Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\n",
      "—Odimegwu David—-\n",
      "\n",
      "Q: xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks.\n",
      "A: Expanded error says: xgboost.core.XGBoostError: sklearn needs to be installed in order to use this module. So, sklearn in requirements solved the problem.\n",
      "George Chizhmak\n",
      "\n",
      "Q: Why did we change the targets to binary format when calculating mutual information score in the homework?\n",
      "A: Solution: Mutual Information score calculates the relationship between categorical variables or discrete variables. So in the homework, because the target which is median_house_value is continuous, we had to change it to binary format which in other words, makes its values discrete as either 0 or 1. If we allowed it to remain in the continuous variable format, the mutual information score could be calculated, but the algorithm would have to divide the continuous variables into bins and that would be highly subjective. That is why continuous variables are not used for mutual information score calculation.\n",
      "—Odimegwu David—-\n",
      "\n",
      "Q: xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks.\n",
      "A: Expanded error says: xgboost.core.XGBoostError: sklearn needs to be installed in order to use this module. So, sklearn in requirements solved the problem.\n",
      "George Chizhmak\n",
      "\n",
      "Q: Is use of libraries like fast.ai or huggingface allowed in the capstone and competition, or are they considered to be \"too much help\"?\n",
      "A: Yes, it’s allowed (as per Alexey).\n",
      "Added By Rileen Sinha\n",
      "\n",
      "Q: Flask image was built and tested successfully, but tensorflow serving image was built and unable to test successfully. What could be the problem?\n",
      "A: The TF and TF Serving versions have to match (as per solution from the slack channel)\n",
      "Added by Chiedu Elue\n",
      "\n",
      "Q: ValueError: continuous format is not supported\n",
      "A: Calling roc_auc_score() to get auc is throwing the above error.\n",
      "Solution to this issue is to make sure that you pass y_actuals as 1st argument and y_pred as 2nd argument.\n",
      "roc_auc_score(y_train, y_pred)\n",
      "Hareesh Tummala\n",
      "\n",
      "Q: ValueError: continuous format is not supported\n",
      "A: Calling roc_auc_score() to get auc is throwing the above error.\n",
      "Solution to this issue is to make sure that you pass y_actuals as 1st argument and y_pred as 2nd argument.\n",
      "roc_auc_score(y_train, y_pred)\n",
      "Hareesh Tummala\n",
      "\n",
      "Q: Standard deviation using Pandas built in Function\n",
      "A: In pandas you can use built in Pandas function names std() to get standard deviation. For example\n",
      "df['column_name'].std() to get standard deviation of that column.\n",
      "df[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\n",
      "(Khurram Majeed)\n",
      "\n",
      "Q: Standard deviation using Pandas built in Function\n",
      "A: In pandas you can use built in Pandas function names std() to get standard deviation. For example\n",
      "df['column_name'].std() to get standard deviation of that column.\n",
      "df[['column_1', 'column_2']].std() to get standard deviation of multiple columns.\n",
      "(Khurram Majeed)\n",
      "\n",
      "Q: Getting NaNs after applying .mean()\n",
      "A: I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\n",
      "I found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\n",
      "Added by Sasmito Yudha Husada\n",
      "\n",
      "Q: Getting NaNs after applying .mean()\n",
      "A: I was using for loops to apply rmse to list of y_val and y_pred. But the resulting rmse is all nan.\n",
      "I found out that the problem was when my data reached the mean step after squaring the error in the rmse function. Turned out there were nan in the array, then I traced the problem back to where I first started to split the data: I had only use fillna(0) on the train data, not on the validation and test data. So the problem was fixed after I applied fillna(0) to all the dataset (train, val, test). Voila, my for loops to get rmse from all the seed values work now.\n",
      "Added by Sasmito Yudha Husada\n",
      "\n",
      "Q: AttributeError: module ‘collections’ has no attribute ‘MutableMapping’\n",
      "A: Following the instruction from video week-5.6, using pipenv to install python libraries throws below error\n",
      "Solution to this error is to make sure that you are working with python==3.9 (as informed in the very first lesson of the zoomcamp) and not python==3.10.\n",
      "Added by Hareesh Tummala\n",
      "\n",
      "Q: AttributeError: module ‘collections’ has no attribute ‘MutableMapping’\n",
      "A: Following the instruction from video week-5.6, using pipenv to install python libraries throws below error\n",
      "Solution to this error is to make sure that you are working with python==3.9 (as informed in the very first lesson of the zoomcamp) and not python==3.10.\n",
      "Added by Hareesh Tummala\n",
      "\n",
      "Q: Handling Column Information for Homework 3 Question 6\n",
      "A: You need to use all features. and price for target. Don't include the average variable we created before.\n",
      "If you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\n",
      "I also used StandardScalar for numerical variable you can try running with or without this\n",
      "(Peter Pan)\n",
      "\n",
      "Q: Handling Column Information for Homework 3 Question 6\n",
      "A: You need to use all features. and price for target. Don't include the average variable we created before.\n",
      "If you use DictVectorizer then make sure to use sparce=True to avoid convergence errors\n",
      "I also used StandardScalar for numerical variable you can try running with or without this\n",
      "(Peter Pan)\n",
      "\n",
      "Q: ValueError: multi_class must be in ('ovo', 'ovr')\n",
      "A: I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\n",
      "I was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\n",
      "Asia Saeed\n",
      "\n",
      "Q: Difference between predict(X) and predict_proba(X)[:, 1]\n",
      "A: In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\n",
      "The solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\n",
      "Vladimir Yesipov\n",
      "Predict_proba shows probailites per class.\n",
      "Ani Mkrtumyan\n",
      "\n",
      "Q: Error decoding JSON response: Expecting value: line 1 column 1 (char 0)\n",
      "A: Problem happens when contacting the server waiting to send your predict-test and your data here in the correct shape.\n",
      "The problem was the format input to the model wasn’t in the right shape. Server receives the data in json format (dict) which is not suitable for the model. U should convert it to like numpy arrays.\n",
      "Ahmed Okka\n",
      "\n",
      "Q: ValueError: multi_class must be in ('ovo', 'ovr')\n",
      "A: I’m getting “ValueError: multi_class must be in ('ovo', 'ovr')” when using roc_auc_score to evaluate feature importance of numerical variables in question 1.\n",
      "I was getting this error because I was passing the parameters to roc_auc_score incorrectly (df_train[col] , y_train) . The correct way is to pass the parameters in this way: roc_auc_score(y_train, df_train[col])\n",
      "Asia Saeed\n",
      "\n",
      "Q: Difference between predict(X) and predict_proba(X)[:, 1]\n",
      "A: In case of using predict(X) for this task we are getting the binary classification predictions which are 0 and 1. This may lead to incorrect evaluation values.\n",
      "The solution is to use predict_proba(X)[:,1], where we get the probability that the value belongs to one of the classes.\n",
      "Vladimir Yesipov\n",
      "Predict_proba shows probailites per class.\n",
      "Ani Mkrtumyan\n",
      "\n",
      "Q: Error decoding JSON response: Expecting value: line 1 column 1 (char 0)\n",
      "A: Problem happens when contacting the server waiting to send your predict-test and your data here in the correct shape.\n",
      "The problem was the format input to the model wasn’t in the right shape. Server receives the data in json format (dict) which is not suitable for the model. U should convert it to like numpy arrays.\n",
      "Ahmed Okka\n",
      "\n",
      "Q: wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1\n",
      "A: If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again\n",
      "\n",
      "Q: wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv hangs on MacOS Ventura M1\n",
      "A: If you face this situation and see IPv6 addresses in the terminal, go to your System Settings > Network > your network connection > Details > Configure IPv6 > set to Manually > OK. Then try again\n",
      "\n",
      "Q: Singular Matrix Error\n",
      "A: I'm trying to invert the matrix but I got error that the matrix is singular matrix\n",
      "The singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\n",
      "\n",
      "Q: Singular Matrix Error\n",
      "A: I'm trying to invert the matrix but I got error that the matrix is singular matrix\n",
      "The singular matrix error is caused by the fact that not every matrix can be inverted. In particular, in the homework it happens because you have to pay close attention when dealing with multiplication (the method .dot) since multiplication is not commutative! X.dot(Y) is not necessarily equal to Y.dot(X), so respect the order otherwise you get the wrong matrix.\n",
      "\n",
      "Q: Standard Deviation Differences in Numpy and Pandas\n",
      "A: Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\n",
      "Numpy\n",
      "Pandas\n",
      "pandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\n",
      "import numpy as np\n",
      "np.std(df.weight, ddof=1)\n",
      "The result will be similar if we change the dof = 1 in numpy\n",
      "(Harish Balasundaram)\n",
      "\n",
      "Q: Standard Deviation Differences in Numpy and Pandas\n",
      "A: Numpy and Pandas packages use different equations to compute the standard deviation. Numpy uses  population standard deviation, whereas pandas uses sample standard deviation by default.\n",
      "Numpy\n",
      "Pandas\n",
      "pandas default standard deviation is computed using one degree of freedom. You can change degree in of freedom in NumPy to change this to unbiased estimator by using ddof parameter:\n",
      "import numpy as np\n",
      "np.std(df.weight, ddof=1)\n",
      "The result will be similar if we change the dof = 1 in numpy\n",
      "(Harish Balasundaram)\n",
      "\n",
      "Q: HPA doesn’t show CPU metrics\n",
      "A: Problem: CPU metrics Shows Unknown\n",
      "NAME         REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\n",
      "credit-hpa   Deployment/credit   <unknown>/20%   1         3         1          18s\n",
      "FailedGetResourceMetric       2m15s (x169 over 44m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API:\n",
      "Solution:\n",
      "-> Delete HPA (kubectl delete hpa credit-hpa)\n",
      "-> kubectl apply -f https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\n",
      "-> Create HPA\n",
      "This should solve the cpu metrics report issue.\n",
      "Added by Priya V\n",
      "\n",
      "Q: HPA doesn’t show CPU metrics\n",
      "A: Problem: CPU metrics Shows Unknown\n",
      "NAME         REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\n",
      "credit-hpa   Deployment/credit   <unknown>/20%   1         3         1          18s\n",
      "FailedGetResourceMetric       2m15s (x169 over 44m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API:\n",
      "Solution:\n",
      "-> Delete HPA (kubectl delete hpa credit-hpa)\n",
      "-> kubectl apply -f https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\n",
      "-> Create HPA\n",
      "This should solve the cpu metrics report issue.\n",
      "Added by Priya V\n",
      "\n",
      "Q: Where is the Python TensorFlow template on Saturn Cloud?\n",
      "A: This template is referred to in the video 8.1b Setting up the Environment on Saturn Cloud\n",
      "but the location shown in the video is no longer correct.\n",
      "This template has been moved to “python deep learning tutorials’ which is shown on the Saturn Cloud home page.\n",
      "Steven Christolis\n",
      "\n",
      "Q: Where is the Python TensorFlow template on Saturn Cloud?\n",
      "A: This template is referred to in the video 8.1b Setting up the Environment on Saturn Cloud\n",
      "but the location shown in the video is no longer correct.\n",
      "This template has been moved to “python deep learning tutorials’ which is shown on the Saturn Cloud home page.\n",
      "Steven Christolis\n",
      "\n",
      "Q: Are projects solo or collaborative/group work?\n",
      "A: Answer: All midterms and capstones are meant to be solo projects. [source @Alexey]\n",
      "\n",
      "Q: Are projects solo or collaborative/group work?\n",
      "A: Answer: All midterms and capstones are meant to be solo projects. [source @Alexey]\n",
      "\n",
      "Q: Error invoking API Gateway deploy API locally\n",
      "A: Problem: Trying to test API gateway in 9.7 - API Gateway: Exposing the Lambda Function, running: $ python test.py\n",
      "With error message:\n",
      "{'message': 'Missing Authentication Token'}\n",
      "Solution:\n",
      "Need to get the deployed API URL for the specific path you are invoking. Example:\n",
      "https://<random string>.execute-api.us-east-2.amazonaws.com/test/predict\n",
      "Added by Andrew Katoch\n",
      "\n",
      "Q: Error invoking API Gateway deploy API locally\n",
      "A: Problem: Trying to test API gateway in 9.7 - API Gateway: Exposing the Lambda Function, running: $ python test.py\n",
      "With error message:\n",
      "{'message': 'Missing Authentication Token'}\n",
      "Solution:\n",
      "Need to get the deployed API URL for the specific path you are invoking. Example:\n",
      "https://<random string>.execute-api.us-east-2.amazonaws.com/test/predict\n",
      "Added by Andrew Katoch\n",
      "\n",
      "Q: Serialized Model Xgboost error\n",
      "A: Save model by calling ‘booster.save_model’, see eg\n",
      "Load model:\n",
      "Dawuta Smit\n",
      "This section is moved to Projects\n",
      "\n",
      "Q: Pass many parameters in the model at once\n",
      "A: We can use the keras.models.Sequential() function to pass many parameters of the cnn at once.\n",
      "Krishna Anand\n",
      "\n",
      "Q: Serialized Model Xgboost error\n",
      "A: Save model by calling ‘booster.save_model’, see eg\n",
      "Load model:\n",
      "Dawuta Smit\n",
      "This section is moved to Projects\n",
      "\n",
      "Q: Pass many parameters in the model at once\n",
      "A: We can use the keras.models.Sequential() function to pass many parameters of the cnn at once.\n",
      "Krishna Anand\n",
      "\n",
      "Q: AttributeError: 'DictVectorizer' object has no attribute 'get_feature_names'\n",
      "A: The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
      "George Chizhmak\n",
      "\n",
      "Q: AttributeError: 'DictVectorizer' object has no attribute 'get_feature_names'\n",
      "A: The solution is to use “get_feature_names_out” instead. See details: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
      "George Chizhmak\n",
      "\n",
      "Q: ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "A: Set the host to ‘0.0.0.0’ on the flask app and dockerfile then RUN the url using localhost.\n",
      "(Theresa S.)\n",
      "\n",
      "Q: ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "A: Set the host to ‘0.0.0.0’ on the flask app and dockerfile then RUN the url using localhost.\n",
      "(Theresa S.)\n",
      "\n",
      "Q: The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021?\n",
      "A: We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\n",
      "If you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.\n",
      "\n",
      "Q: The course videos are from the previous iteration. Will you release new ones or we’ll use the videos from 2021?\n",
      "A: We won’t re-record the course videos. The focus of the course and the skills we want to teach remained the same, and the videos are still up-to-date.\n",
      "If you haven’t taken part in the previous iteration, you can start watching the videos. It’ll be useful for you and you will learn new things. However, we recommend using Python 3.10 now instead of Python 3.8.\n",
      "\n",
      "Q: I did the first two projects and skipped the last one so I wouldn't have two peer review in second capstone right?\n",
      "A: Yes. You only need to review peers when you submit your project.\n",
      "Confirmed on Slack by Alexey Grigorev (added by Rileen Sinha)\n",
      "\n",
      "Q: I did the first two projects and skipped the last one so I wouldn't have two peer review in second capstone right?\n",
      "A: Yes. You only need to review peers when you submit your project.\n",
      "Confirmed on Slack by Alexey Grigorev (added by Rileen Sinha)\n",
      "\n",
      "Q: What are the project deadlines?\n",
      "A: Answer: You can see them here (it’s taken from the 2022 cohort page). Go to the cohort folder for your own cohort’s deadline.\n",
      "\n",
      "Q: What are the project deadlines?\n",
      "A: Answer: You can see them here (it’s taken from the 2022 cohort page). Go to the cohort folder for your own cohort’s deadline.\n",
      "\n",
      "Q: ValueError: shapes not aligned\n",
      "A: If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\n",
      "If this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\n",
      "(Santhosh Kumar)\n",
      "\n",
      "Q: Features in Ridge Regression Model\n",
      "A: Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\n",
      "Drop all categorical features first before proceeding.\n",
      "(Aileah Gotladera)\n",
      "While it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\n",
      "(Erjon)\n",
      "\n",
      "Q: ValueError: shapes not aligned\n",
      "A: If we try to perform an arithmetic operation between 2 arrays of different shapes or different dimensions, it throws an error like operands could not be broadcast together with shapes. There are some scenarios when broadcasting can occur and when it fails.\n",
      "If this happens sometimes we can use * operator instead of dot() method to solve the issue. So that the error is solved and also we get the dot product.\n",
      "(Santhosh Kumar)\n",
      "\n",
      "Q: Features in Ridge Regression Model\n",
      "A: Make sure that the features used in ridge regression model are only NUMERICAL ones not categorical.\n",
      "Drop all categorical features first before proceeding.\n",
      "(Aileah Gotladera)\n",
      "While it is True that ridge regression accepts only numerical values, the categorical ones can be useful for your model. You have to transform them using one-hot encoding before training the model. To avoid the error of non convergence, put sparse=True when doing so.\n",
      "(Erjon)\n",
      "\n",
      "Q: Checking GPU and CPU utilization using ‘nvitop’\n",
      "A: The Python package ‘’ is an interactive GPU process viewer similar to ‘htop’ for CPU.\n",
      "https://pypi.org/project//\n",
      "Image source: https://pypi.org/project//\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: Checking GPU and CPU utilization using ‘nvitop’\n",
      "A: The Python package ‘’ is an interactive GPU process viewer similar to ‘htop’ for CPU.\n",
      "https://pypi.org/project//\n",
      "Image source: https://pypi.org/project//\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "Q: Reading the dataset directly from github\n",
      "A: The dataset can be read directly to pandas dataframe from the github link using the technique shown below\n",
      "dfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\n",
      "Krishna Anand\n",
      "\n",
      "Q: Reading the dataset directly from github\n",
      "A: The dataset can be read directly to pandas dataframe from the github link using the technique shown below\n",
      "dfh=pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\")\n",
      "Krishna Anand\n",
      "\n",
      "Q: What syntax use in Pandas for multiple conditions using logical AND and OR\n",
      "A: If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\n",
      "(Olga Rudakova)\n",
      "–\n",
      "\n",
      "Q: What syntax use in Pandas for multiple conditions using logical AND and OR\n",
      "A: If you would like to use multiple conditions as an example below you will get the error. The correct syntax for OR is |, and for AND is &\n",
      "(Olga Rudakova)\n",
      "–\n",
      "\n",
      "Q: Getting error when connect git on Saturn Cloud: permission denied\n",
      "A: Problem description:\n",
      "When follow module 8.1b video to setup git in Saturn Cloud, run `ssh -T git@github.com` lead error `git@github.com: Permission denied (publickey).`\n",
      "Solution description:\n",
      "Alternative way, we can setup git in our Saturn Cloud env with generate SSH key in our Saturn Cloud and add it to our git account host. After it, we can access/manage our git through Saturn’s jupyter server. All steps detailed on this following tutorial: https://saturncloud.io/docs/using-saturn-cloud/gitrepo/\n",
      "Added by Ryan Pramana\n",
      "\n",
      "Q: Getting error when connect git on Saturn Cloud: permission denied\n",
      "A: Problem description:\n",
      "When follow module 8.1b video to setup git in Saturn Cloud, run `ssh -T git@github.com` lead error `git@github.com: Permission denied (publickey).`\n",
      "Solution description:\n",
      "Alternative way, we can setup git in our Saturn Cloud env with generate SSH key in our Saturn Cloud and add it to our git account host. After it, we can access/manage our git through Saturn’s jupyter server. All steps detailed on this following tutorial: https://saturncloud.io/docs/using-saturn-cloud/gitrepo/\n",
      "Added by Ryan Pramana\n",
      "\n",
      "Q: Q: ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts')\n",
      "A: After entering `pipenv shell` don’t forget to use `exit` before `pipenv --rm`, as it may cause errors when trying to install packages, it is unclear whether you are “in the shell”(using Windows) at the moment as there are no clear markers for it.\n",
      "It can also mess up PATH, if that’s the case, here’s terminal commands for fixing that:\n",
      "# for Windows\n",
      "set VIRTUAL_ENV \"\"\n",
      "# for Unix\n",
      "export VIRTUAL_ENV=\"\"\n",
      "Also manually re-creating removed folder at `C:\\Users\\username\\.virtualenvs\\removed-envname` can help, removed-envname can be seen at the error message.\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Q: ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts')\n",
      "A: After entering `pipenv shell` don’t forget to use `exit` before `pipenv --rm`, as it may cause errors when trying to install packages, it is unclear whether you are “in the shell”(using Windows) at the moment as there are no clear markers for it.\n",
      "It can also mess up PATH, if that’s the case, here’s terminal commands for fixing that:\n",
      "# for Windows\n",
      "set VIRTUAL_ENV \"\"\n",
      "# for Unix\n",
      "export VIRTUAL_ENV=\"\"\n",
      "Also manually re-creating removed folder at `C:\\Users\\username\\.virtualenvs\\removed-envname` can help, removed-envname can be seen at the error message.\n",
      "Added by Andrii Larkin\n",
      "\n",
      "Q: Use of random seed in HW3\n",
      "A: For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\n",
      "Answer: for both splits random_state = 42 should be used\n",
      "(Bhaskar Sarma)\n",
      "\n",
      "Q: Use of random seed in HW3\n",
      "A: For the test_train_split question on week 3's homework, are we supposed to use 42 as the random_state in both splits or only the 1st one?\n",
      "Answer: for both splits random_state = 42 should be used\n",
      "(Bhaskar Sarma)\n",
      "\n",
      "Q: Information Gain\n",
      "A: Information gain  in Y due to X, or the mutual information of Y and X\n",
      "Where  is the entropy of Y. \n",
      "\n",
      "If X is completely uninformative about Y:\n",
      "If X is completely informative about Y: )\n",
      "Hrithik Kumar Advani\n",
      "\n",
      "Q: Information Gain\n",
      "A: Information gain  in Y due to X, or the mutual information of Y and X\n",
      "Where  is the entropy of Y. \n",
      "\n",
      "If X is completely uninformative about Y:\n",
      "If X is completely informative about Y: )\n",
      "Hrithik Kumar Advani\n",
      "\n",
      "Q: Using Tensorflow 2.15 for AWS deployment\n",
      "A: Using the 2.14 version with python 3.11 works fine.\n",
      "In case it doesn’t work, I tried with tensorflow 2.4.4 whl, however, make sure to run it on top of supported python versions like 3.8, else there will be issues installing tf==2.4.4\n",
      "Added by Abhijit Chakraborty\n",
      "\n",
      "Q: Using Tensorflow 2.15 for AWS deployment\n",
      "A: Using the 2.14 version with python 3.11 works fine.\n",
      "In case it doesn’t work, I tried with tensorflow 2.4.4 whl, however, make sure to run it on top of supported python versions like 3.8, else there will be issues installing tf==2.4.4\n",
      "Added by Abhijit Chakraborty\n",
      "\n",
      "Q: Out of memory errors when running tensorflow\n",
      "A: I found this code snippet fixed my OOM errors, as I have an Nvidia GPU. Can't speak to OOM errors on CPU, though.\n",
      "https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\n",
      "```\n",
      "physical_devices = tf.configlist_physical_devices('GPU')\n",
      "try:\n",
      "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
      "except:\n",
      "# Invalid device or cannot modify virtual devices once initialized.\n",
      "pass\n",
      "```\n",
      "\n",
      "Q: Out of memory errors when running tensorflow\n",
      "A: I found this code snippet fixed my OOM errors, as I have an Nvidia GPU. Can't speak to OOM errors on CPU, though.\n",
      "https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\n",
      "```\n",
      "physical_devices = tf.configlist_physical_devices('GPU')\n",
      "try:\n",
      "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
      "except:\n",
      "# Invalid device or cannot modify virtual devices once initialized.\n",
      "pass\n",
      "```\n",
      "\n",
      "Q: Loading the dataset directly through Kaggle Notebooks\n",
      "A: For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\n",
      "!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "Once the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\n",
      "df = pd.read_csv('housing.csv')\n",
      "Harish Balasundaram\n",
      "\n",
      "Q: What is Stratified k-fold?\n",
      "A: For multiclass classification it is important to keep class balance when you split the data set. In this case Stratified k-fold returns folds that contains approximately the sme percentage of samples of each classes.\n",
      "Please check the realisation in sk-learn library:\n",
      "https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold\n",
      "Olga Rudakova\n",
      "\n",
      "Q: Loading the dataset directly through Kaggle Notebooks\n",
      "A: For users of kaggle notebooks, the dataset can be loaded through widget using the below command. Please remember that ! before wget is essential\n",
      "!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n",
      "Once the dataset is loaded to the kaggle notebook server, it can be read through the below pandas command\n",
      "df = pd.read_csv('housing.csv')\n",
      "Harish Balasundaram\n",
      "\n",
      "Q: What is Stratified k-fold?\n",
      "A: For multiclass classification it is important to keep class balance when you split the data set. In this case Stratified k-fold returns folds that contains approximately the sme percentage of samples of each classes.\n",
      "Please check the realisation in sk-learn library:\n",
      "https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold\n",
      "Olga Rudakova\n",
      "\n",
      "Q: ImportError: generic_type: type \"InterpreterWrapper\" is already registered!\n",
      "A: When I run   import tflite_runtime.interpreter as tflite , I get an error message says “ImportError: generic_type: type \"InterpreterWrapper\" is already registered!”\n",
      "Solution description\n",
      "This error occurs when you import both tensorflow  and tflite_runtime.interpreter  “import tensorflow as tf” and “import tflite_runtime.interpreter as tflite” in the same notebook.  To fix the issue, restart the kernel and import only tflite_runtime.interpreter \" import tflite_runtime.interpreter as tflite\".\n",
      "Asia Saeed\n",
      "\n",
      "Q: ImportError: generic_type: type \"InterpreterWrapper\" is already registered!\n",
      "A: When I run   import tflite_runtime.interpreter as tflite , I get an error message says “ImportError: generic_type: type \"InterpreterWrapper\" is already registered!”\n",
      "Solution description\n",
      "This error occurs when you import both tensorflow  and tflite_runtime.interpreter  “import tensorflow as tf” and “import tflite_runtime.interpreter as tflite” in the same notebook.  To fix the issue, restart the kernel and import only tflite_runtime.interpreter \" import tflite_runtime.interpreter as tflite\".\n",
      "Asia Saeed\n",
      "\n",
      "Q: I don't know math. Can I take the course?\n",
      "A: Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\n",
      "Here are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: I don't know math. Can I take the course?\n",
      "A: Yes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\n",
      "Here are some interesting videos covering linear algebra that you can already watch: ML Zoomcamp 1.8 - Linear Algebra Refresher from Alexey Grigorev or the excellent playlist from 3Blue1Brown Vectors | Chapter 1, Essence of linear algebra. Never hesitate to ask the community for help if you have any question.\n",
      "(Mélanie Fouesnard)\n",
      "\n",
      "Q: ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "A: Solution description: duplicating the\n",
      "df.churn = (df.churn == 'yes').astype(int)\n",
      "This is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\n",
      "It is telling us that it only contains 0's.\n",
      "Delete one of the below cells and you will get the accuracy\n",
      "Humberto Rodriguez\n",
      "\n",
      "Q: ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "A: Solution description: duplicating the\n",
      "df.churn = (df.churn == 'yes').astype(int)\n",
      "This is causing you to have only 0's in your churn column. In fact, match with the error you are getting:  ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0.\n",
      "It is telling us that it only contains 0's.\n",
      "Delete one of the below cells and you will get the accuracy\n",
      "Humberto Rodriguez\n"
     ]
    }
   ],
   "source": [
    "decoded_prompt = encoding.decode(tokens)\n",
    "print(decoded_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53f7f4",
   "metadata": {},
   "source": [
    "### Q 7 Bonus: generating the answer (ungraded)\n",
    "Let's send the prompt to OpenAI. What's the response?\n",
    "\n",
    "Note: you can replace OpenAI with Ollama. See module 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9a75717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d9aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40ad9c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To copy a file to a Docker container, you can use the `docker cp` command. The basic syntax is:\n",
      "\n",
      "```\n",
      "docker cp /path/to/local/file_or_directory container_id:/path/in/container\n",
      "```\n",
      "\n",
      "This command allows you to copy files or directories from your local machine into a running Docker container.\n",
      "Number of tokens in the answer: 63\n"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI()  # This uses the OPENAI_API_KEY environment variable\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)\n",
    "print(f\"Number of tokens in the answer: {len(encoding.encode(answer))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9809a1b",
   "metadata": {},
   "source": [
    "### Bonus: calculating the costs (ungraded)\n",
    "Suppose that on average per request we send 150 tokens and receive back 250 tokens.\n",
    "\n",
    "How much will it cost to run 1000 requests?\n",
    "\n",
    "You can see the prices here\n",
    "\n",
    "On June 17, the prices for gpt4o are:\n",
    "\n",
    "Input: $0.005 / 1K tokens\n",
    "Output: $0.015 / 1K tokens\n",
    "You can redo the calculations with the values you got in Q6 and Q7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29312ddb",
   "metadata": {},
   "source": [
    "Total cose is Here’s how you calculate the cost:\n",
    "\n",
    "**Given:**\n",
    "- 150 input tokens/request\n",
    "- 250 output tokens/request\n",
    "- 1000 requests\n",
    "- Input price: $0.005 per 1K tokens\n",
    "- Output price: $0.015 per 1K tokens\n",
    "\n",
    "---\n",
    "\n",
    "**Final answer:**  \n",
    "> It will cost **$4.50** to run 1000 requests with these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597cc78b",
   "metadata": {},
   "source": [
    "Total cose is Here’s how you calculate the cost:\n",
    "\n",
    "**Given:**\n",
    "- 335 input tokens/request\n",
    "- 64 output tokens/request\n",
    "- 1000 requests\n",
    "- Input price: $0.005 per 1K tokens\n",
    "- Output price: $0.015 per 1K tokens\n",
    "\n",
    "---\n",
    "\n",
    "**Final answer:**  \n",
    "> It will cost **$2.635** to run 1000 requests with these parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
